{
  "AzureBlob": {
    "connection_string": "YOUR_BLOB_CONNECTION_STRING",
    "inputcontainer": "inputcontainer",
    "outputcontainer": "outputcontainer"
  },

  "AzureOpenAI": {
    "endpoint": "https://YOUR-GPT.openai.azure.com/",
    "api_key": "YOUR_GPT_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "gpt-5-deployment"
  },

  "AzureEmbedding": {
    "endpoint": "https://YOUR-EMBEDDING.openai.azure.com/",
    "api_key": "YOUR_EMBEDDING_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "embedding-deployment",
    "dimension": 1536
  },

  "AzureAISearch": {
    "endpoint": "https://YOUR-SEARCH.search.windows.net",
    "api_key": "YOUR_ADMIN_KEY"
  },

  "DocumentIntelligence": {
    "endpoint": "https://YOUR-DOCINTEL.cognitiveservices.azure.com/",
    "api_key": "YOUR_DOC_INTEL_KEY"
  },

  "confidence_threshold": 0.90,

  "fields": [
    "FullName",
    "DateOfBirth",
    "DocumentNumber",
    "Address"
  ]
}



main.py
import json
import argparse
import uuid
from datetime import datetime

from azure.storage.blob import BlobServiceClient
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex,
    SearchField,
    SimpleField,
    SearchFieldDataType,
    VectorSearch,
    HnswAlgorithmConfiguration,
    VectorSearchProfile
)

from openai import AzureOpenAI


# =========================
# UTIL FUNCTIONS
# =========================

def load_config():
    with open("config.json") as f:
        return json.load(f)

def now():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

def supported(name):
    return name.lower().endswith((".pdf", ".doc", ".docx", ".jpg", ".jpeg", ".png"))

def chunk_text(text, size=2000):
    return [text[i:i+size] for i in range(0, len(text), size)]


# =========================
# OCR
# =========================

def extract_text(config, file_bytes):
    client = DocumentAnalysisClient(
        config["DocumentIntelligence"]["endpoint"],
        AzureKeyCredential(config["DocumentIntelligence"]["api_key"])
    )

    poller = client.begin_analyze_document("prebuilt-read", file_bytes)
    result = poller.result()

    text = ""
    for page in result.pages:
        for line in page.lines:
            text += line.content + "\n"

    return text


# =========================
# EMBEDDING
# =========================

def get_embedding(config, text):
    client = AzureOpenAI(
        api_key=config["AzureEmbedding"]["api_key"],
        azure_endpoint=config["AzureEmbedding"]["endpoint"],
        api_version=config["AzureEmbedding"]["api_version"]
    )

    response = client.embeddings.create(
        model=config["AzureEmbedding"]["deployment_name"],
        input=text
    )

    return response.data[0].embedding


# =========================
# CREATE INDEX (LATEST SDK)
# =========================

def create_index_if_not_exists(config, index_name):

    dimension = config["AzureEmbedding"]["dimension"]

    index_client = SearchIndexClient(
        config["AzureAISearch"]["endpoint"],
        AzureKeyCredential(config["AzureAISearch"]["api_key"])
    )

    fields = [
        SimpleField(name="id", type=SearchFieldDataType.String, key=True),

        SearchField(
            name="content",
            type=SearchFieldDataType.String,
            searchable=True
        ),

        SearchField(
            name="embedding",
            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
            searchable=True,
            vector_search_dimensions=dimension,
            vector_search_profile_name="vector-profile"
        )
    ]

    vector_search = VectorSearch(
        algorithms=[
            HnswAlgorithmConfiguration(name="hnsw-algorithm")
        ],
        profiles=[
            VectorSearchProfile(
                name="vector-profile",
                algorithm_configuration_name="hnsw-algorithm"
            )
        ]
    )

    index = SearchIndex(
        name=index_name,
        fields=fields,
        vector_search=vector_search
    )

    try:
        index_client.create_index(index)
        print("Index created:", index_name)
    except:
        print("Index exists:", index_name)


# =========================
# UPLOAD CHUNKS
# =========================

def upload_chunks(config, index_name, chunks):

    client = SearchClient(
        config["AzureAISearch"]["endpoint"],
        index_name,
        AzureKeyCredential(config["AzureAISearch"]["api_key"])
    )

    docs = []

    for chunk in chunks:
        embedding = get_embedding(config, chunk)

        docs.append({
            "id": str(uuid.uuid4()),
            "content": chunk,
            "embedding": embedding
        })

    client.upload_documents(docs)


# =========================
# VECTOR SEARCH
# =========================

def vector_search(config, index_name, query):

    client = SearchClient(
        config["AzureAISearch"]["endpoint"],
        index_name,
        AzureKeyCredential(config["AzureAISearch"]["api_key"])
    )

    embedding = get_embedding(config, query)

    results = client.search(
        search_text=None,
        vectors=[{
            "value": embedding,
            "fields": "embedding",
            "k": 5
        }]
    )

    context = ""
    for r in results:
        context += r["content"] + "\n"

    return context


# =========================
# GPT EXTRACTION
# =========================

def extract_fields(config, context):

    client = AzureOpenAI(
        api_key=config["AzureOpenAI"]["api_key"],
        azure_endpoint=config["AzureOpenAI"]["endpoint"],
        api_version=config["AzureOpenAI"]["api_version"]
    )

    prompt = f"""
Extract the following fields and return STRICT JSON.

Fields:
{config["fields"]}

Context:
{context}
"""

    response = client.chat.completions.create(
        model=config["AzureOpenAI"]["deployment_name"],
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    try:
        return json.loads(response.choices[0].message.content)
    except:
        print("Invalid JSON from GPT")
        return {}


# =========================
# MAIN PIPELINE
# =========================

def main():

    parser = argparse.ArgumentParser()
    parser.add_argument("--inputfolder", default=None)
    parser.add_argument("--ocr", default="true")
    args = parser.parse_args()

    config = load_config()
    ocr_enabled = args.ocr.lower() == "true"

    blob_service = BlobServiceClient.from_connection_string(
        config["AzureBlob"]["connection_string"]
    )

    input_container = blob_service.get_container_client(
        config["AzureBlob"]["inputcontainer"]
    )

    output_container = blob_service.get_container_client(
        config["AzureBlob"]["outputcontainer"]
    )

    prefix = args.inputfolder.strip("/") + "/" if args.inputfolder else ""

    blobs = input_container.list_blobs(name_starts_with=prefix)

    providers = {}

    for blob in blobs:
        parts = blob.name.split("/")
        provider = parts[0] if not prefix else parts[1]
        providers.setdefault(provider, []).append(blob.name)

    for provider, files in providers.items():

        print("Processing provider:", provider)

        index_name = provider.lower().replace("_", "-")
        create_index_if_not_exists(config, index_name)

        full_text = ""

        for file in files:

            if not supported(file):
                continue

            blob = blob_service.get_blob_client(
                config["AzureBlob"]["inputcontainer"],
                file
            )

            file_bytes = blob.download_blob().readall()

            if ocr_enabled:
                text = extract_text(config, file_bytes)
            else:
                text = file_bytes.decode(errors="ignore")

            full_text += text + "\n"

        chunks = chunk_text(full_text)

        upload_chunks(config, index_name, chunks)

        query = "Extract " + ", ".join(config["fields"])
        context = vector_search(config, index_name, query)

        result = extract_fields(config, context)

        row = {
            "id": provider,
            "extractiondatetime": now()
        }

        high_conf = True

        for field in config["fields"]:
            value = result.get(field, "")
            confidence = 0.95 if value else 0.0

            row[field] = value
            row[field+"_confidence"] = confidence
            row[field+"_sourcedocument"] = provider

            if confidence < config["confidence_threshold"]:
                high_conf = False

        label = "Highconfidence" if high_conf else "Lowconfidence"

        headers = row.keys()
        csv_data = ",".join(headers) + "\n"
        csv_data += ",".join(str(row[h]) for h in headers)

        output_path = f"{label}/{provider}.csv"

        output_container.upload_blob(output_path, csv_data, overwrite=True)

        print("Saved:", output_path)


if __name__ == "__main__":
    main()
