helper.py

  """
HELPER - Azure Service Managers
================================
Manages Azure Blob, OpenAI, Document Intelligence, AI Search
"""

from azure.storage.blob import BlobServiceClient
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
from openai import AzureOpenAI
from typing import List, Dict, Any
import json


class AzureBlobManager:
    """Azure Blob Storage Manager"""
    
    def __init__(self, connection_string: str, input_container: str, output_container: str):
        self.blob_service = BlobServiceClient.from_connection_string(connection_string)
        self.input_container = input_container
        self.output_container = output_container
        
        # Ensure containers exist
        try:
            self.blob_service.create_container(input_container)
        except:
            pass
        try:
            self.blob_service.create_container(output_container)
        except:
            pass
    
    def list_blobs(self) -> List[str]:
        """List all blobs in input container"""
        container_client = self.blob_service.get_container_client(self.input_container)
        return [blob.name for blob in container_client.list_blobs()]
    
    def download_blob(self, blob_name: str) -> bytes:
        """Download blob as bytes"""
        blob_client = self.blob_service.get_blob_client(self.input_container, blob_name)
        return blob_client.download_blob().readall()
    
    def upload_blob(self, blob_name: str, data: bytes, overwrite: bool = True):
        """Upload blob to output container"""
        blob_client = self.blob_service.get_blob_client(self.output_container, blob_name)
        blob_client.upload_blob(data, overwrite=overwrite)


class AzureOpenAIManager:
    """Azure OpenAI Manager (GPT-4o + Embeddings)"""
    
    def __init__(
        self,
        endpoint: str,
        api_key: str,
        api_version: str,
        deployment_name: str,
        embedding_endpoint: str,
        embedding_api_key: str,
        embedding_deployment: str
    ):
        self.deployment_name = deployment_name
        self.embedding_deployment = embedding_deployment
        
        # GPT-4o client
        self.gpt_client = AzureOpenAI(
            azure_endpoint=endpoint,
            api_key=api_key,
            api_version=api_version
        )
        
        # Embedding client
        self.embedding_client = AzureOpenAI(
            azure_endpoint=embedding_endpoint,
            api_key=embedding_api_key,
            api_version=api_version
        )
        
        # Token tracking
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0
    
    def generate_embeddings(self, text: str) -> List[float]:
        """Generate text embedding (3072 dims)"""
        response = self.embedding_client.embeddings.create(
            input=text,
            model=self.embedding_deployment
        )
        return response.data[0].embedding


class DocumentIntelligenceManager:
    """Azure Document Intelligence Manager (OCR)"""
    
    def __init__(self, endpoint: str, key: str):
        self.client = DocumentAnalysisClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(key)
        )
    
    def analyze_document(self, document_bytes: bytes) -> Dict[str, Any]:
        """
        Analyze document with OCR
        Returns: {'content': 'extracted text', 'pages': [...]}
        """
        poller = self.client.begin_analyze_document(
            "prebuilt-read",
            document=document_bytes
        )
        result = poller.result()
        
        # Extract all text
        content = result.content if hasattr(result, 'content') else ''
        
        # Extract pages
        pages = []
        if hasattr(result, 'pages'):
            for page in result.pages:
                pages.append({
                    'page_number': page.page_number,
                    'width': page.width,
                    'height': page.height,
                    'unit': page.unit
                })
        
        return {
            'content': content,
            'pages': pages
        }



-------


  main.py

  """
MAIN PIPELINE - Document Extraction with Multimodal RAG
========================================================
Entry point. Run: python main.py
"""

import json
from datetime import datetime
from typing import List, Dict, Any

from helper import AzureBlobManager, AzureOpenAIManager, DocumentIntelligenceManager
from unified_rag import create_rag_extractor


def load_config(path: str = "config.json") -> Dict[str, Any]:
    with open(path, 'r') as f:
        return json.load(f)


def process_provider(
    provider: str,
    documents: List[str],
    blob_mgr,
    doc_intel_mgr,
    openai_mgr,
    rag_extractor,
    config: Dict[str, Any]
) -> Dict[str, Any]:
    
    print(f"\n{'='*60}\n  PROVIDER: {provider} ({len(documents)} docs)\n{'='*60}")
    
    fields = config['fields']
    mode = config['rag']['mode']
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    index_name = f"{provider.lower()}-{timestamp}"
    
    results = []
    total_cost = 0.0
    
    for idx, doc_name in enumerate(documents, 1):
        print(f"\n [{idx}/{len(documents)}] {doc_name}")
        
        try:
            doc_bytes = blob_mgr.download_blob(doc_name)
            
            # OCR (PRIMARY)
            print(f"   ğŸ” OCR...")
            ocr_result = doc_intel_mgr.analyze_document(doc_bytes)
            doc_text = ocr_result.get('content', '')
            
            if len(doc_text) < 50:
                print(f"   âš ï¸  Insufficient OCR - skip")
                continue
            
            print(f"   âœ“ OCR: {len(doc_text)} chars")
            
            # RAG Extraction
            if idx == 1:
                print(f"   ğŸ“„ First doc - no RAG")
                if mode == 'multimodal':
                    result = rag_extractor.extract_without_rag(doc_text, doc_bytes, None, doc_name)
                else:
                    result = rag_extractor.extract_without_rag(doc_text, None, doc_name)
            else:
                print(f"   ğŸ“‹ RAG extraction...")
                if mode == 'multimodal':
                    result = rag_extractor.extract_with_rag(doc_text, doc_bytes, provider, index_name, doc_name)
                else:
                    result = rag_extractor.extract_with_rag(doc_text, provider, index_name, doc_name)
            
            extracted = result.get('extracted_fields', {})
            avg_conf = sum([f.get('confidence',0) for f in extracted.values() if isinstance(f,dict)]) / len(extracted) if extracted else 0
            
            print(f"   âœ“ Confidence: {avg_conf:.2f} | RAG: {result.get('used_rag')} | Vision: {result.get('has_vision')}")
            
            results.append({
                'document_name': doc_name,
                'extracted_fields': extracted,
                'avg_confidence': avg_conf,
                'used_rag': result.get('used_rag', False),
                'has_vision': result.get('has_vision', False)
            })
            
            total_cost += 0.07 if mode == 'multimodal' else 0.05
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    print(f"\n  âœ“ Processed: {len(results)}/{len(documents)} | Cost: ${total_cost:.2f}")
    
    return {
        'provider': provider,
        'results': results,
        'total_cost': total_cost,
        'index_name': index_name
    }


def save_results(provider_results: Dict, blob_mgr, config: Dict):
    provider = provider_results['provider']
    results = provider_results['results']
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    fields = config['fields']
    
    # CSV - one row
    csv_lines = [','.join(['provider','doc_count'] + fields + ['avg_confidence'])]
    agg = {'provider': provider, 'doc_count': len(results)}
    
    for field in fields:
        for r in results:
            if field in r['extracted_fields']:
                fd = r['extracted_fields'][field]
                agg[field] = fd.get('value','') if isinstance(fd,dict) else str(fd)
                break
        if field not in agg:
            agg[field] = ''
    
    avg = sum([r['avg_confidence'] for r in results]) / len(results) if results else 0
    agg['avg_confidence'] = f"{avg:.2f}"
    
    csv_lines.append(','.join([agg.get('provider',''), str(agg.get('doc_count',0))] + [agg.get(f,'') for f in fields] + [agg.get('avg_confidence','0.00')]))
    
    csv_file = f"{provider}_{timestamp}.csv"
    blob_mgr.upload_blob(csv_file, '\n'.join(csv_lines).encode('utf-8'))
    print(f"  âœ“ CSV: {csv_file}")
    
    # JSON - detailed
    json_file = f"{provider}_{timestamp}_detailed.json"
    blob_mgr.upload_blob(json_file, json.dumps(provider_results, indent=2).encode('utf-8'))
    print(f"  âœ“ JSON: {json_file}")
    
    # Costs
    cost_file = f"{provider}_{timestamp}_costs.json"
    cost_data = {
        'provider': provider,
        'total_cost_usd': provider_results['total_cost'],
        'documents': len(results),
        'cost_per_doc': provider_results['total_cost']/len(results) if results else 0
    }
    blob_mgr.upload_blob(cost_file, json.dumps(cost_data, indent=2).encode('utf-8'))
    print(f"  âœ“ Costs: {cost_file}")


def main():
    print("\n" + "="*60)
    print("  MULTIMODAL RAG â€” DOCUMENT EXTRACTION")
    print("  Healthcare / HIPAA Â· Azure BAA")
    print("="*60)
    
    config = load_config()
    print(f"\nâœ“ Config loaded | Fields: {', '.join(config['fields'])}")
    print(f"âœ“ RAG Mode: {config['rag']['mode'].upper()}")
    
    # Initialize managers
    blob_mgr = AzureBlobManager(
        config['AzureBlob']['connection_string'],
        config['AzureBlob']['inputcontainer'],
        config['AzureBlob']['outputcontainer']
    )
    print(f"âœ“ Blob connected")
    
    openai_mgr = AzureOpenAIManager(
        config['AzureOpenAI']['endpoint'],
        config['AzureOpenAI']['api_key'],
        config['AzureOpenAI']['api_version'],
        config['AzureOpenAI']['deployment_name'],
        config['AzureEmbedding']['endpoint'],
        config['AzureEmbedding']['api_key'],
        config['AzureEmbedding']['deployment_name']
    )
    print(f"âœ“ OpenAI connected")
    
    doc_intel_mgr = DocumentIntelligenceManager(
        config['DocumentIntelligence']['endpoint'],
        config['DocumentIntelligence']['key']
    )
    print(f"âœ“ Doc Intelligence connected")
    
    rag_extractor = create_rag_extractor(
        config['rag'],
        config['AzureAISearch']['endpoint'],
        config['AzureAISearch']['api_key'],
        openai_mgr,
        config['fields']
    )
    print(f"âœ“ RAG Extractor ready")
    
    # Get documents
    all_docs = blob_mgr.list_blobs()
    print(f"\nâœ“ Found {len(all_docs)} documents")
    
    if not all_docs:
        print("\nâš ï¸  No documents. Upload to input container and retry.")
        return
    
    # Group by provider
    providers = {}
    for doc in all_docs:
        prov = doc.split('_')[0] if '_' in doc else 'default'
        if prov not in providers:
            providers[prov] = []
        providers[prov].append(doc)
    
    print(f"âœ“ Grouped into {len(providers)} providers")
    
    # Process each
    for prov, docs in providers.items():
        prov_results = process_provider(prov, docs, blob_mgr, doc_intel_mgr, openai_mgr, rag_extractor, config)
        save_results(prov_results, blob_mgr, config)
    
    print("\n" + "="*60)
    print("  COMPLETE âœ“")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()


---------

  log

"""
LOGGING CONFIGURATION - Timestamped Log Files
==============================================

Creates timestamped log files for each pipeline run.
Logs: logs/rag_processing_YYYYMMDD_HHMMSS.log

One log file per run + console output.
"""

import logging
import os
from datetime import datetime
from typing import Dict, Any


def setup_logging(log_dir: str = "logs", log_level: str = "INFO") -> str:
    """
    Set up logging with timestamped log file
    
    Returns: Path to log file
    """
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    log_filename = f"rag_processing_{timestamp}.log"
    log_path = os.path.join(log_dir, log_filename)
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(log_path, mode='w', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("="*80)
    logger.info("MULTIMODAL RAG EXTRACTION PIPELINE")
    logger.info(f"Log file: {log_path}")
    logger.info("="*80)
    
    return log_path


def log_config(config: Dict[str, Any]):
    """Log configuration"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*80)
    logger.info("CONFIGURATION")
    logger.info("="*80)
    
    rag = config.get('rag', {})
    logger.info(f"RAG Mode:              {rag.get('mode', 'text').upper()}")
    logger.info(f"Top-K:                 {rag.get('top_k', 3)}")
    logger.info(f"Similarity Threshold:  {rag.get('similarity_threshold', 0.70)}")
    
    if rag.get('mode') == 'multimodal':
        logger.info(f"Text Weight:           {rag.get('text_weight', 0.70)}")
        logger.info(f"Visual Weight:         {rag.get('visual_weight', 0.30)}")
    
    fields = config.get('fields', [])
    logger.info(f"Fields:                {len(fields)} - {', '.join(fields[:3])}...")
    logger.info(f"Confidence Threshold:  {config.get('confidence_threshold', 0.90)}")
    logger.info("="*80)


def log_provider_start(provider: str, doc_count: int, index: str):
    """Log provider start"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*80)
    logger.info(f"PROVIDER: {provider}")
    logger.info("="*80)
    logger.info(f"Documents:  {doc_count}")
    logger.info(f"Index:      {index}")
    logger.info("="*80)


def log_document(doc_num: int, total: int, name: str):
    """Log document start"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("-"*80)
    logger.info(f"DOC {doc_num}/{total}: {name}")
    logger.info("-"*80)


def log_ocr(chars: int, pages: int = 1):
    """Log OCR result"""
    logger = logging.getLogger(__name__)
    logger.info(f"OCR: {chars} chars, {pages} page(s)")


def log_vision(success: bool, chars: int = 0, error: str = ""):
    """Log vision result"""
    logger = logging.getLogger(__name__)
    if success:
        logger.info(f"Vision: SUCCESS ({chars} chars)")
    else:
        logger.warning(f"Vision: FAILED - {error}")


def log_rag(used: bool, similar: int, mode: str, vision: bool):
    """Log RAG details"""
    logger = logging.getLogger(__name__)
    logger.info(f"RAG: {used} | Similar docs: {similar} | Mode: {mode} | Vision: {vision}")


def log_extraction(fields: Dict[str, Any], conf: float):
    """Log extraction results"""
    logger = logging.getLogger(__name__)
    logger.info(f"Extracted: {len(fields)} fields | Avg confidence: {conf:.4f}")
    
    for name, data in fields.items():
        if isinstance(data, dict):
            val = str(data.get('value', ''))[:50]
            c = data.get('confidence', 0)
            logger.info(f"  {name:20s} = {val:50s} (conf: {c:.4f})")


def log_error(doc: str, err: str):
    """Log error"""
    logger = logging.getLogger(__name__)
    logger.error(f"ERROR [{doc}]: {err}")


def log_provider_summary(prov: str, done: int, total: int, cost: float):
    """Log provider summary"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*80)
    logger.info(f"SUMMARY: {prov}")
    logger.info("="*80)
    logger.info(f"Processed:  {done}/{total}")
    logger.info(f"Success:    {(done/total*100) if total > 0 else 0:.1f}%")
    logger.info(f"Total Cost: ${cost:.4f}")
    logger.info(f"Per Doc:    ${(cost/done) if done > 0 else 0:.4f}")
    logger.info("="*80)


def log_outputs(csv: str, json: str, costs: str):
    """Log output files"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("Outputs:")
    logger.info(f"  CSV:   {csv}")
    logger.info(f"  JSON:  {json}")
    logger.info(f"  Costs: {costs}")


def log_complete(providers: int, docs: int):
    """Log pipeline complete"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*80)
    logger.info("COMPLETE")
    logger.info("="*80)
    logger.info(f"Providers: {providers}")
    logger.info(f"Documents: {docs}")
    logger.info("="*80)


-----

prompt.py

"""
FIELD LIBRARY - Dynamic Field Configuration
============================================

Users edit this file to add field extraction hints.
Fields themselves are defined in config.json - this is OPTIONAL hints only.

Add any field you want â€” system is fully dynamic.
"""

FIELD_LIBRARY = {
    # â”€â”€ Identity Document Fields â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    "name": {
        "description": "Full legal name as appears on document",
        "extraction_hint": "Usually at top, may be in CAPS or title case",
        "validation": "First and last name required"
    },
    
    "passport_number": {
        "description": "Passport identification number",
        "extraction_hint": "Alphanumeric code, often starts with letter. Check MRZ zone at bottom of passport.",
        "validation": "Format varies by country (e.g., US: 9 chars, UK: 9 chars)"
    },
    
    "date_of_birth": {
        "description": "Date of birth",
        "extraction_hint": "Format varies: DD/MM/YYYY, MM/DD/YYYY, or YYYY-MM-DD. Check both main page and MRZ.",
        "validation": "Valid date, person must be alive, reasonable age"
    },
    
    "document_number": {
        "description": "Document number (ID card number, driver's license number)",
        "extraction_hint": "May differ from passport number. Look for 'Document No.' or 'ID No.' label.",
        "validation": "Alphanumeric, length varies by document type"
    },
    
    "nationality": {
        "description": "Nationality / citizenship",
        "extraction_hint": "3-letter country code (e.g., USA, GBR, IND) or full country name",
        "validation": "Must be valid country code or name"
    },
    
    "issue_date": {
        "description": "Document issue date",
        "extraction_hint": "Date when document was issued. Format similar to DOB.",
        "validation": "Must be before expiry_date, not in future"
    },
    
    "expiry_date": {
        "description": "Document expiration date",
        "extraction_hint": "Date when document expires. Check MRZ zone for coded date.",
        "validation": "Must be after issue_date"
    },
    
    # â”€â”€ Healthcare / CMS-1500 Fields (Example) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Uncomment and add these if processing CMS-1500 forms
    
    # "provider_npi": {
    #     "description": "National Provider Identifier",
    #     "extraction_hint": "10-digit number in Box 33a of CMS-1500 form",
    #     "validation": "Exactly 10 digits"
    # },
    
    # "patient_name": {
    #     "description": "Patient full name",
    #     "extraction_hint": "Box 2 on CMS-1500 form: Last Name, First Name, Middle Initial",
    #     "validation": "Last name required"
    # },
    
    # "patient_dob": {
    #     "description": "Patient date of birth",
    #     "extraction_hint": "Box 3 on CMS-1500 form, format MM DD YYYY",
    #     "validation": "Valid date"
    # },
    
    # "diagnosis_code": {
    #     "description": "ICD-10 diagnosis code",
    #     "extraction_hint": "Box 21 on CMS-1500 form, alphanumeric code",
    #     "validation": "Valid ICD-10 format"
    # },
    
    # "service_date": {
    #     "description": "Date of service",
    #     "extraction_hint": "Box 24A on CMS-1500 form",
    #     "validation": "Valid date, not in future"
    # },
    
    # â”€â”€ Add Your Own Fields Here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # System is fully dynamic - just add new entries!
    
    # "custom_field_name": {
    #     "description": "What this field contains",
    #     "extraction_hint": "Where to find it, format hints",
    #     "validation": "Format or validation rules"
    # },
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPORTANT NOTES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
1. Fields are defined in config.json "fields" array
   This FIELD_LIBRARY is OPTIONAL extraction hints only

2. To add a new field:
   - Add to config.json "fields" array
   - (Optional) Add hints here in FIELD_LIBRARY

3. System is fully dynamic - no code changes needed

4. Extraction hints help GPT-4o find fields more accurately
   but are not required

5. Example adding a new field:

   In config.json:
   "fields": ["name", "passport_number", "my_new_field"]

   In this file (optional):
   "my_new_field": {
       "description": "Description of field",
       "extraction_hint": "Where to find it",
       "validation": "Format rules"
   }

6. For healthcare CMS-1500 forms, uncomment the healthcare
   section above and add to config.json fields array
"""

----


config.json

{
  "AzureBlob": {
    "connection_string": "YOUR_BLOB_CONNECTION_STRING",
    "inputcontainer": "inputcontainer",
    "outputcontainer": "outputcontainer"
  },

  "AzureOpenAI": {
    "endpoint": "https://YOUR-GPT.openai.azure.com/",
    "api_key": "YOUR_GPT_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "gpt-4o"
  },

  "AzureEmbedding": {
    "endpoint": "https://YOUR-EMBEDDING.openai.azure.com/",
    "api_key": "YOUR_EMBEDDING_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "text-embedding-3-large",
    "dimension": 3072
  },

  "DocumentIntelligence": {
    "endpoint": "https://YOUR_DOC_INTEL.cognitiveservices.azure.com/",
    "key": "YOUR_DOC_INTEL_KEY"
  },

  "AzureAISearch": {
    "endpoint": "https://YOUR_SEARCH_SERVICE.search.windows.net",
    "api_key": "YOUR_SEARCH_KEY"
  },

  "fields": [
    "name",
    "passport_number",
    "date_of_birth",
    "document_number",
    "nationality",
    "issue_date",
    "expiry_date"
  ],

  "confidence_threshold": 0.90,

  "RAG": {
    "mode": "text",
    "top_k": 3,
    "similarity_threshold": 0.70,
    "text_weight": 0.70,
    "visual_weight": 0.30
  },

  "costs": {
    "gpt4o_input_per_1k": 0.0025,
    "gpt4o_output_per_1k": 0.01,
    "embedding_per_1k": 0.00013,
    "doc_intel_per_page": 0.01
  },

  "logging": {
    "log_dir": "logs",
    "log_level": "INFO"
  }
}

  
