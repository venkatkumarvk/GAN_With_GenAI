import importlib  # Add this line

---

 def get_document_config(config, doc_type, api_type):
    """
    Get configuration for a specific document type and API type.
    
    Args:
        config: Full configuration dictionary
        doc_type: Document type ('invoice', 'eob', 'claim')
        api_type: API type ('general', 'batch')
    
    Returns:
        tuple: (extraction_fields, systemprompt, prompt_template, model_config)
    """
    document_types = config["processing"]["document_types"]
    
    if doc_type not in document_types:
        raise ValueError(f"Unsupported document type: {doc_type}. Available types: {list(document_types.keys())}")
    
    doc_config = document_types[doc_type]
    extraction_fields = doc_config["extraction_fields"]
    prompt_module_name = doc_config["prompt_module"]
    
    # Get model configuration based on API type
    if api_type == "general":
        azure_openai_config = config["azure_openai_general"]
    elif api_type == "batch":
        azure_openai_config = config["azure_openai_batch"]
    else:
        raise ValueError(f"Unsupported API type: {api_type}. Use 'general' or 'batch'")
    
    # Check if document type has specific model configuration
    if doc_type in azure_openai_config.get("models", {}):
        model_config = azure_openai_config["models"][doc_type].copy()
    else:
        # Use default configuration
        model_config = azure_openai_config.get("default", {}).copy()
    
    # Dynamically import the prompt module
    try:
        prompt_module = importlib.import_module(prompt_module_name)
        
        # Get systemprompt and prompt from the module
        if not hasattr(prompt_module, 'systemprompt'):
            raise AttributeError(f"Module '{prompt_module_name}' missing 'systemprompt' variable")
        if not hasattr(prompt_module, 'prompt'):
            raise AttributeError(f"Module '{prompt_module_name}' missing 'prompt' variable")
            
        systemprompt = prompt_module.systemprompt
        prompt_template = prompt_module.prompt
        
    except ImportError as e:
        raise ImportError(f"Could not import prompt module '{prompt_module_name}': {str(e)}")
    except AttributeError as e:
        raise AttributeError(f"Error accessing prompt variables: {str(e)}")
    
    return extraction_fields, systemprompt, prompt_template, model_config

def create_formatted_prompt(prompt_template, extraction_fields):
    """
    Format the prompt template with extraction fields.
    
    Args:
        prompt_template: The prompt template string
        extraction_fields: List of fields to extract
    
    Returns:
        str: Formatted prompt
    """
    fields_str = ", ".join(extraction_fields)
    return prompt_template.format(extraction_fields=fields_str)


  ----
def process_azure_pdf_files(config, api_type, azure_folder, doc_type, logger):
---
 # Get document-specific configuration including model config
    extraction_fields, systemprompt, prompt_template, model_config = get_document_config(config, doc_type, api_type)
    logger.info(f"Processing {doc_type} documents with fields: {extraction_fields}")
    logger.info(f"Using prompt module: {config['processing']['document_types'][doc_type]['prompt_module']}")
    logger.info(f"Using Azure OpenAI {api_type} API configuration:")
    logger.info(f"  Endpoint: {model_config.get('azure_endpoint')}")
    logger.info(f"  Deployment: {model_config.get('deployment_name')}")
    logger.info(f"  API Version: {model_config.get('api_version')}")
    logger.info(f"  Model Name: {model_config.get('modelname')}")
    
    # Initialize helpers with archive container
    archive_container = config["azure_storage"].get("input_archive_container")
    logger.info(f"Initializing Azure Storage Helper with containers:")
    logger.info(f"  Input: {config['azure_storage']['input_container']}")
    logger.info(f"  Output: {config['azure_storage']['output_container']}")
    logger.info(f"  Archive: {archive_container}")
    
    storage_helper = AzureStorageHelper(
        config["azure_storage"]["connection_string"],
        config["azure_storage"]["input_container"],
        config["azure_storage"]["output_container"],
        archive_container,
        logger
    )
    
    # Pass document type and extraction fields to PDFProcessor
    pdf_processor = PDFProcessor(config, logger, doc_type, extraction_fields)
    
    # Initialize AI client with document-specific model configuration
    logger.info(f"Initializing Azure OpenAI Client with {api_type} API for {doc_type}")
    ai_client = AzureOpenAIClient(model_config, logger)
  ----

  # Create formatted prompts using the document-specific prompt template
                formatted_prompt = create_formatted_prompt(prompt_template, extraction_fields)
                prompts = [formatted_prompt for _ in range(len(batch_pages))]

----
if api_type == "batch":
                        logger.debug("Using batch API for processing")
                        raw_results = ai_client.process_batch(base64_strings, prompts, systemprompt)
                    else:
                        logger.debug("Using general API for processing")
                        raw_results = ai_client.process_general(base64_strings, prompts, systemprompt)


  ---
  # Upload CSV to blob storage
csv_content, primary_field, secondary_field = pdf_processor.create_csv_for_results(

csv_blob_name = f"{folder_path}{doc_type}_{base_filename}_{primary_field}_{secondary_field}_{timestamp}.csv"
  ----
  # Upload original PDF to appropriate folder
                source_folder = f"source_documents/{doc_type}/" + folder_path

  ---
  parser.add_argument("--doctype", choices=["invoice", "eob", "claim"], required=True,
                      help="Document type to process (invoice, eob, or claim)")

  ---
  # # Validate document type exists in config
        if args.doctype not in config["processing"]["document_types"]:
            available_types = list(config["processing"]["document_types"].keys())
            raise ValueError(f"Document type '{args.doctype}' not found in config. Available types: {available_types}")
---
logger.info("Processing complete!")
    logger.info(f"Summary:")
    logger.info(f"  Document type: {doc_type}")
    logger.info(f"  API type: {api_type}")
    logger.info(f"  Total files processed: {len(pdf_blobs)}")
  ---
if args.source == "azure":
            process_azure_pdf_files(config, args.apitype, args.folder, args.doctype, logger)
  ---

----


class PDFProcessor:
    def __init__(self, config, logger, doc_type=None, extraction_fields=None):
        self.config = config
        self.logger = logger
        self.doc_type = doc_type or "invoice"
        self.extraction_fields = extraction_fields or config["processing"].get("extraction_fields", ["invoice_number", "total_amount", "date", "vendor"])
        
        # Log the processor configuration
        self.logger.info(f"PDFProcessor initialized for document type: {self.doc_type}")
        self.logger.info(f"Extraction fields: {self.extraction_fields}")
-----
def create_csv_for_results(self, results, filename):
        """
        Create CSV content from extraction results, adapting to different document types.
        
        Returns:
            tuple: (csv_content, primary_field_value, secondary_field_value)
        """
        if not results:
            return None, "unknown", "unknown"
        
        try:
            # Create CSV header based on extraction fields
            csv_lines = []
            header = ["filename", "page_number", "document_type"] + self.extraction_fields + ["confidence"]
            csv_lines.append(",".join(header))
            
            # Initialize field values for filename generation
            primary_field_value = "unknown"
            secondary_field_value = "unknown"
            
            # Process each result
            for page_num, category, extracted_data in results:
                row = [filename, str(page_num + 1), self.doc_type]
                
                # Extract field values based on document type
                if extracted_data and isinstance(extracted_data, dict):
                    extractions = extracted_data.get("extractions", {})
                    
                    # Add field values to row
                    for field in self.extraction_fields:
                        field_data = extractions.get(field, {})
                        value = field_data.get("value", "N/A") if isinstance(field_data, dict) else str(field_data)
                        row.append(f'"{value}"')  # Quote values to handle commas
                        
                        # Set primary and secondary field values for filename
                        if field == self.extraction_fields[0] and value != "N/A":  # First field as primary
                            primary_field_value = str(value).replace('"', '').replace(',', '_')
                        elif len(self.extraction_fields) > 1 and field == self.extraction_fields[1] and value != "N/A":  # Second field as secondary
                            secondary_field_value = str(value).replace('"', '').replace(',', '_')
                    
                    # Add overall confidence
                    confidence = extracted_data.get("overall_confidence", 0)
                    row.append(str(confidence))
                else:
                    # If no extracted data, fill with N/A
                    row.extend(["N/A"] * len(self.extraction_fields))
                    row.append("0")
                
                csv_lines.append(",".join(row))
            
            csv_content = "\n".join(csv_lines)
            self.logger.debug(f"Created CSV with {len(results)} rows for {self.doc_type} document")
            
            return csv_content, primary_field_value, secondary_field_value
            
        except Exception as e:
            self.logger.error(f"Error creating CSV: {str(e)}")
            return None, "error", "error"

---

  class AzureOpenAIClient:
    def __init__(self, model_config, logger):
        """
        Initialize Azure OpenAI client with model-specific configuration.
        
        Args:
            model_config: Dictionary containing model-specific configuration
            logger: Logger instance
        """
        self.logger = logger
        
        # Extract configuration from model_config
        self.api_key = model_config.get("api_key")
        self.azure_endpoint = model_config.get("azure_endpoint")
        self.api_version = model_config.get("api_version")
        self.deployment_name = model_config.get("deployment_name")
        self.modelname = model_config.get("modelname")
        self.api_type = model_config.get("api_type")
        
        # Validate required configuration
        required_fields = ["api_key", "azure_endpoint", "api_version", "deployment_name"]
        missing = [field for field in required_fields if not model_config.get(field)]
        
        if missing:
            raise ValueError(f"Missing required Azure OpenAI configuration: {missing}")
        
        # Initialize the Azure OpenAI client
        try:
            from openai import AzureOpenAI
            
            self.client = AzureOpenAI(
                api_key=self.api_key,
                api_version=self.api_version,
                azure_endpoint=self.azure_endpoint
            )
            
            self.logger.info(f"Initialized Azure OpenAI client:")
            self.logger.info(f"  Endpoint: {self.azure_endpoint}")
            self.logger.info(f"  Deployment: {self.deployment_name}")
            self.logger.info(f"  API Version: {self.api_version}")
            self.logger.info(f"  Model Name: {self.modelname}")
            self.logger.info(f"  API Type: {self.api_type}")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize Azure OpenAI client: {str(e)}")
            raise

---
def process_general(self, base64_strings, prompts, systemprompt=None):
---
  for i, (base64_string, user_prompt) in enumerate(zip(base64_strings, prompts)):
            try:
                self.logger.debug(f"Processing image {i+1}/{len(base64_strings)}")
                
                # Prepare messages
                messages = []
                
                # Add system message if provided
                if systemprompt:
                    messages.append({
                        "role": "system",
                        "content": systemprompt
                    })
                
                # Add user message with image
                user_content = [
                    {
                        "type": "text",
                        "text": user_prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_string}"
                        }
                    }
                ]
                
                messages.append({
                    "role": "user",
                    "content": user_content
                })
                
                # Make API call
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=messages,
                    max_tokens=2000,
                    temperature=0.1
                )
                
                result = response.choices[0].message.content
                results.append(result)
                
                self.logger.debug(f"Successfully processed image {i+1}")
                
            except Exception as e:
                self.logger.error(f"Error processing image {i+1}: {str(e)}")
                results.append(None)
  ---
def process_batch(self, base64_strings, prompts, systemprompt=None):
--
class PDFProcessor:
    def __init__(self, config, logger, doc_type=None, extraction_fields=None):
        self.config = config
        self.logger = logger
        self.doc_type = doc_type or "invoice"
        self.extraction_fields = extraction_fields or config["processing"].get("extraction_fields", ["invoice_number", "total_amount", "date", "vendor"])
        
        # Log the processor configuration
        self.logger.info(f"PDFProcessor initialized for document type: {self.doc_type}")
        self.logger.info(f"Extraction fields: {self.extraction_fields}")

--
  def create_csv_for_results(self, results, filename):
        """
        Create CSV content from extraction results, adapting to different document types.
        
        Returns:
            tuple: (csv_content, primary_field_value, secondary_field_value)
        """
        if not results:
            return None, "unknown", "unknown"
        
        try:
            # Create CSV header based on extraction fields
            csv_lines = []
            header = ["filename", "page_number", "document_type"] + self.extraction_fields + ["confidence"]
            csv_lines.append(",".join(header))
            
            # Initialize field values for filename generation
            primary_field_value = "unknown"
            secondary_field_value = "unknown"
            
            # Process each result
            for page_num, category, extracted_data in results:
                row = [filename, str(page_num + 1), self.doc_type]
                
                # Extract field values based on document type
                if extracted_data and isinstance(extracted_data, dict):
                    extractions = extracted_data.get("extractions", {})
                    
                    # Add field values to row
                    for field in self.extraction_fields:
                        field_data = extractions.get(field, {})
                        value = field_data.get("value", "N/A") if isinstance(field_data, dict) else str(field_data)
                        row.append(f'"{value}"')  # Quote values to handle commas
                        
                        # Set primary and secondary field values for filename
                        if field == self.extraction_fields[0] and value != "N/A":  # First field as primary
                            primary_field_value = str(value).replace('"', '').replace(',', '_')
                        elif len(self.extraction_fields) > 1 and field == self.extraction_fields[1] and value != "N/A":  # Second field as secondary
                            secondary_field_value = str(value).replace('"', '').replace(',', '_')
                    
                    # Add overall confidence
                    confidence = extracted_data.get("overall_confidence", 0)
                    row.append(str(confidence))
                else:
                    # If no extracted data, fill with N/A
                    row.extend(["N/A"] * len(self.extraction_fields))
                    row.append("0")
                
                csv_lines.append(",".join(row))
            
            csv_content = "\n".join(csv_lines)
            self.logger.debug(f"Created CSV with {len(results)} rows for {self.doc_type} document")
            
            return csv_content, primary_field_value, secondary_field_value
            
        except Exception as e:
            self.logger.error(f"Error creating CSV: {str(e)}")
            return None, "error", "error"
