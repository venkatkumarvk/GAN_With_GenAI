text.py

  """
TEXT RAG - Text-Only Document Extraction
=========================================

OCR text is PRIMARY source.
Uses Azure AI Search vector search to find similar past documents.
Passes them as few-shot examples to GPT-4o for extraction.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import json
import logging
from typing import List, Dict, Any, Optional

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class TextRAGExtractor:
    """
    Text-Only RAG Extractor

    Flow:
        Document â†’ OCR (primary) â†’ Embed â†’ Search Similar â†’ GPT-4o Extract

    Mode: Fast, cost-effective, 92-95% accuracy
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(f"TextRAGExtractor ready | top_k={top_k} | threshold={similarity_threshold}")
        print(f"    âœ“ RAG Mode : TEXT-ONLY")
        print(f"    âœ“ Vision   : Disabled")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PUBLIC: Extract with RAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def extract_with_rag(
        self,
        document_text: str,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Text RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, embeddings
            }
        """
        logger.info(f"Text RAG extraction | doc={source_document} | provider={provider}")

        # Step 1 â€” Generate text embedding (OCR is primary)
        embedding = self.openai_manager.generate_embeddings(document_text)
        logger.info("  âœ“ Text embedding generated")

        # Step 2 â€” Search similar documents
        similar_docs = self._search_similar(embedding, provider, index_name)

        # Step 3 â€” Build prompt
        if similar_docs:
            print(f"    âœ“ RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  âœ“ {len(similar_docs)} similar docs retrieved")
            user_prompt = self._build_rag_prompt(document_text, similar_docs)
        else:
            print(f"    âš  RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  âš  No similar docs â€” standard extraction")
            user_prompt = self._build_standard_prompt(document_text)

        # Step 4 â€” GPT-4o extraction
        extracted_fields = self._call_gpt(user_prompt, document_type)
        
        # Step 5 â€” Store in Azure AI Search for future RAG
        confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        self._store_in_index(
            index_name=index_name,
            document_name=source_document,
            provider=provider,
            document_text=document_text,
            text_embedding=text_emb,
            extracted_fields=extracted_fields,
            avg_confidence=avg_confidence
        )

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':        False,
            'visual_description': '',
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding   # combined = text in text-only mode
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        document_type: Optional[str] = None,
        source_document: str = '',
        provider: str = '',
        index_name: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG â€” used for first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            document_type:   Document type hint
            source_document: Source filename
            provider:        Provider name (for storage)
            index_name:      Index name (for storage)

        Returns:
            Same shape as extract_with_rag
        """
        logger.info(f"Text extraction (no RAG) | doc={source_document}")

        embedding   = self.openai_manager.generate_embeddings(document_text)
        user_prompt = self._build_standard_prompt(document_text)
        extracted_fields = self._call_gpt(user_prompt, document_type)
        
        # Store in index for future RAG (if provider/index provided)
        if provider and index_name:
            confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            self._store_in_index(
                index_name=index_name,
                document_name=source_document,
                provider=provider,
                document_text=document_text,
                text_embedding=embedding,
                extracted_fields=extracted_fields,
                avg_confidence=avg_confidence
            )

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          False,
            'similar_docs_count': 0,
            'has_vision':        False,
            'visual_description': '',
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding
            }
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PRIVATE HELPERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _store_in_index(
        self,
        index_name: str,
        document_name: str,
        provider: str,
        document_text: str,
        text_embedding: List[float],
        extracted_fields: Dict[str, Any],
        avg_confidence: float
    ):
        """
        Store document in Azure AI Search index for future RAG
        """
        try:
            from azure.search.documents.indexes import SearchIndexClient
            from azure.search.documents.indexes.models import (
                SearchIndex, SearchField, SearchFieldDataType,
                VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile
            )
            
            # Create index if not exists
            index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=self.search_credential
            )
            
            try:
                index_client.get_index(index_name)
                logger.info(f"  Index '{index_name}' exists")
            except:
                logger.info(f"  Creating index '{index_name}'...")
                
                fields = [
                    SearchField(name="id", type=SearchFieldDataType.String, key=True, filterable=True),
                    SearchField(name="document_name", type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SearchField(name="provider", type=SearchFieldDataType.String, filterable=True),
                    SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="extracted_fields", type=SearchFieldDataType.String),
                    SearchField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
                    SearchField(
                        name="content_vector",
                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        vector_search_dimensions=3072,
                        vector_search_profile_name="text-profile"
                    )
                ]
                
                vector_search = VectorSearch(
                    algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
                    profiles=[VectorSearchProfile(name="text-profile", algorithm_configuration_name="hnsw-config")]
                )
                
                index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
                index_client.create_index(index)
                logger.info(f"  âœ“ Index '{index_name}' created")
            
            # Upload document
            doc_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            doc_id = f"{provider}_{document_name}".replace('.', '_').replace('/', '_')[:512]
            
            document = {
                "id": doc_id,
                "document_name": document_name,
                "provider": provider,
                "content": document_text[:50000],
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": avg_confidence,
                "content_vector": text_embedding
            }
            
            doc_client.upload_documents([document])
            logger.info(f"  âœ“ Stored in index: {doc_id}")
            
        except Exception as e:
            logger.error(f"Failed to store in index: {e}", exc_info=True)

    def _search_similar(
        self,
        embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search for similar documents"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=["document_name", "content", "extracted_fields", "avg_confidence"]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _build_rag_prompt(
        self,
        document_text: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build user prompt with RAG few-shot examples"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass
            parts.append("")

        parts.append("â”€" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")
        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:8000])
        parts.append("")
        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(self, document_text: str) -> str:
        """Build standard prompt without RAG context"""
        return (
            f"Extract the following fields from this document:\n\n"
            f"OCR TEXT (Primary Source):\n{document_text[:8000]}\n\n"
            f"Fields to extract: {', '.join(self.fields)}\n\n"
            "Return ONLY a JSON object with field values and confidence scores."
        )

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> Dict[str, Any]:
        """Call GPT-4o and return parsed extraction result"""
        try:
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            # Track tokens
            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content)

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}


------


  multimodal.py

  """
MULTIMODAL RAG - Vision + Text Document Extraction
====================================================

OCR text is PRIMARY source.
Vision (GPT-4 Vision) is SUPPLEMENTARY enhancement.
Combined embedding (text + visual) improves RAG similarity search.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import base64
import json
import logging
from typing import Dict, Any, List, Optional, Tuple

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class MultimodalRAGExtractor:
    """
    Multimodal RAG Extractor â€” Text + Vision

    Flow:
        Document â†’ OCR (primary)       â†’ Text Embed   â”€â”
                 â†’ Vision (supplement) â†’ Visual Embed â”€â”¤ â†’ Combined â†’ Search â†’ GPT-4o
                                                       â”€â”˜

    Mode: Higher accuracy (95-97%), 100% coverage (handles image-only docs)
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        text_weight: float = 0.7,
        visual_weight: float = 0.3
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
            text_weight:          Weight for text embedding in combined vector (default 0.7)
            visual_weight:        Weight for visual embedding in combined vector (default 0.3)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold
        self.text_weight          = text_weight
        self.visual_weight        = visual_weight

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(
            f"MultimodalRAGExtractor ready | "
            f"top_k={top_k} | text_w={text_weight} | visual_w={visual_weight}"
        )
        print(f"    âœ“ RAG Mode : MULTIMODAL (Text + Vision)")
        print(f"    âœ“ Vision   : Enabled")
        print(f"    âœ“ Weights  : text={text_weight} | visual={visual_weight}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PUBLIC: Extract with RAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def extract_with_rag(
        self,
        document_text: str,
        image_bytes: bytes,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Multimodal RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            image_bytes:     Raw image bytes for vision analysis
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, has_vision,
              visual_description, embeddings
            }
        """
        logger.info(f"Multimodal RAG | doc={source_document} | provider={provider}")

        # Step 1 â€” Vision analysis (supplementary)
        visual_description = ''
        if image_bytes:
            print(f"    ðŸ” Vision analysis (supplementary)...")
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']
                print(f"    âœ“ Vision       : {len(visual_description)} chars")
                logger.info(f"  âœ“ Vision complete ({len(visual_description)} chars)")
            else:
                print(f"    âš  Vision failed â€” OCR only (primary)")
                logger.warning(f"  âš  Vision failed: {result.get('error')}")
        else:
            print(f"    âš  No image bytes â€” OCR only (primary)")

        # Step 2 â€” Generate embeddings
        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )
        emb_mode = "text + visual" if visual_description else "text only (vision unavailable)"
        print(f"    âœ“ Embeddings   : {emb_mode}")
        logger.info(f"  âœ“ Embeddings generated: {emb_mode}")

        # Step 3 â€” Search similar documents
        similar_docs = self._search_similar(combined_emb, provider, index_name)

        # Step 4 â€” Build prompt
        if similar_docs:
            print(f"    âœ“ RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  âœ“ {len(similar_docs)} similar docs retrieved")
            user_prompt = self._build_rag_prompt(document_text, visual_description, similar_docs)
        else:
            print(f"    âš  RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  âš  No similar docs â€” standard extraction")
            user_prompt = self._build_standard_prompt(document_text, visual_description)

        # Step 5 â€” GPT-4o extraction
        extracted_fields = self._call_gpt(user_prompt, document_type)
        
        # Step 6 â€” Store in Azure AI Search for future RAG
        confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        self._store_in_index(
            index_name=index_name,
            document_name=source_document,
            provider=provider,
            document_text=document_text,
            visual_description=visual_description,
            combined_embedding=combined_emb,
            extracted_fields=extracted_fields,
            avg_confidence=avg_confidence
        )

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        image_bytes: Optional[bytes] = None,
        document_type: Optional[str] = None,
        source_document: str = '',
        provider: str = '',
        index_name: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG â€” first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            image_bytes:     Image bytes for vision (optional)
            document_type:   Document type hint
            source_document: Source filename
            provider:        Provider name (for storage)
            index_name:      Index name (for storage)
        """
        logger.info(f"Multimodal extraction (no RAG) | doc={source_document}")

        visual_description = ''
        if image_bytes:
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']

        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )

        user_prompt      = self._build_standard_prompt(document_text, visual_description)
        extracted_fields = self._call_gpt(user_prompt, document_type)
        
        # Store in index for future RAG (if provider/index provided)
        if provider and index_name:
            confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            self._store_in_index(
                index_name=index_name,
                document_name=source_document,
                provider=provider,
                document_text=document_text,
                visual_description=visual_description,
                combined_embedding=combined_emb,
                extracted_fields=extracted_fields,
                avg_confidence=avg_confidence
            )

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           False,
            'similar_docs_count': 0,
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PRIVATE HELPERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _analyze_vision(
        self,
        image_bytes: bytes,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """Analyze document image using GPT-4 Vision"""
        try:
            b64 = base64.b64encode(image_bytes).decode('utf-8')

            type_hints = {
                'passport': "\n\nFocus on: MRZ zone, country emblem, biographical page layout.",
                'license':  "\n\nFocus on: License number position, photo, endorsements.",
                'id_card':  "\n\nFocus on: ID number placement, official seals, front/back layout."
            }
            doc_hint = type_hints.get(document_type, '') if document_type else ''

            prompt = (
                "Analyze this identity document image and describe:\n"
                "1. Document type and layout\n"
                "2. Security features (holograms, watermarks, seals)\n"
                "3. Document condition and image quality\n"
                "4. Photo position and quality\n"
                "5. Any notable visual elements or anomalies\n\n"
                "Keep description concise (max 400 chars) â€” it supplements OCR text."
                + doc_hint
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.deployment_name,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url":    f"data:image/jpeg;base64,{b64}",
                                "detail": "high"
                            }
                        }
                    ]
                }],
                max_tokens=500,
                temperature=0.0
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return {
                'success':            True,
                'visual_description': response.choices[0].message.content
            }

        except Exception as e:
            logger.error(f"Vision analysis failed: {e}", exc_info=True)
            return {'success': False, 'visual_description': '', 'error': str(e)}

    def _generate_embeddings(
        self,
        document_text: str,
        visual_description: str
    ) -> Tuple[List[float], Optional[List[float]], List[float]]:
        """
        Generate text, visual, and combined embeddings

        Returns:
            (text_embedding, visual_embedding_or_None, combined_embedding)
        """
        text_emb = self.openai_manager.generate_embeddings(document_text)

        if visual_description:
            visual_emb   = self.openai_manager.generate_embeddings(visual_description)
            combined_emb = [
                self.text_weight * text_emb[i] + self.visual_weight * visual_emb[i]
                for i in range(len(text_emb))
            ]
            return text_emb, visual_emb, combined_emb
        else:
            return text_emb, None, text_emb   # fallback: combined = text

    def _search_similar(
        self,
        combined_embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search using combined embedding"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": combined_embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=[
                    "document_name", "content",
                    "extracted_fields", "visual_description", "avg_confidence"
                ]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _store_in_index(
        self,
        index_name: str,
        document_name: str,
        provider: str,
        document_text: str,
        visual_description: str,
        combined_embedding: List[float],
        extracted_fields: Dict[str, Any],
        avg_confidence: float
    ):
        """
        Store document in Azure AI Search index for future RAG
        
        Creates index if doesn't exist, then uploads document
        """
        try:
            from azure.search.documents.indexes import SearchIndexClient
            from azure.search.documents.indexes.models import (
                SearchIndex, SearchField, SearchFieldDataType,
                VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile
            )
            
            # Create index if not exists
            index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=self.search_credential
            )
            
            try:
                index_client.get_index(index_name)
                logger.info(f"  Index '{index_name}' exists")
            except:
                logger.info(f"  Creating index '{index_name}'...")
                
                fields = [
                    SearchField(name="id", type=SearchFieldDataType.String, key=True, filterable=True),
                    SearchField(name="document_name", type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SearchField(name="provider", type=SearchFieldDataType.String, filterable=True),
                    SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="visual_description", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="extracted_fields", type=SearchFieldDataType.String),
                    SearchField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
                    SearchField(
                        name="content_vector",
                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        vector_search_dimensions=3072,
                        vector_search_profile_name="multimodal-profile"
                    )
                ]
                
                vector_search = VectorSearch(
                    algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
                    profiles=[VectorSearchProfile(name="multimodal-profile", algorithm_configuration_name="hnsw-config")]
                )
                
                index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
                index_client.create_index(index)
                logger.info(f"  âœ“ Index '{index_name}' created")
            
            # Upload document
            doc_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            doc_id = f"{provider}_{document_name}".replace('.', '_').replace('/', '_')[:512]
            
            document = {
                "id": doc_id,
                "document_name": document_name,
                "provider": provider,
                "content": document_text[:50000],  # Limit size
                "visual_description": visual_description,
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": avg_confidence,
                "content_vector": combined_embedding
            }
            
            doc_client.upload_documents([document])
            logger.info(f"  âœ“ Stored in index: {doc_id}")
            
        except Exception as e:
            logger.error(f"Failed to store in index: {e}", exc_info=True)

    def _build_rag_prompt(
        self,
        document_text: str,
        visual_description: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build prompt with RAG few-shot examples + visual context"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass

            visual_ctx = doc.get('visual_description', '')
            if visual_ctx:
                parts.append(f"Visual   : {visual_ctx[:150]}")

            parts.append("")

        parts.append("â”€" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")

        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:7000])
        parts.append("")

        if visual_description:
            parts.append("VISUAL ANALYSIS (Supplementary):")
            parts.append(visual_description)
            parts.append("")

        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(
        self,
        document_text: str,
        visual_description: str
    ) -> str:
        """Standard prompt (no RAG) with optional visual context"""
        parts = [
            "Extract the following fields from this document:",
            "",
            "OCR TEXT (Primary Source):",
            document_text[:7000],
            ""
        ]
        if visual_description:
            parts += ["VISUAL ANALYSIS (Supplementary):", visual_description, ""]

        parts += [
            f"Fields to extract: {', '.join(self.fields)}",
            "Return ONLY a JSON object with field values and confidence scores."
        ]
        return "\n".join(parts)

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> Dict[str, Any]:
        """Call GPT-4o and return parsed extraction"""
        try:
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )
            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content)

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}


          -------

          unified.py

          """
UNIFIED RAG - Router
=====================

Reads config.json and initializes the correct RAG extractor:
  - mode = "text"       â†’ TextRAGExtractor       (text_rag.py)
  - mode = "multimodal" â†’ MultimodalRAGExtractor  (multimodal_rag.py)

This file just routes â€” all logic lives in text_rag.py / multimodal_rag.py

DO NOT EDIT - Stable code
Users configure via config.json:

    "rag": {
        "mode": "text",           â† "text" or "multimodal"
        "use_vision": false,      â† true only when mode = "multimodal"
        "top_k": 3,
        "similarity_threshold": 0.7,
        "text_weight": 0.7,       â† only used in multimodal mode
        "visual_weight": 0.3      â† only used in multimodal mode
    }
"""

import logging
from typing import List, Dict, Any

from text_rag       import TextRAGExtractor
from multimodal_rag import MultimodalRAGExtractor

logger = logging.getLogger(__name__)


def create_rag_extractor(
    rag_config: Dict[str, Any],
    search_endpoint: str,
    search_api_key: str,
    openai_manager,
    fields: List[str]
):
    """
    Factory function â€” reads config and returns the correct RAG extractor

    Args:
        rag_config:       config['rag'] dictionary from config.json
        search_endpoint:  Azure AI Search endpoint
        search_api_key:   Azure AI Search API key
        openai_manager:   AzureOpenAIManager instance
        fields:           List of fields to extract

    Returns:
        TextRAGExtractor  OR  MultimodalRAGExtractor
    """
    mode       = rag_config.get('mode', 'text').lower()
    top_k      = rag_config.get('top_k', 3)
    threshold  = rag_config.get('similarity_threshold', 0.7)

    print(f"\n{'='*60}")
    print(f"  RAG EXTRACTOR INITIALIZING")
    print(f"{'='*60}")
    print(f"  Mode      : {mode.upper()}")

    # â”€â”€ TEXT mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if mode == 'text':
        print(f"  File      : text_rag.py â†’ TextRAGExtractor")
        logger.info(f"RAG mode: TEXT | top_k={top_k} | threshold={threshold}")

        return TextRAGExtractor(
            search_endpoint      = search_endpoint,
            search_api_key       = search_api_key,
            openai_manager       = openai_manager,
            fields               = fields,
            top_k                = top_k,
            similarity_threshold = threshold
        )

    # â”€â”€ MULTIMODAL mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif mode == 'multimodal':
        text_weight   = rag_config.get('text_weight',   0.7)
        visual_weight = rag_config.get('visual_weight', 0.3)

        print(f"  File      : multimodal_rag.py â†’ MultimodalRAGExtractor")
        print(f"  Weights   : text={text_weight} | visual={visual_weight}")
        logger.info(
            f"RAG mode: MULTIMODAL | top_k={top_k} | "
            f"text_w={text_weight} | visual_w={visual_weight}"
        )

        return MultimodalRAGExtractor(
            search_endpoint      = search_endpoint,
            search_api_key       = search_api_key,
            openai_manager       = openai_manager,
            fields               = fields,
            top_k                = top_k,
            similarity_threshold = threshold,
            text_weight          = text_weight,
            visual_weight        = visual_weight
        )

    # â”€â”€ Unknown mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    else:
        logger.warning(f"Unknown RAG mode '{mode}' â€” falling back to TEXT mode")
        print(f"  âš  Unknown mode '{mode}' â€” falling back to TEXT mode")

        return TextRAGExtractor(
            search_endpoint      = search_endpoint,
            search_api_key       = search_api_key,
            openai_manager       = openai_manager,
            fields               = fields,
            top_k                = top_k,
            similarity_threshold = threshold
        )


-------

  prompt.py


  """
FIELD LIBRARY - Dynamic Field Configuration
============================================

Users edit this file to add field extraction hints.
Fields themselves are defined in config.json - this is OPTIONAL hints only.

Add any field you want â€” system is fully dynamic.
"""

FIELD_LIBRARY = {
    # â”€â”€ Identity Document Fields â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    "name": {
        "description": "Full legal name as appears on document",
        "extraction_hint": "Usually at top, may be in CAPS or title case",
        "validation": "First and last name required"
    },
    
    "passport_number": {
        "description": "Passport identification number",
        "extraction_hint": "Alphanumeric code, often starts with letter. Check MRZ zone at bottom of passport.",
        "validation": "Format varies by country (e.g., US: 9 chars, UK: 9 chars)"
    },
    
    "date_of_birth": {
        "description": "Date of birth",
        "extraction_hint": "Format varies: DD/MM/YYYY, MM/DD/YYYY, or YYYY-MM-DD. Check both main page and MRZ.",
        "validation": "Valid date, person must be alive, reasonable age"
    },
    
    "document_number": {
        "description": "Document number (ID card number, driver's license number)",
        "extraction_hint": "May differ from passport number. Look for 'Document No.' or 'ID No.' label.",
        "validation": "Alphanumeric, length varies by document type"
    },
    
    "nationality": {
        "description": "Nationality / citizenship",
        "extraction_hint": "3-letter country code (e.g., USA, GBR, IND) or full country name",
        "validation": "Must be valid country code or name"
    },
    
    "issue_date": {
        "description": "Document issue date",
        "extraction_hint": "Date when document was issued. Format similar to DOB.",
        "validation": "Must be before expiry_date, not in future"
    },
    
    "expiry_date": {
        "description": "Document expiration date",
        "extraction_hint": "Date when document expires. Check MRZ zone for coded date.",
        "validation": "Must be after issue_date"
    },
    
    # â”€â”€ Healthcare / CMS-1500 Fields (Example) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Uncomment and add these if processing CMS-1500 forms
    
    # "provider_npi": {
    #     "description": "National Provider Identifier",
    #     "extraction_hint": "10-digit number in Box 33a of CMS-1500 form",
    #     "validation": "Exactly 10 digits"
    # },
    
    # "patient_name": {
    #     "description": "Patient full name",
    #     "extraction_hint": "Box 2 on CMS-1500 form: Last Name, First Name, Middle Initial",
    #     "validation": "Last name required"
    # },
    
    # "patient_dob": {
    #     "description": "Patient date of birth",
    #     "extraction_hint": "Box 3 on CMS-1500 form, format MM DD YYYY",
    #     "validation": "Valid date"
    # },
    
    # "diagnosis_code": {
    #     "description": "ICD-10 diagnosis code",
    #     "extraction_hint": "Box 21 on CMS-1500 form, alphanumeric code",
    #     "validation": "Valid ICD-10 format"
    # },
    
    # "service_date": {
    #     "description": "Date of service",
    #     "extraction_hint": "Box 24A on CMS-1500 form",
    #     "validation": "Valid date, not in future"
    # },
    
    # â”€â”€ Add Your Own Fields Here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # System is fully dynamic - just add new entries!
    
    # "custom_field_name": {
    #     "description": "What this field contains",
    #     "extraction_hint": "Where to find it, format hints",
    #     "validation": "Format or validation rules"
    # },
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPORTANT NOTES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
1. Fields are defined in config.json "fields" array
   This FIELD_LIBRARY is OPTIONAL extraction hints only

2. To add a new field:
   - Add to config.json "fields" array
   - (Optional) Add hints here in FIELD_LIBRARY

3. System is fully dynamic - no code changes needed

4. Extraction hints help GPT-4o find fields more accurately
   but are not required

5. Example adding a new field:

   In config.json:
   "fields": ["name", "passport_number", "my_new_field"]

   In this file (optional):
   "my_new_field": {
       "description": "Description of field",
       "extraction_hint": "Where to find it",
       "validation": "Format rules"
   }

6. For healthcare CMS-1500 forms, uncomment the healthcare
   section above and add to config.json fields array
"""


  --------------

  prompt_builder.py

  """
PROMPT BUILDER - System Prompt Construction
============================================

Builds extraction prompts using field configuration.
Reads from prompt_config.py for field hints.
"""

from typing import List, Dict, Any, Optional
from prompt_config import FIELD_LIBRARY


class ExtractionPromptBuilder:
    """Builds GPT-4o extraction prompts"""
    
    def __init__(self, fields: List[str]):
        """
        Args:
            fields: List of field names to extract (from config.json)
        """
        self.fields = fields
    
    def build_system_prompt(self, document_type: Optional[str] = None) -> str:
        """
        Build system prompt for GPT-4o extraction
        
        Args:
            document_type: Optional document type hint ('passport', 'license', 'id_card', 'cms1500')
        
        Returns:
            Complete system prompt string
        """
        
        prompt_parts = []
        
        # Base instruction
        prompt_parts.append(
            "You are an expert document data extraction system specialized in identity documents and healthcare forms.\n\n"
            "Your task is to extract specific fields from document text with high accuracy.\n"
        )
        
        # Document type context
        if document_type:
            type_hints = {
                'passport': (
                    "This is a PASSPORT document. "
                    "Pay special attention to the MRZ (Machine Readable Zone) at the bottom, "
                    "which contains encoded information. "
                    "Look for holograms, watermarks, and official seals for authenticity."
                ),
                'license': (
                    "This is a DRIVER'S LICENSE. "
                    "Check for state-specific formats, endorsements section, and restrictions. "
                    "Look for security features like holograms."
                ),
                'id_card': (
                    "This is an IDENTITY CARD (national ID or government-issued ID). "
                    "Look for official seals, government emblems, and security features. "
                    "May have front and back sides."
                ),
                'cms1500': (
                    "This is a CMS-1500 HEALTHCARE CLAIM FORM. "
                    "Fields are in numbered boxes (Box 1, Box 2, etc.). "
                    "Extract exactly as written, preserving codes and formats."
                )
            }
            
            if document_type.lower() in type_hints:
                prompt_parts.append(type_hints[document_type.lower()] + "\n")
        
        # Field extraction instructions
        prompt_parts.append("FIELDS TO EXTRACT:\n")
        
        for field in self.fields:
            if field in FIELD_LIBRARY:
                field_info = FIELD_LIBRARY[field]
                prompt_parts.append(
                    f"- {field}: {field_info['description']}\n"
                    f"  Hint: {field_info['extraction_hint']}\n"
                    f"  Validation: {field_info['validation']}\n"
                )
            else:
                # Field not in library - generic instruction
                prompt_parts.append(
                    f"- {field}: Extract this field value from the document\n"
                )
        
        # Output format instructions
        prompt_parts.append("\n" + "="*60 + "\n")
        prompt_parts.append("OUTPUT FORMAT:\n\n")
        prompt_parts.append(
            "Return ONLY a JSON object with this exact structure:\n\n"
            "{\n"
        )
        
        for field in self.fields:
            prompt_parts.append(
                f'  "{field}": {{\n'
                f'    "value": "extracted value here",\n'
                f'    "confidence": 0.95\n'
                f'  }},\n'
            )
        
        prompt_parts.append("}\n\n")
        
        # Confidence scoring guidelines
        prompt_parts.append("CONFIDENCE SCORING:\n")
        prompt_parts.append(
            "- 0.95-1.00: Perfectly clear, unambiguous text\n"
            "- 0.85-0.94: Clear but minor uncertainty (OCR artifacts, slight blur)\n"
            "- 0.70-0.84: Readable but degraded quality or partial occlusion\n"
            "- 0.50-0.69: Difficult to read, significant uncertainty\n"
            "- 0.00-0.49: Extremely unclear or likely incorrect\n\n"
        )
        
        # Critical rules
        prompt_parts.append("CRITICAL RULES:\n")
        prompt_parts.append(
            "1. NEVER hallucinate or guess. If a field is not found, set value to empty string.\n"
            "2. Extract EXACTLY as written in document - do not reformat unless necessary.\n"
            "3. For dates: Preserve original format if clear, otherwise use YYYY-MM-DD.\n"
            "4. For codes (passport numbers, IDs): Preserve exact alphanumeric sequence.\n"
            "5. Return ONLY the JSON object - no preamble, no explanation, no markdown.\n"
            "6. Confidence reflects extraction certainty, not field importance.\n"
            "7. If multiple values found for same field, use the clearest/most official one.\n"
        )
        
        return "".join(prompt_parts)
    
    def build_rag_user_prompt(
        self,
        document_text: str,
        visual_description: str = "",
        similar_examples: List[Dict[str, Any]] = None
    ) -> str:
        """
        Build user prompt with RAG examples
        
        Args:
            document_text: OCR text from current document
            visual_description: GPT-4o vision analysis (if multimodal)
            similar_examples: List of similar past extractions
        
        Returns:
            User prompt with examples
        """
        
        prompt_parts = []
        
        # Add similar examples if provided
        if similar_examples:
            prompt_parts.append(
                f"Here are {len(similar_examples)} similar documents with their extractions "
                f"as reference examples:\n\n"
            )
            
            for idx, example in enumerate(similar_examples, 1):
                similarity = example.get('@search.score', 0.0)
                doc_name = example.get('document_name', f'Example {idx}')
                extracted = example.get('extracted_fields', '{}')
                
                prompt_parts.append(f"EXAMPLE {idx} (similarity: {similarity:.2f})\n")
                prompt_parts.append(f"Document: {doc_name}\n")
                prompt_parts.append(f"Extraction:\n{extracted}\n\n")
            
            prompt_parts.append("="*60 + "\n\n")
        
        # Current document to extract
        prompt_parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:\n\n")
        
        # OCR text (PRIMARY source)
        prompt_parts.append("OCR TEXT (Primary Source):\n")
        prompt_parts.append(document_text[:8000])  # Limit to 8000 chars
        prompt_parts.append("\n\n")
        
        # Visual description (SUPPLEMENTARY, if multimodal)
        if visual_description:
            prompt_parts.append("VISUAL ANALYSIS (Supplementary):\n")
            prompt_parts.append(visual_description)
            prompt_parts.append("\n\n")
        
        # Remind of output format
        prompt_parts.append(f"Extract these fields: {', '.join(self.fields)}\n")
        prompt_parts.append("Return ONLY the JSON object with field values and confidence scores.\n")
        
        return "".join(prompt_parts)


  ------------------

  costtracking.py

  """
COST TRACKING MODULE
====================

Handles all cost tracking and reporting for document extraction pipeline

Tracks costs for:
- Azure OpenAI (GPT-4o)
- Azure Document Intelligence (OCR)
- Azure OpenAI Embeddings
- Azure AI Search (storage)
"""

import json
import logging
from typing import Dict, Any
from datetime import datetime

logger = logging.getLogger(__name__)


class CostTracker:
    """
    Comprehensive cost tracking for document extraction pipeline
    """
    
    def __init__(self, costs_config: Dict[str, float]):
        """
        Initialize cost tracker with pricing configuration
        
        Args:
            costs_config: Dictionary with pricing info from config.json
        """
        self.costs_config = costs_config
        
        # Default pricing (can be overridden by config)
        self.default_pricing = {
            'gpt4o_input_per_1k': 0.0025,   # $0.0025 per 1K input tokens
            'gpt4o_output_per_1k': 0.01,    # $0.01 per 1K output tokens
            'doc_intel_per_page': 0.01,     # $0.01 per page
            'embedding_per_1k': 0.00013,    # $0.00013 per 1K tokens
            'search_storage_per_gb': 0.10   # $0.10 per GB per month
        }
        
        # Merge with provided config
        for key, value in self.default_pricing.items():
            if key not in self.costs_config:
                self.costs_config[key] = value
    
    def calculate_gpt4o_cost(
        self, 
        prompt_tokens: int, 
        completion_tokens: int
    ) -> Dict[str, float]:
        """
        Calculate GPT-4o API costs
        
        Args:
            prompt_tokens: Number of input tokens
            completion_tokens: Number of output tokens
        
        Returns:
            Dictionary with input_cost, output_cost, total_cost
        """
        input_cost = (prompt_tokens / 1000) * self.costs_config['gpt4o_input_per_1k']
        output_cost = (completion_tokens / 1000) * self.costs_config['gpt4o_output_per_1k']
        total_cost = input_cost + output_cost
        
        return {
            'input_cost': round(input_cost, 4),
            'output_cost': round(output_cost, 4),
            'total_cost': round(total_cost, 4),
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens
        }
    
    def calculate_ocr_cost(
        self, 
        total_documents: int, 
        avg_pages_per_doc: float = 2.0
    ) -> Dict[str, float]:
        """
        Calculate Azure Document Intelligence (OCR) costs
        
        Args:
            total_documents: Number of documents processed
            avg_pages_per_doc: Average pages per document
        
        Returns:
            Dictionary with total_pages, cost_per_page, total_cost
        """
        total_pages = total_documents * avg_pages_per_doc
        cost_per_page = self.costs_config['doc_intel_per_page']
        total_cost = total_pages * cost_per_page
        
        return {
            'total_documents': total_documents,
            'total_pages': int(total_pages),
            'cost_per_page': cost_per_page,
            'total_cost': round(total_cost, 4)
        }
    
    def calculate_embedding_cost(
        self, 
        total_tokens: int
    ) -> Dict[str, float]:
        """
        Calculate embedding API costs
        
        Args:
            total_tokens: Total tokens embedded
        
        Returns:
            Dictionary with total_tokens, cost_per_1k, total_cost
        """
        cost_per_1k = self.costs_config['embedding_per_1k']
        total_cost = (total_tokens / 1000) * cost_per_1k
        
        return {
            'total_tokens': total_tokens,
            'cost_per_1k': cost_per_1k,
            'total_cost': round(total_cost, 4)
        }
    
    def calculate_search_storage_cost(
        self, 
        total_documents: int,
        avg_doc_size_kb: float = 10.0,
        months: int = 1
    ) -> Dict[str, float]:
        """
        Estimate Azure AI Search storage costs
        
        Args:
            total_documents: Number of documents stored
            avg_doc_size_kb: Average document size in KB
            months: Number of months to estimate
        
        Returns:
            Dictionary with storage_gb, monthly_cost, total_cost
        """
        # Calculate storage (text + vector)
        # Vector: 3072 dimensions Ã— 4 bytes = 12KB per document
        total_kb = total_documents * (avg_doc_size_kb + 12)
        storage_gb = total_kb / (1024 * 1024)
        
        monthly_cost = storage_gb * self.costs_config['search_storage_per_gb']
        total_cost = monthly_cost * months
        
        return {
            'total_documents': total_documents,
            'storage_gb': round(storage_gb, 4),
            'monthly_cost': round(monthly_cost, 4),
            'total_cost': round(total_cost, 4)
        }
    
    def calculate_provider_costs(
        self,
        provider: str,
        provider_id: str,
        total_documents: int,
        token_usage: Dict[str, int],
        avg_pages_per_doc: float = 2.0
    ) -> Dict[str, Any]:
        """
        Calculate all costs for a provider
        
        Args:
            provider: Provider name
            provider_id: Provider ID with timestamp
            total_documents: Number of documents processed
            token_usage: Dictionary with prompt_tokens, completion_tokens, total_tokens
            avg_pages_per_doc: Average pages per document
        
        Returns:
            Complete cost breakdown dictionary
        """
        # GPT-4o costs
        gpt4o_cost = self.calculate_gpt4o_cost(
            token_usage.get('prompt_tokens', 0),
            token_usage.get('completion_tokens', 0)
        )
        
        # OCR costs
        ocr_cost = self.calculate_ocr_cost(total_documents, avg_pages_per_doc)
        
        # Embedding costs
        embedding_cost = self.calculate_embedding_cost(
            token_usage.get('total_tokens', 0)
        )
        
        # Search storage costs (estimate 1 month)
        search_cost = self.calculate_search_storage_cost(total_documents)
        
        # Total costs
        total_estimated = (
            gpt4o_cost['total_cost'] +
            ocr_cost['total_cost'] +
            embedding_cost['total_cost'] +
            search_cost['monthly_cost']
        )
        
        return {
            'provider': provider,
            'provider_id': provider_id,
            'timestamp': datetime.utcnow().isoformat(),
            'total_documents': total_documents,
            'costs': {
                'gpt4o': gpt4o_cost,
                'ocr': ocr_cost,
                'embedding': embedding_cost,
                'search_storage': search_cost,
                'total_estimated': round(total_estimated, 4)
            },
            'usage': token_usage,
            'pricing_config': self.costs_config
        }


def save_provider_costs(
    provider: str,
    provider_id: str,
    total_documents: int,
    openai_manager,
    costs_config: Dict[str, float],
    blob_manager,
    avg_pages_per_doc: float = 2.0
):
    """
    Calculate and save provider-specific cost tracking
    
    Args:
        provider: Provider name
        provider_id: Provider ID with timestamp
        total_documents: Number of documents processed
        openai_manager: AzureOpenAIManager instance
        costs_config: Cost configuration from config.json
        blob_manager: AzureBlobManager instance
        avg_pages_per_doc: Average pages per document
    """
    try:
        # Initialize cost tracker
        cost_tracker = CostTracker(costs_config)
        
        # Get token usage from OpenAI manager
        token_usage = openai_manager.get_token_usage()
        
        # Calculate all costs
        cost_data = cost_tracker.calculate_provider_costs(
            provider=provider,
            provider_id=provider_id,
            total_documents=total_documents,
            token_usage=token_usage,
            avg_pages_per_doc=avg_pages_per_doc
        )
        
        # Save to blob storage
        cost_path = f"CostTracking/{provider_id}_costs.json"
        blob_manager.upload_to_blob(
            json.dumps(cost_data, indent=2),
            cost_path,
            'application/json'
        )
        
        print(f"    âœ“ Cost tracking saved: {cost_path}")
        print(f"      Total estimated cost: ${cost_data['costs']['total_estimated']:.4f}")
        
        logger.info(f"Cost tracking saved for {provider}: ${cost_data['costs']['total_estimated']:.4f}")
        
        return cost_data
        
    except Exception as e:
        print(f"    âœ— Cost tracking failed: {e}")
        logger.error(f"Cost tracking error for {provider}: {e}", exc_info=True)
        return None


def calculate_global_costs(
    openai_manager,
    costs_config: Dict[str, float],
    total_providers: int,
    total_documents: int,
    avg_pages_per_doc: float = 2.0
) -> Dict[str, Any]:
    """
    Calculate global costs for entire run
    
    Args:
        openai_manager: AzureOpenAIManager instance
        costs_config: Cost configuration
        total_providers: Number of providers processed
        total_documents: Total documents across all providers
        avg_pages_per_doc: Average pages per document
    
    Returns:
        Global cost summary
    """
    cost_tracker = CostTracker(costs_config)
    token_usage = openai_manager.get_token_usage()
    
    # Calculate costs
    gpt4o_cost = cost_tracker.calculate_gpt4o_cost(
        token_usage.get('prompt_tokens', 0),
        token_usage.get('completion_tokens', 0)
    )
    
    ocr_cost = cost_tracker.calculate_ocr_cost(
        total_documents,
        avg_pages_per_doc
    )
    
    embedding_cost = cost_tracker.calculate_embedding_cost(
        token_usage.get('total_tokens', 0)
    )
    
    search_cost = cost_tracker.calculate_search_storage_cost(
        total_documents
    )
    
    total_cost = (
        gpt4o_cost['total_cost'] +
        ocr_cost['total_cost'] +
        embedding_cost['total_cost'] +
        search_cost['monthly_cost']
    )
    
    return {
        'total_providers': total_providers,
        'total_documents': total_documents,
        'total_cost': round(total_cost, 4),
        'cost_breakdown': {
            'gpt4o': gpt4o_cost,
            'ocr': ocr_cost,
            'embedding': embedding_cost,
            'search_storage': search_cost
        },
        'usage': token_usage,
        'cost_per_document': round(total_cost / total_documents, 4) if total_documents > 0 else 0
    }


def print_cost_summary(cost_data: Dict[str, Any]):
    """
    Print formatted cost summary
    
    Args:
        cost_data: Cost data dictionary
    """
    print(f"\n{'='*70}")
    print("COST SUMMARY")
    print(f"{'='*70}")
    
    if 'provider' in cost_data:
        # Provider-specific summary
        print(f"Provider:               {cost_data['provider']}")
        print(f"Provider ID:            {cost_data['provider_id']}")
        print(f"Documents Processed:    {cost_data['total_documents']}")
        print(f"\nCost Breakdown:")
        print(f"  GPT-4o:              ${cost_data['costs']['gpt4o']['total_cost']:.4f}")
        print(f"  OCR:                 ${cost_data['costs']['ocr']['total_cost']:.4f}")
        print(f"  Embeddings:          ${cost_data['costs']['embedding']['total_cost']:.4f}")
        print(f"  Search Storage:      ${cost_data['costs']['search_storage']['monthly_cost']:.4f}/month")
        print(f"\nTotal Estimated:       ${cost_data['costs']['total_estimated']:.4f}")
    else:
        # Global summary
        print(f"Total Providers:        {cost_data.get('total_providers', 0)}")
        print(f"Total Documents:        {cost_data.get('total_documents', 0)}")
        print(f"\nCost Breakdown:")
        costs = cost_data.get('cost_breakdown', {})
        print(f"  GPT-4o:              ${costs.get('gpt4o', {}).get('total_cost', 0):.4f}")
        print(f"  OCR:                 ${costs.get('ocr', {}).get('total_cost', 0):.4f}")
        print(f"  Embeddings:          ${costs.get('embedding', {}).get('total_cost', 0):.4f}")
        print(f"  Search Storage:      ${costs.get('search_storage', {}).get('monthly_cost', 0):.4f}/month")
        print(f"\nTotal Cost:            ${cost_data.get('total_cost', 0):.4f}")
        print(f"Cost per Document:     ${cost_data.get('cost_per_document', 0):.4f}")
    
    print(f"{'='*70}\n")


# Example usage
if __name__ == "__main__":
    # Example cost configuration
    costs_config = {
        'gpt4o_input_per_1k': 0.0025,
        'gpt4o_output_per_1k': 0.01,
        'doc_intel_per_page': 0.01,
        'embedding_per_1k': 0.00013,
        'search_storage_per_gb': 0.10
    }
    
    # Create cost tracker
    tracker = CostTracker(costs_config)
    
    # Example: Calculate costs for 10 documents
    token_usage = {
        'prompt_tokens': 50000,
        'completion_tokens': 10000,
        'total_tokens': 60000
    }
    
    costs = tracker.calculate_provider_costs(
        provider='example_provider',
        provider_id='example_20240212_143022',
        total_documents=10,
        token_usage=token_usage
    )
    
    # Print summary
    print_cost_summary(costs)


-------

  logging.py

  """
LOGGING CONFIGURATION - Timestamped Log Files
==============================================

Creates timestamped log files for each pipeline run.
Logs: logs/rag_processing_YYYYMMDD_HHMMSS.log

One log file per run + console output.
"""

import logging
import os
from datetime import datetime
from typing import Dict, Any


def setup_logging(log_dir: str = "logs", log_level: str = "INFO") -> str:
    """
    Set up logging with timestamped log file
    
    Returns: Path to log file
    """
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    log_filename = f"rag_processing_{timestamp}.log"
    log_path = os.path.join(log_dir, log_filename)
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(log_path, mode='w', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("="*80)
    logger.info("MULTIMODAL RAG EXTRACTION PIPELINE")
    logger.info(f"Log file: {log_path}")
    logger.info("="*80)
    
    return log_path


def log_config(config: Dict[str, Any]):
    """Log configuration"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*80)
    logger.info("CONFIGURATION")
    logger.info("="*80)
    
    rag = config.get('rag', {})
    logger.info(f"RAG Mode:              {rag.get('mode', 'text').upper()}")
    logger.info(f"Top-K:                 {rag.get('top_k', 3)}")
    logger.info(f"Similarity Threshold:  {rag.get('similarity_threshold', 0.70)}")
    
    if rag.get('mode') == 'multimodal':
        logger.info(f"Text Weight:           {rag.get('text_weight', 0.70)}")
        logger.info(f"Visual Weight:         {rag.get('visual_weight', 0.30)}")
    
    fields = config.get('fields', [])
    logger.info(f"Fields:                {len(fields)} - {', '.join(fields[:3])}...")
    logger.info(f"Confidence Threshold:  {config.get('confidence_threshold', 0.90)}")
    logger.info("="*80)


def log_provider_start(provider: str, doc_count: int, index: str):
    """Log provider start"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*80)
    logger.info(f"PROVIDER: {provider}")
    logger.info("="*80)
    logger.info(f"Documents:  {doc_count}")
    logger.info(f"Index:      {index}")
    logger.info("="*80)


def log_document(doc_num: int, total: int, name: str):
    """Log document start"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("-"*80)
    logger.info(f"DOC {doc_num}/{total}: {name}")
    logger.info("-"*80)


def log_ocr(chars: int, pages: int = 1):
    """Log OCR result"""
    logger = logging.getLogger(__name__)
    logger.info(f"OCR: {chars} chars, {pages} page(s)")


def log_vision(success: bool, chars: int = 0, error: str = ""):
    """Log vision result"""
    logger = logging.getLogger(__name__)
    if success:
        logger.info(f"Vision: SUCCESS ({chars} chars)")
    else:
        logger.warning(f"Vision: FAILED - {error}")


def log_rag(used: bool, similar: int, mode: str, vision: bool):
    """Log RAG details"""
    logger = logging.getLogger(__name__)
    logger.info(f"RAG: {used} | Similar docs: {similar} | Mode: {mode} | Vision: {vision}")


def log_extraction(fields: Dict[str, Any], conf: float):
    """Log extraction results"""
    logger = logging.getLogger(__name__)
    logger.info(f"Extracted: {len(fields)} fields | Avg confidence: {conf:.4f}")
    
    for name, data in fields.items():
        if isinstance(data, dict):
            val = str(data.get('value', ''))[:50]
            c = data.get('confidence', 0)
            logger.info(f"  {name:20s} = {val:50s} (conf: {c:.4f})")


def log_error(doc: str, err: str):
    """Log error"""
    logger = logging.getLogger(__name__)
    logger.error(f"ERROR [{doc}]: {err}")


def log_provider_summary(prov: str, done: int, total: int, cost: float):
    """Log provider summary"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*80)
    logger.info(f"SUMMARY: {prov}")
    logger.info("="*80)
    logger.info(f"Processed:  {done}/{total}")
    logger.info(f"Success:    {(done/total*100) if total > 0 else 0:.1f}%")
    logger.info(f"Total Cost: ${cost:.4f}")
    logger.info(f"Per Doc:    ${(cost/done) if done > 0 else 0:.4f}")
    logger.info("="*80)


def log_outputs(csv: str, json: str, costs: str):
    """Log output files"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("Outputs:")
    logger.info(f"  CSV:   {csv}")
    logger.info(f"  JSON:  {json}")
    logger.info(f"  Costs: {costs}")


def log_complete(providers: int, docs: int):
    """Log pipeline complete"""
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*80)
    logger.info("COMPLETE")
    logger.info("="*80)
    logger.info(f"Providers: {providers}")
    logger.info(f"Documents: {docs}")
    logger.info("="*80)


  
