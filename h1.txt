#config
"pricing": {
  "input_token_cost_per_million": 2.50,
  "output_token_cost_per_million": 10.0,
  "batch_discount_percentage": 50
}

#2. Update the AzureOpenAIClient class in llm.py to track tokens:
class AzureOpenAIClient:
    def __init__(self, config):
        self.api_key = config["azure_openai"]["api_key"]
        self.api_version = config["azure_openai"]["api_version"]
        self.endpoint = config["azure_openai"]["azure_endpoint"]
        self.deployment_name = config["azure_openai"]["deployment_name"]
        self.batch_size = config["processing"]["batch_size"]
        self.timeout = config["processing"]["timeout_seconds"]
        
        # Initialize token tracking
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        
        # Initialize pricing
        self.input_token_cost_per_million = config["pricing"]["input_token_cost_per_million"]
        self.output_token_cost_per_million = config["pricing"]["output_token_cost_per_million"]
        self.batch_discount_percentage = config["pricing"]["batch_discount_percentage"]
        
        self.client = AzureOpenAI(
            api_key=self.api_key,
            api_version=self.api_version,
            azure_endpoint=self.endpoint
        )
    
    def track_tokens(self, response):
        """
        Track token usage from API response
        """
        try:
            # For batch API
            if isinstance(response, dict) and "usage" in response:
                usage = response["usage"]
                self.total_input_tokens += usage.get("prompt_tokens", 0)
                self.total_output_tokens += usage.get("completion_tokens", 0)
            # For individual API calls
            elif hasattr(response, "usage"):
                self.total_input_tokens += response.usage.prompt_tokens
                self.total_output_tokens += response.usage.completion_tokens
        except Exception as e:
            print(f"Could not track tokens: {e}")
    
    def get_cost(self, api_type="general"):
        """
        Calculate the cost of API usage
        """
        input_cost = (self.total_input_tokens / 1000000) * self.input_token_cost_per_million
        output_cost = (self.total_output_tokens / 1000000) * self.output_token_cost_per_million
        total_cost = input_cost + output_cost
        
        if api_type == "batch":
            # Apply batch discount
            total_cost = total_cost * (1 - self.batch_discount_percentage / 100)
        
        return {
            "input_tokens": self.total_input_tokens,
            "output_tokens": self.total_output_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost,
            "discount_applied": api_type == "batch"
        }

#process batch
  def process_batch(self, image_base64_strings, prompts):
    """
    Process images in a batch using the Azure OpenAI batch API.
    Tracks token usage.
    """
    tmp_jsonl_path = None
    
    try:
        # Prepare the JSONL batch file
        jsonl_file = self.prepare_batch_jsonl(image_base64_strings, prompts)
        
        # Create a temporary file for the JSONL content
        with tempfile.NamedTemporaryFile(suffix='.jsonl', delete=False) as tmp_jsonl:
            tmp_jsonl.write(jsonl_file.getvalue())
            tmp_jsonl_path = tmp_jsonl.name
        
        # Upload with explicit file open
        print("Uploading batch file to Azure...")
        with open(tmp_jsonl_path, 'rb') as f:
            file = self.client.files.create(
                file=f,
                purpose="batch"
            )
        
        # We can delete the temp file immediately after upload
        if tmp_jsonl_path:
            os.unlink(tmp_jsonl_path)
            tmp_jsonl_path = None
        
        file_id = file.id
        print(f"File uploaded (ID: {file_id}). Creating batch job...")
        
        # Submit batch job
        batch_response = self.client.batches.create(
            input_file_id=file_id,
            endpoint="/chat/completions",
            completion_window="24h"
        )
        
        batch_id = batch_response.id
        print(f"Batch job created (ID: {batch_id}). Waiting for processing...")
        
        # Track batch job status
        status = "validating"
        start_time = time.time()
        
        while status not in ("completed", "failed", "canceled"):
            time.sleep(10)
            
            # Check for timeout
            if time.time() - start_time > self.timeout:
                print(f"Timeout after {self.timeout} seconds. Canceling batch job.")
                try:
                    self.client.batches.cancel(batch_id)
                except:
                    pass
                raise TimeoutError(f"Batch processing timed out after {self.timeout} seconds")
            
            batch_response = self.client.batches.retrieve(batch_id)
            status = batch_response.status
            status_message = f"{datetime.now()} Batch Id: {batch_id}, Status: {status}"
            print(status_message)
            
            # Track token usage if available
            if hasattr(batch_response, "usage") and batch_response.usage:
                self.track_tokens(batch_response.usage)
        
        # Retrieve results
        output_file_id = batch_response.output_file_id
        
        if not output_file_id:
            output_file_id = batch_response.error_file_id
            if not output_file_id:
                print("No output or error file was produced by the batch job.")
                raise Exception("No output file produced")
        
        print(f"Batch completed. Retrieving results...")
        file_response = self.client.files.content(output_file_id)
        raw_responses = file_response.text.strip().split('\n')
        
        # Try to estimate token usage from responses if not available from API
        for raw_response in raw_responses:
            try:
                json_response = json.loads(raw_response)
                if "response" in json_response and "body" in json_response["response"]:
                    content = json_response["response"]["body"]
                    if isinstance(content, str):
                        content = json.loads(content)
                    
                    if "usage" in content:
                        self.track_tokens(content["usage"])
            except:
                pass
        
        # If we can't get token usage from the API, use rough estimation
        # This is just a fallback and won't be very accurate
        if self.total_input_tokens == 0 and self.total_output_tokens == 0:
            # Rough estimation of tokens: 1 token â‰ˆ 4 chars for English text
            for prompt in prompts:
                self.total_input_tokens += len(prompt) // 4
                # Add 500 tokens per image (very rough estimate)
                self.total_input_tokens += 500
            
            # Estimate output tokens (rough)
            for raw_response in raw_responses:
                if len(raw_response) > 0:
                    self.total_output_tokens += len(raw_response) // 4
        
        return raw_responses
    
    except Exception as e:
        print(f"Error during batch processing: {str(e)}")
        raise
    finally:
        # Ensure temporary file is deleted if it exists
        if tmp_jsonl_path and os.path.exists(tmp_jsonl_path):
            try:
                os.unlink(tmp_jsonl_path)
            except:
                pass
#general
                  def process_general(self, image_base64_strings, prompts):
    """
    Process images using the general (non-batch) API.
    Tracks token usage.
    """
    results = []
    
    for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
        try:
            print(f"Processing image {i+1}/{len(image_base64_strings)}")
            
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_img}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            # Track token usage
            self.track_tokens(response)
            
            if hasattr(response, 'choices') and len(response.choices) > 0:
                content = response.choices[0].message.content
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "response": {
                        "body": {
                            "choices": [
                                {
                                    "message": {
                                        "content": content
                                    }
                                }
                            ]
                        }
                    }
                }))
            else:
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "error": "No response content"
                }))
        
        except Exception as e:
            print(f"Error processing image {i+1}: {str(e)}")
            results.append(json.dumps({
                "custom_id": f"request-{i+1}",
                "error": str(e)
            }))
    
    return results

  #proces
  def process_azure_pdf_files(config, api_type, azure_folder):
    # (existing code)
    
    # At the end of the function, add:
    # Calculate and display the cost
    cost_info = ai_client.get_cost(api_type)
    
    print("\n===== COST SUMMARY =====")
    print(f"Total Input Tokens: {cost_info['input_tokens']:,}")
    print(f"Total Output Tokens: {cost_info['output_tokens']:,}")
    print(f"Input Token Cost: ${cost_info['input_cost']:.4f}")
    print(f"Output Token Cost: ${cost_info['output_cost']:.4f}")
    
    if cost_info['discount_applied']:
        original_cost = cost_info['input_cost'] + cost_info['output_cost']
        discount = config["pricing"]["batch_discount_percentage"]
        print(f"Original Cost: ${original_cost:.4f}")
        print(f"Batch API Discount: {discount}%")
    
    print(f"Total Cost: ${cost_info['total_cost']:.4f}")
    print("=========================")
    
    print("Processing complete!")
