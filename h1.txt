import os
import hashlib
from azure.storage.blob import BlobServiceClient

def upload_reference_to_blob(config):
    """Uploads reference data to Azure Blob Storage."""
    conn_str = config["storage"]["connection_string"]
    container_name = config["storage"]["container_name"]
    reference_dir = config["paths"]["reference_dir"]

    # Create Blob Service Client
    blob_service_client = BlobServiceClient.from_connection_string(conn_str)
    container_client = blob_service_client.get_container_client(container_name)

    # Create container if not exists
    try:
        container_client.create_container()
    except Exception:
        pass  # Container already exists

    uploaded = False
    # Walk through reference directory
    for root, _, files in os.walk(reference_dir):
        for file in files:
            local_path = os.path.join(root, file)
            
            # Create blob path maintaining directory structure
            blob_path = os.path.relpath(local_path, reference_dir).replace("\\", "/")
            blob_client = container_client.get_blob_client(blob_path)

            # Read file and compute hash
            with open(local_path, "rb") as data:
                blob_data = data.read()
                blob_hash = hashlib.md5(blob_data).hexdigest()
                
                # Check if blob already exists
                exists = False
                try:
                    existing_blob = blob_client.get_blob_properties()
                    existing_hash = existing_blob.get('content_md5', '').hex()
                    exists = (existing_hash == blob_hash)
                except:
                    pass

                # Upload if not exists
                if not exists:
                    blob_client.upload_blob(blob_data, overwrite=True)
                    uploaded = True
                    print(f"Uploaded: {blob_path}")

    if uploaded:
        print("‚úÖ New reference data uploaded to Blob Storage.")
    else:
        print("‚ÑπÔ∏è No new reference files found.")
    
    return uploaded

def split_pdf_to_pages(input_pdf, output_dir):
    """Split PDF into pages and save as temporary files."""
    from PyPDF2 import PdfReader, PdfWriter
    
    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    # Read PDF
    reader = PdfReader(input_pdf)
    total_pages = len(reader.pages)

    # Split and save each page
    for i, page in enumerate(reader.pages, start=1):
        writer = PdfWriter()
        writer.add_page(page)
        
        # Generate page filename
        page_filename = f"{os.path.splitext(os.path.basename(input_pdf))[0]}_page{i}.pdf"
        page_path = os.path.join(output_dir, page_filename)
        
        # Write page to file
        with open(page_path, "wb") as out_file:
            writer.write(out_file)
    
    return total_pages

---

import os
import json
import shutil
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import (
    DocumentIntelligenceClient,
    DocumentIntelligenceAdministrationClient,
)
from azure.ai.documentintelligence.models import (
    BuildDocumentClassifierRequest
)
from azure.storage.blob import BlobServiceClient
from helper import upload_reference_to_blob, split_pdf_to_pages

def train_model(config):
    """
    Trains a custom classifier using Azure Document Intelligence
    with reference data uploaded to Blob Storage.
    """
    # Extract configuration details
    endpoint = config["document_intelligence"]["endpoint"]
    key = config["document_intelligence"]["api_key"]
    project_name = config["document_intelligence"]["project_name"]
    
    # Storage configuration
    connection_string = config["storage"]["connection_string"]
    container_name = config["storage"]["container_name"]
    
    # Create Blob Service Client to verify container
    try:
        blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        container_client = blob_service_client.get_container_client(container_name)
        
        # Verify container exists
        if not container_client.exists():
            print(f"‚ùå Container {container_name} does not exist.")
            return None

        # List blobs to ensure training data is present
        blobs = list(container_client.list_blobs())
        if not blobs:
            print("‚ùå No training data found in the container.")
            return None

        print(f"‚úÖ Found {len(blobs)} blobs in container {container_name}")
    except Exception as storage_error:
        print(f"‚ùå Storage connection error: {storage_error}")
        return None

    # Create Document Intelligence Administration Client
    try:
        admin_client = DocumentIntelligenceAdministrationClient(
            endpoint, AzureKeyCredential(key)
        )

        # Prepare build request
        build_request = BuildDocumentClassifierRequest(
            container_url=container_client.url,
            description=f"Custom document classifier for {project_name}"
        )

        # Start model training
        print("üöÄ Starting model training ...")
        poller = admin_client.begin_build_document_classifier(build_request)
        model = poller.result()

        print(f"‚úÖ Model training completed successfully.")
        print(f"üÜî New model ID: {model.model_id}")

        # Update config with new model ID
        config["document_intelligence"]["model_id"] = model.model_id
        
        # Optional: Save categories as metadata or model description
        categories = config.get("categories", {})
        print("üìã Document Categories:")
        for category, subcategories in categories.items():
            print(f"  - {category}: {subcategories}")

        # Save updated configuration
        with open("config.json", "w") as f:
            json.dump(config, f, indent=4)

        return model.model_id

    except Exception as train_error:
        print(f"‚ùå Model training failed: {train_error}")
        return None

def classify_documents(config):
    """
    Classifies PDFs and images into folders based on model predictions.
    """
    # Extract configuration
    endpoint = config["document_intelligence"]["endpoint"]
    key = config["document_intelligence"]["api_key"]
    model_id = config["document_intelligence"]["model_id"]

    # Path configurations
    input_dir = config["paths"]["input_dir"]
    output_dir = config["paths"]["output_dir"]
    classified_dir = config["paths"]["classified_dir"]
    unclassified_dir = config["paths"]["unclassified_dir"]
    source_dir = config["paths"]["source_dir"]
    confidence_threshold = config["runtime"]["confidence_threshold"]

    # Ensure directories exist
    for path in [output_dir, classified_dir, unclassified_dir, source_dir]:
        os.makedirs(path, exist_ok=True)

    # Create Document Intelligence Client
    client = DocumentIntelligenceClient(endpoint, AzureKeyCredential(key))

    # Process each file in input directory
    for file in os.listdir(input_dir):
        file_path = os.path.join(input_dir, file)
        
        # Skip non-files and unsupported file types
        if not os.path.isfile(file_path):
            continue
        if not file.lower().endswith((".pdf", ".png", ".jpg", ".jpeg")):
            continue

        print(f"üìÑ Processing {file} ...")

        # Copy original file to source directory
        shutil.copy(file_path, os.path.join(source_dir, file))

        # Temporary directory for PDF page processing
        temp_dir = "temp_pages"
        os.makedirs(temp_dir, exist_ok=True)

        # Handle PDF files differently (page-by-page)
        if file.lower().endswith(".pdf"):
            total_pages = split_pdf_to_pages(file_path, temp_dir)

            for page_file in os.listdir(temp_dir):
                page_path = os.path.join(temp_dir, page_file)
                
                # Classify document
                with open(page_path, "rb") as f:
                    poller = client.begin_classify_document(
                        model_id=model_id, document=f
                    )
                result = poller.result()

                # Determine category
                if result.documents and result.documents[0].confidence >= confidence_threshold:
                    category = result.documents[0].doc_type
                else:
                    category = "unclassified"

                # Move to appropriate directory
                dest_folder = (
                    unclassified_dir
                    if category == "unclassified"
                    else os.path.join(classified_dir, category)
                )
                os.makedirs(dest_folder, exist_ok=True)
                shutil.move(page_path, os.path.join(dest_folder, page_file))

                print(f"   ‚Üí {page_file} ‚Üí {category}")

            print(f"‚úÖ {file}: {total_pages} pages classified.")

        # Handle non-PDF files
        else:
            with open(file_path, "rb") as f:
                poller = client.begin_classify_document(model_id=model_id, document=f)
            result = poller.result()

            # Determine category
            if result.documents and result.documents[0].confidence >= confidence_threshold:
                category = result.documents[0].doc_type
            else:
                category = "unclassified"

            # Move to appropriate directory
            dest_folder = (
                unclassified_dir
                if category == "unclassified"
                else os.path.join(classified_dir, category)
            )
            os.makedirs(dest_folder, exist_ok=True)
            shutil.move(file_path, os.path.join(dest_folder, file))

            print(f"‚úÖ {file} classified as {category}")

    print("üéØ Classification complete.")

# Main execution
if __name__ == "__main__":
    # Load configuration
    with open("config.json", "r") as f:
        config = json.load(f)

    # Upload reference data to blob storage
    new_data_uploaded = upload_reference_to_blob(config)

    # Train model if new data is uploaded or auto-retrain is enabled
    if config["runtime"]["auto_retrain"] and new_data_uploaded:
        model_id = train_model(config)
    else:
        model_id = config["document_intelligence"]["model_id"]
        print(f"‚ÑπÔ∏è Using existing model ID: {model_id}")

    # Classify documents
    classify_documents(config)
