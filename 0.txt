import os
import json
import pandas as pd
from datetime import datetime
from helper import ConfigManager, AzureBlobManager, DocumentIntelligenceManager, AzureOpenAIManager

CONFIG_PATH = "config.json"

def main():
    # -----------------------------
    # 1. Load Configuration
    # -----------------------------
    config_manager = ConfigManager(CONFIG_PATH)
    confidence_threshold = float(config_manager.config.get("confidence_threshold", 0.9))
    fields = config_manager.get("fields")
    
    # -----------------------------
    # 2. Initialize Managers
    # -----------------------------
    blob_cfg = config_manager.get("AzureBlob")
    blob_mgr = AzureBlobManager(blob_cfg["connection_string"], blob_cfg["inputcontainer"], blob_cfg["outputcontainer"])
    
    doc_intel_cfg = config_manager.get("DocumentIntelligence")
    doc_mgr = DocumentIntelligenceManager(doc_intel_cfg["endpoint"], doc_intel_cfg["key"])
    
    azure_cfg = config_manager.get("AzureOpenAI")
    embed_cfg = config_manager.get("AzureEmbedding")
    gpt_mgr = AzureOpenAIManager(
        endpoint=azure_cfg["endpoint"],
        api_key=azure_cfg["api_key"],
        api_version=azure_cfg["api_version"],
        deployment_name=azure_cfg["deployment_name"],
        embedding_endpoint=embed_cfg["endpoint"],
        embedding_key=embed_cfg["api_key"],
        embedding_deployment_name=embed_cfg["deployment_name"],
        embedding_dimension=int(embed_cfg.get("dimension", 3072))
    )
    
    # -----------------------------
    # 3. Process Providers
    # -----------------------------
    providers = blob_mgr.get_providers()
    all_results = []

    for provider in providers:
        print(f"Processing provider: {provider}")
        files = blob_mgr.get_provider_files(provider)
        for file_info in files:
            file_name = file_info["name"]
            print(f"  File: {file_name}")
            
            # Download as base64
            base64_data = blob_mgr.download_blob_as_base64(file_name)
            
            # OCR/Document extraction
            ocr_result = doc_mgr.analyze_document(base64_data, file_info["extension"])
            if not ocr_result["success"]:
                print(f"    OCR failed: {ocr_result.get('error')}")
                continue
            
            text_content = ocr_result["text"]
            page_count = ocr_result.get("page_count", 0)
            
            # GPT Field extraction
            extraction = gpt_mgr.extract_fields(text_content, fields, file_name)
            extracted_fields = extraction.get("extracted_fields", {})
            
            # Apply confidence threshold
            high_conf = {}
            low_conf = {}
            for f, v in extracted_fields.items():
                conf = float(v.get("confidence", 0.0))
                if conf >= confidence_threshold:
                    high_conf[f] = v
                else:
                    low_conf[f] = v
            
            # Generate embeddings
            embedding_vector = gpt_mgr.generate_embeddings(text_content)
            
            # Save result per document
            result_record = {
                "provider": provider,
                "file_name": file_name,
                "page_count": page_count,
                "high_confidence_fields": high_conf,
                "low_confidence_fields": low_conf,
                "embedding_vector": embedding_vector
            }
            all_results.append(result_record)
    
    # -----------------------------
    # 4. Save Results
    # -----------------------------
    timestamp = datetime.utcnow().strftime("%Y%m%d%H%M%S")
    
    # High-confidence CSV
    high_conf_records = []
    for rec in all_results:
        for f, v in rec["high_confidence_fields"].items():
            high_conf_records.append({
                "provider": rec["provider"],
                "file_name": rec["file_name"],
                "field": f,
                "value": v.get("value"),
                "confidence": v.get("confidence")
            })
    if high_conf_records:
        df_high = pd.DataFrame(high_conf_records)
        blob_mgr.upload_dataframe_as_csv(df_high, f"high_confidence_{timestamp}.csv")
    
    # Low-confidence CSV
    low_conf_records = []
    for rec in all_results:
        for f, v in rec["low_confidence_fields"].items():
            low_conf_records.append({
                "provider": rec["provider"],
                "file_name": rec["file_name"],
                "field": f,
                "value": v.get("value"),
                "confidence": v.get("confidence")
            })
    if low_conf_records:
        df_low = pd.DataFrame(low_conf_records)
        blob_mgr.upload_dataframe_as_csv(df_low, f"low_confidence_{timestamp}.csv")
    
    # Raw JSON results
    blob_mgr.upload_to_blob(json.dumps(all_results, indent=2), f"all_results_{timestamp}.json", content_type="application/json")
    
    # -----------------------------
    # 5. Cost summary
    # -----------------------------
    token_usage = gpt_mgr.get_token_usage()
    cost_summary = gpt_mgr.calculate_cost(config_manager.get("costs"))
    
    print("\n=== Token Usage ===")
    print(token_usage)
    print("\n=== Estimated Cost ===")
    print(cost_summary)


if __name__ == "__main__":
    main()
