import os
import json
import base64
import logging
import hashlib
from datetime import datetime
from io import BytesIO
from typing import List, Dict, Any, Tuple

import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential

from azure.storage.blob import BlobServiceClient, ContentSettings
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchableField, SearchFieldDataType,
    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,
    SemanticConfiguration, SemanticField, SemanticPrioritizedFields, SemanticSearch
)
from openai import AzureOpenAI

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)


class ConfigManager:
    def __init__(self, config_path: str = 'config.json'):
        self.config_path = config_path
        self.config = self.load_config()

    def load_config(self) -> Dict[str, Any]:
        with open(self.config_path, 'r') as f:
            cfg = json.load(f)
        return cfg

    def get(self, section: str, key: str = None) -> Any:
        if key:
            return self.config.get(section, {}).get(key)
        return self.config.get(section)


class AzureBlobManager:
    def __init__(self, connection_string: str, input_container: str, output_container: str):
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self.input_container = input_container
        self.output_container = output_container
        self.supported_extensions = {'.pdf', '.doc', '.docx', '.png', '.jpg', '.jpeg', '.tiff', '.tif'}

    def get_providers(self) -> List[str]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs()
        providers = set()
        for blob in blobs:
            parts = blob.name.split('/')
            if parts[0]:
                providers.add(parts[0])
        return sorted(list(providers))

    def get_provider_files(self, provider: str) -> List[Dict[str, str]]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs(name_starts_with=f"{provider}/")
        files = []
        for blob in blobs:
            ext = os.path.splitext(blob.name)[1].lower()
            if ext in self.supported_extensions:
                files.append({
                    'name': blob.name,
                    'filename': os.path.basename(blob.name),
                    'provider': provider,
                    'size': blob.size,
                    'extension': ext
                })
        return files

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def download_blob_as_base64(self, blob_name: str) -> str:
        blob_client = self.blob_service_client.get_blob_client(self.input_container, blob_name)
        data = blob_client.download_blob().readall()
        return base64.b64encode(data).decode('utf-8')

    def upload_to_blob(self, data: str, blob_path: str, content_type: str = 'text/plain'):
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        blob_client.upload_blob(data, overwrite=True, content_settings=ContentSettings(content_type=content_type))

    def upload_dataframe_as_csv(self, df: pd.DataFrame, blob_path: str):
        csv_buffer = BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        self.upload_to_blob(csv_buffer.getvalue(), blob_path, 'text/csv')


class DocumentIntelligenceManager:
    def __init__(self, endpoint: str, key: str):
        self.client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def analyze_document(self, base64_data: str, file_extension: str) -> Dict[str, Any]:
        try:
            content_type_map = {
                '.pdf': 'application/pdf',
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.tiff': 'image/tiff',
                '.tif': 'image/tiff',
                '.bmp': 'image/bmp'
            }
            content_type = content_type_map.get(file_extension.lower(), 'application/pdf')
            document_bytes = base64.b64decode(base64_data)
            poller = self.client.begin_analyze_document("prebuilt-read", analyze_request=document_bytes, content_type=content_type)
            result = poller.result()
            text = ''
            for page in getattr(result, 'pages', []):
                for line in getattr(page, 'lines', []):
                    text += getattr(line, 'content', '') + "\n"
            return {'success': True, 'text': text.strip(), 'page_count': len(getattr(result, 'pages', []))}
        except Exception as e:
            logger.error(f"OCR failed: {e}")
            return {'success': False, 'text': '', 'page_count': 0, 'error': str(e)}


class AzureOpenAIManager:
    def __init__(self, endpoint: str, api_key: str, api_version: str, deployment_name: str,
                 embedding_config: Dict[str, Any]):
        self.client = AzureOpenAI(azure_endpoint=endpoint, api_key=api_key, api_version=api_version)
        self.deployment_name = deployment_name
        self.embedding_config = embedding_config
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def extract_fields(self, text: str, fields: List[str], source_document: str) -> Dict[str, Any]:
        system_prompt = f"You are a document extraction expert. Extract fields: {', '.join(fields)}. Return JSON."
        user_prompt = f"Document text:\n\n{text[:8000]}"
        response = self.client.chat.completions.create(
            model=self.deployment_name,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            temperature=0,
            max_tokens=2000
        )
        self.total_tokens += response.usage.total_tokens
        self.prompt_tokens += response.usage.prompt_tokens
        self.completion_tokens += response.usage.completion_tokens

        content = response.choices[0].message.content.strip()
        if content.startswith("```"):
            content = content.strip("```json").strip()
        try:
            data = json.loads(content)
            for k in data:
                if isinstance(data[k], dict):
                    data[k]['source_document'] = source_document
                else:
                    data[k] = {'value': str(data[k]), 'confidence': 0.5, 'source_document': source_document}
            return {'success': True, 'extracted_fields': data, 'raw_response': content}
        except Exception as e:
            logger.error(f"JSON parse failed: {e}")
            return {'success': False, 'extracted_fields': {}, 'raw_response': content}

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def generate_embeddings(self, text: str) -> List[float]:
        try:
            if not text.strip():
                text = "No content available"
            text = text[:30000]
            response = self.client.embeddings.create(model=self.embedding_config['deployment_name'], input=text)
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
            return [0.0] * self.embedding_config.get('dimension', 3072)

    def get_token_usage(self) -> Dict[str, int]:
        return {'total_tokens': self.total_tokens, 'prompt_tokens': self.prompt_tokens, 'completion_tokens': self.completion_tokens}

    def calculate_cost(self, costs_config: Dict[str, float]) -> Dict[str, float]:
        input_cost = (self.prompt_tokens / 1000) * costs_config['gpt4o_input_per_1k']
        output_cost = (self.completion_tokens / 1000) * costs_config['gpt4o_output_per_1k']
        total_cost = input_cost + output_cost
        return {'input_cost': input_cost, 'output_cost': output_cost, 'total_cost': total_cost}


class AzureAISearchManager:
    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.credential = AzureKeyCredential(api_key)
        self.index_client = SearchIndexClient(endpoint=endpoint, credential=self.credential)

    def get_index_name(self, provider: str) -> str:
        sanitized = ''.join(c for c in provider.lower() if c.isalnum() or c == '-')
        return sanitized.strip('-')

    def create_index(self, provider: str, embedding_dimension: int = 3072):
        index_name = self.get_index_name(provider)
        fields = [
            SimpleField(name="id", type=SearchFieldDataType.String, key=True),
            SearchableField(name="content", type=SearchFieldDataType.String),
            SearchableField(name="provider", type=SearchFieldDataType.String, filterable=True),
            SimpleField(name="page_count", type=SearchFieldDataType.Int32, filterable=True),
            SearchField(name="content_vector", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        searchable=True, vector_search_dimensions=embedding_dimension,
                        vector_search_profile_name="vector-profile")
        ]
        vector_search = VectorSearch(
            algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
            profiles=[VectorSearchProfile(name="vector-profile", algorithm_configuration_name="hnsw-config")]
        )
        index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
        self.index_client.create_or_update_index(index)
        return index_name

    def upload_documents(self, provider: str, documents: List[Dict[str, Any]]):
        if not documents:
            return
        index_name = self.get_index_name(provider)
        client = SearchClient(endpoint=self.endpoint, index_name=index_name, credential=self.credential)
        client.upload_documents(documents=documents)



-----

main.py

import os
import uuid
import json
import pandas as pd
from datetime import datetime

from helper import ConfigManager, AzureBlobManager, DocumentIntelligenceManager, AzureOpenAIManager, AzureAISearchManager

# Load configuration
cfg = ConfigManager('config.json')
blob_cfg = cfg.get("AzureBlob")
docint_cfg = cfg.get("DocumentIntelligence")
openai_cfg = cfg.get("AzureOpenAI")
embedding_cfg = cfg.get("AzureEmbedding")
search_cfg = cfg.get("AzureAISearch")
fields = cfg.get("fields")
confidence_threshold = cfg.get("confidence_threshold")
costs_cfg = cfg.get("costs")

# Initialize managers
blob_manager = AzureBlobManager(blob_cfg['connection_string'], blob_cfg['inputcontainer'], blob_cfg['outputcontainer'])
doc_intel_manager = DocumentIntelligenceManager(docint_cfg['endpoint'], docint_cfg['key'])
openai_manager = AzureOpenAIManager(openai_cfg['endpoint'], openai_cfg['api_key'], openai_cfg['api_version'],
                                    openai_cfg['deployment_name'], embedding_cfg)
search_manager = AzureAISearchManager(search_cfg['endpoint'], search_cfg['api_key'])

all_summary_rows = []
final_summary = {
    'total_documents': 0,
    'total_success': 0,
    'total_failure': 0,
    'ocr_success': True,
    'embedding_success': True,
    'vector_store_success': True,
    'total_cost': 0
}

providers = blob_manager.get_providers()
print(f"Found providers: {providers}")

for provider in providers:
    print(f"Processing provider: {provider}")
    files = blob_manager.get_provider_files(provider)
    provider_docs_count = len(files)
    provider_success_count = 0

    # Create vector index for provider
    try:
        search_manager.create_index(provider, embedding_cfg.get('dimension', 3072))
        vector_index_success = True
    except Exception as e:
        print(f"Vector index creation failed for {provider}: {e}")
        vector_index_success = False
        final_summary['vector_store_success'] = False

    for file in files:
        doc_id = str(uuid.uuid4())
        blob_name = file['name']
        filename = file['filename']

        # Download and OCR
        base64_data = blob_manager.download_blob_as_base64(blob_name)
        ocr_result = doc_intel_manager.analyze_document(base64_data, file['extension'])
        if not ocr_result['success']:
            final_summary['ocr_success'] = False
            text_content = ""
        else:
            text_content = ocr_result['text']

        # Field extraction
        extraction_result = openai_manager.extract_fields(text_content, fields, filename)
        extracted_fields = extraction_result.get('extracted_fields', {})

        # Embeddings
        embedding_vector = openai_manager.generate_embeddings(text_content)
        if not embedding_vector:
            final_summary['embedding_success'] = False

        # Prepare row for CSV (single-row per document)
        row_data = {'id': doc_id, 'provider': provider, 'filename': filename,
                    'ocr_success': ocr_result['success'], 'embedding_success': bool(embedding_vector),
                    'vector_index_success': vector_index_success}

        for field_name in fields:
            field_data = extracted_fields.get(field_name, {})
            row_data[f"{field_name}_value"] = field_data.get('value', '')
            row_data[f"{field_name}_confidence"] = field_data.get('confidence', 0)
            row_data[f"{field_name}_source_document"] = field_data.get('source_document', '')

        all_summary_rows.append(row_data)

        # Upload to Azure AI Search
        if vector_index_success and embedding_vector:
            try:
                search_doc = {
                    "id": doc_id,
                    "content": text_content,
                    "provider": provider,
                    "page_count": ocr_result.get('page_count', 0),
                    "content_vector": embedding_vector
                }
                search_manager.upload_documents(provider, [search_doc])
            except Exception as e:
                print(f"Failed to upload vector doc {filename}: {e}")
                final_summary['vector_store_success'] = False

        provider_success_count += 1

    final_summary['total_documents'] += provider_docs_count
    final_summary['total_success'] += provider_success_count
    final_summary['total_failure'] += (provider_docs_count - provider_success_count)

# Save single-row CSV summary
summary_df = pd.DataFrame(all_summary_rows)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
csv_blob_path = f"summary_{timestamp}.csv"
blob_manager.upload_dataframe_as_csv(summary_df, csv_blob_path)
print(f"CSV summary uploaded to blob: {csv_blob_path}")

# Calculate costs
cost_info = openai_manager.calculate_cost(costs_cfg)
final_summary['total_cost'] = cost_info['total_cost']
final_summary['details'] = cost_info

# Save final summary as JSON
final_summary_blob_path = f"final_summary_{timestamp}.json"
blob_manager.upload_to_blob(json.dumps(final_summary, indent=2), final_summary_blob_path)
print(f"Final summary uploaded to blob: {final_summary_blob_path}")

print("Processing complete.")

