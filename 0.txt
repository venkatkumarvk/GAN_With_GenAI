"""
Azure RAG Document Processing - Helper Module
Production-grade utilities for document processing, Azure services integration,
and data management.
"""

import os
import json
import base64
import logging
import time
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from io import BytesIO
import re

# Azure SDK imports
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex,
    SimpleField,
    SearchableField,
    SearchField,
    SearchFieldDataType,
    VectorSearch,
    HnswAlgorithmConfiguration,
    VectorSearchProfile,
    SemanticConfiguration,
    SemanticField,
    SemanticPrioritizedFields,
    SemanticSearch
)
from openai import AzureOpenAI

# Third-party imports
import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('document_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ConfigManager:
    """Manages configuration loading and validation"""
    
    def __init__(self, config_path: str = 'config.json'):
        self.config_path = config_path
        self.config = self.load_config()
        self.validate_config()
    
    def load_config(self) -> Dict[str, Any]:
        """Load configuration from JSON file"""
        try:
            with open(self.config_path, 'r') as f:
                config = json.load(f)
            logger.info(f"Configuration loaded from {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {str(e)}")
            raise
    
    def validate_config(self):
        """Validate required configuration keys"""
        required_sections = ['AzureBlob', 'AzureOpenAI', 'AzureEmbedding', 'DocumentIntelligence', 'AzureAISearch']
        for section in required_sections:
            if section not in self.config:
                raise ValueError(f"Missing required configuration section: {section}")
        logger.info("Configuration validated successfully")
    
    def get(self, section: str, key: str = None) -> Any:
        """Get configuration value"""
        if key:
            return self.config.get(section, {}).get(key)
        return self.config.get(section)


class AzureBlobManager:
    """Manages Azure Blob Storage operations"""
    
    def __init__(self, connection_string: str, input_container: str, output_container: str):
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self.input_container = input_container
        self.output_container = output_container
        self.supported_extensions = {'.pdf', '.doc', '.docx', '.png', '.jpg', '.jpeg', '.tiff', '.tif'}
        logger.info("AzureBlobManager initialized")
    
    def get_providers(self) -> List[str]:
        """Get list of provider folders from input container"""
        try:
            container_client = self.blob_service_client.get_container_client(self.input_container)
            blobs = container_client.list_blobs()
            
            providers = set()
            for blob in blobs:
                parts = blob.name.split('/')
                if len(parts) > 0 and parts[0]:
                    providers.add(parts[0])
            
            provider_list = sorted(list(providers))
            logger.info(f"Found {len(provider_list)} providers: {provider_list}")
            return provider_list
        except Exception as e:
            logger.error(f"Failed to get providers: {str(e)}")
            raise
    
    def get_provider_files(self, provider: str) -> List[Dict[str, str]]:
        """Get all files for a specific provider"""
        try:
            container_client = self.blob_service_client.get_container_client(self.input_container)
            blobs = container_client.list_blobs(name_starts_with=f"{provider}/")
            
            files = []
            for blob in blobs:
                file_ext = os.path.splitext(blob.name)[1].lower()
                if file_ext in self.supported_extensions:
                    files.append({
                        'name': blob.name,
                        'provider': provider,
                        'size': blob.size,
                        'extension': file_ext
                    })
                else:
                    logger.warning(f"Skipping unsupported file type: {blob.name}")
            
            logger.info(f"Found {len(files)} valid files for provider '{provider}'")
            return files
        except Exception as e:
            logger.error(f"Failed to get files for provider '{provider}': {str(e)}")
            return []
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def download_blob_as_base64(self, blob_name: str) -> str:
        """Download blob and convert to base64"""
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=self.input_container,
                blob=blob_name
            )
            blob_data = blob_client.download_blob().readall()
            base64_data = base64.b64encode(blob_data).decode('utf-8')
            logger.debug(f"Downloaded and encoded blob: {blob_name}")
            return base64_data
        except Exception as e:
            logger.error(f"Failed to download blob '{blob_name}': {str(e)}")
            raise
    
    def upload_to_blob(self, data: str, blob_path: str, content_type: str = 'text/plain'):
        """Upload data to blob storage"""
        try:
            from azure.storage.blob import ContentSettings
            
            blob_client = self.blob_service_client.get_blob_client(
                container=self.output_container,
                blob=blob_path
            )
            
            content_settings = ContentSettings(content_type=content_type)
            blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
            logger.info(f"Uploaded to blob: {blob_path}")
        except Exception as e:
            logger.error(f"Failed to upload to blob '{blob_path}': {str(e)}")
            raise
    
    def upload_dataframe_as_csv(self, df: pd.DataFrame, blob_path: str):
        """Upload DataFrame as CSV to blob storage"""
        try:
            from azure.storage.blob import ContentSettings
            
            csv_buffer = BytesIO()
            df.to_csv(csv_buffer, index=False, encoding='utf-8')
            csv_data = csv_buffer.getvalue()
            
            blob_client = self.blob_service_client.get_blob_client(
                container=self.output_container,
                blob=blob_path
            )
            
            content_settings = ContentSettings(content_type='text/csv')
            blob_client.upload_blob(csv_data, overwrite=True, content_settings=content_settings)
            logger.info(f"Uploaded CSV to blob: {blob_path}")
        except Exception as e:
            logger.error(f"Failed to upload CSV '{blob_path}': {str(e)}")
            raise


class DocumentIntelligenceManager:
    """Manages Azure Document Intelligence OCR operations"""
    
    def __init__(self, endpoint: str, key: str):
        self.client = DocumentIntelligenceClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(key)
        )
        logger.info("DocumentIntelligenceManager initialized")
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def analyze_document(self, base64_data: str, file_extension: str) -> Dict[str, Any]:
        """Analyze document using Document Intelligence Read model"""
        try:
            document_bytes = base64.b64decode(base64_data)
            content_type_map = {
                '.pdf': 'application/pdf',
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.tiff': 'image/tiff',
                '.tif': 'image/tiff',
                '.bmp': 'image/bmp'
            }
            content_type = content_type_map.get(file_extension.lower(), 'application/pdf')
            
            poller = self.client.begin_analyze_document(
                model_id="prebuilt-read",
                analyze_request=document_bytes,
                content_type=content_type
            )
            
            result = poller.result()
            extracted_text = ""
            page_count = 0
            
            if hasattr(result, 'pages') and result.pages:
                page_count = len(result.pages)
                for page in result.pages:
                    if hasattr(page, 'lines') and page.lines:
                        for line in page.lines:
                            extracted_text += line.content + "\n"
            
            if not extracted_text:
                return {'text': '', 'page_count': page_count, 'success': False, 'error': 'No text content found'}
            
            return {'text': extracted_text.strip(), 'page_count': page_count, 'success': True}
            
        except Exception as e:
            logger.error(f"OCR error: {str(e)}", exc_info=True)
            return {'text': '', 'page_count': 0, 'success': False, 'error': str(e)}


class AzureOpenAIManager:
    """Manages Azure OpenAI operations for extraction and embeddings"""
    
    def __init__(self, openai_config: Dict[str, Any], embedding_config: Dict[str, Any]):
        self.client = AzureOpenAI(
            azure_endpoint=openai_config['endpoint'],
            api_key=openai_config['api_key'],
            api_version=openai_config['api_version']
        )
        self.deployment_name = openai_config['deployment_name']
        self.embedding_client = AzureOpenAI(
            azure_endpoint=embedding_config['endpoint'],
            api_key=embedding_config['api_key'],
            api_version=embedding_config['api_version']
        )
        self.embedding_deployment_name = embedding_config['deployment_name']
        self.embedding_dimension = embedding_config.get('dimension', 1536)
        
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0
        logger.info(f"AzureOpenAIManager initialized with embedding dimension {self.embedding_dimension}")
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def extract_fields(self, text: str, fields: List[str], source_document: str) -> Dict[str, Any]:
        """Extract specified fields from text using GPT"""
        try:
            system_prompt = f"""You are a document extraction expert. Extract the following fields from the document:
{', '.join(fields)}

For each field:
1. Extract the exact value if found
2. Provide a confidence score (0.0 to 1.0)
3. If not found, return null for value and 0.0 for confidence

Return ONLY a JSON object with this exact structure:
{{
    "field_name": {{"value": "extracted_value", "confidence": 0.95}},
    ...
}}

Be precise and conservative with confidence scores. Only use high confidence (>0.9) when you are certain."""
            
            user_prompt = f"Document text:\n\n{text[:8000]}"  # Limit text to prevent token overflow
            
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                max_tokens=2000
            )
            
            self.total_tokens += response.usage.total_tokens
            self.prompt_tokens += response.usage.prompt_tokens
            self.completion_tokens += response.usage.completion_tokens
            
            content = response.choices[0].message.content.strip()
            if content.startswith('```'):
                content = re.sub(r'^```json\n', '', content)
                content = re.sub(r'\n```$', '', content)
            
            extracted_data = json.loads(content)
            
            # Normalize data
            normalized_data = {}
            for field in extracted_data:
                field_value = extracted_data[field]
                if isinstance(field_value, dict):
                    normalized_data[field] = {
                        'value': field_value.get('value', ''),
                        'confidence': float(field_value.get('confidence', 0.5)),
                        'source_document': source_document
                    }
                else:
                    normalized_data[field] = {'value': str(field_value), 'confidence': 0.5, 'source_document': source_document}
            
            return {'extracted_fields': normalized_data, 'raw_response': content, 'success': True}
        
        except Exception as e:
            logger.error(f"Field extraction failed: {str(e)}")
            return {'extracted_fields': {}, 'raw_response': '', 'success': False, 'error': str(e)}
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate_embeddings(self, text: str) -> List[float]:
        """Generate embeddings using separate embedding deployment"""
        try:
            if not text or len(text.strip()) < 3:
                text = "No content available"
            max_chars = 30000
            if len(text) > max_chars:
                text = text[:max_chars]
            
            response = self.embedding_client.embeddings.create(
                model=self.embedding_deployment_name,
                input=text
            )
            embeddings = response.data[0].embedding
            if len(embeddings) != self.embedding_dimension:
                logger.warning(f"Embedding dimension mismatch: expected {self.embedding_dimension}, got {len(embeddings)}")
                embeddings = [0.0] * self.embedding_dimension
            return embeddings
        except Exception as e:
            logger.error(f"Embedding generation failed: {str(e)}", exc_info=True)
            return [0.0] * self.embedding_dimension
    
    def get_token_usage(self) -> Dict[str, int]:
        return {'total_tokens': self.total_tokens, 'prompt_tokens': self.prompt_tokens, 'completion_tokens': self.completion_tokens}
    
    def calculate_cost(self, costs_config: Dict[str, float]) -> Dict[str, float]:
        input_cost = (self.prompt_tokens / 1000) * costs_config['gpt4o_input_per_1k']
        output_cost = (self.completion_tokens / 1000) * costs_config['gpt4o_output_per_1k']
        return {'input_cost': round(input_cost, 4), 'output_cost': round(output_cost, 4), 'total_cost': round(input_cost+output_cost, 4)}




-----

import os
import pandas as pd
from helper import ConfigManager, AzureBlobManager, DocumentIntelligenceManager, AzureOpenAIManager
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    # Load configuration
    config_manager = ConfigManager('config.json')
    
    # Initialize managers
    blob_manager = AzureBlobManager(
        connection_string=config_manager.get('AzureBlob', 'connection_string'),
        input_container=config_manager.get('AzureBlob', 'inputcontainer'),
        output_container=config_manager.get('AzureBlob', 'outputcontainer')
    )
    
    doc_intel_manager = DocumentIntelligenceManager(
        endpoint=config_manager.get('DocumentIntelligence', 'endpoint'),
        key=config_manager.get('DocumentIntelligence', 'key')
    )
    
    openai_manager = AzureOpenAIManager(
        openai_config=config_manager.get('AzureOpenAI'),
        embedding_config=config_manager.get('AzureEmbedding')
    )
    
    fields = config_manager.get('fields')
    confidence_threshold = config_manager.get('confidence_threshold', 0.9)
    
    # Process each provider
    providers = blob_manager.get_providers()
    for provider in providers:
        logger.info(f"Processing provider: {provider}")
        files = blob_manager.get_provider_files(provider)
        all_records = []
        
        for file in files:
            blob_name = file['name']
            logger.info(f"Processing file: {blob_name}")
            
            # Download file as base64
            try:
                base64_data = blob_manager.download_blob_as_base64(blob_name)
            except Exception as e:
                logger.error(f"Failed to download {blob_name}: {str(e)}")
                continue
            
            # OCR the document
            ocr_result = doc_intel_manager.analyze_document(base64_data, file['extension'])
            if not ocr_result['success']:
                logger.warning(f"OCR failed for {blob_name}, skipping")
                continue
            text = ocr_result['text']
            
            # Extract structured fields
            extraction_result = openai_manager.extract_fields(text, fields, blob_name)
            if not extraction_result['success']:
                logger.warning(f"Extraction failed for {blob_name}, skipping")
                continue
            
            extracted_fields = extraction_result['extracted_fields']
            
            # Generate embedding for the full text
            embedding = openai_manager.generate_embeddings(text)
            
            # Flatten record for CSV
            record = {'provider': provider, 'file': blob_name, 'page_count': ocr_result.get('page_count', 0)}
            for field_name, info in extracted_fields.items():
                record[f"{field_name}_value"] = info.get('value')
                record[f"{field_name}_confidence"] = info.get('confidence')
            
            record['embedding'] = embedding  # optionally store embedding as list
            all_records.append(record)
        
        # Save results as CSV in Azure Blob
        if all_records:
            df = pd.DataFrame(all_records)
            timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
            csv_blob_path = f"{provider}/extracted_{timestamp}.csv"
            blob_manager.upload_dataframe_as_csv(df, csv_blob_path)
            logger.info(f"Saved results for {provider} to {csv_blob_path}")
    
    # Print token usage and cost
    token_usage = openai_manager.get_token_usage()
    logger.info(f"Token usage: {token_usage}")
    
    cost_info = openai_manager.calculate_cost(config_manager.get('costs'))
    logger.info(f"Estimated cost: {cost_info}")

if __name__ == "__main__":
    main()
