"""
Azure RAG Document Processing - Helper Module
Utilities for Azure Blob, OCR, OpenAI extraction, embeddings, and AI Search
"""

import os
import json
import base64
import logging
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Tuple
from io import BytesIO
import re

import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential

from azure.storage.blob import BlobServiceClient, ContentSettings
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchableField, SearchField, SearchFieldDataType,
    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,
    SemanticConfiguration, SemanticField, SemanticPrioritizedFields, SemanticSearch
)
from openai import AzureOpenAI

# Logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("document_processing.log"), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)


class ConfigManager:
    def __init__(self, config_path: str = "config.json"):
        self.config_path = config_path
        self.config = self.load_config()
        self.validate_config()

    def load_config(self) -> Dict[str, Any]:
        try:
            with open(self.config_path, "r") as f:
                cfg = json.load(f)
            logger.info(f"Configuration loaded from {self.config_path}")
            return cfg
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            raise

    def validate_config(self):
        required_sections = ["AzureBlob", "AzureOpenAI", "AzureEmbedding", "DocumentIntelligence", "AzureAISearch"]
        for section in required_sections:
            if section not in self.config:
                raise ValueError(f"Missing required configuration section: {section}")
        logger.info("Configuration validated successfully")

    def get(self, section: str, key: str = None) -> Any:
        """Get config section or key"""
        if key:
            return self.config.get(section, {}).get(key)
        return self.config.get(section)


class AzureBlobManager:
    def __init__(self, connection_string: str, input_container: str, output_container: str):
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self.input_container = input_container
        self.output_container = output_container
        self.supported_extensions = {".pdf", ".doc", ".docx", ".png", ".jpg", ".jpeg", ".tiff", ".tif"}
        logger.info("AzureBlobManager initialized")

    def get_providers(self) -> List[str]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs()
        providers = {b.name.split("/")[0] for b in blobs if b.name}
        return sorted(list(providers))

    def get_provider_files(self, provider: str) -> List[Dict[str, Any]]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs(name_starts_with=f"{provider}/")
        files = []
        for b in blobs:
            ext = os.path.splitext(b.name)[1].lower()
            if ext in self.supported_extensions:
                files.append({"name": b.name, "provider": provider, "size": b.size, "extension": ext})
        return files

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def download_blob_as_base64(self, blob_name: str) -> str:
        blob_client = self.blob_service_client.get_blob_client(self.input_container, blob_name)
        data = blob_client.download_blob().readall()
        return base64.b64encode(data).decode("utf-8")

    def upload_to_blob(self, data: str, blob_path: str, content_type: str = "text/plain"):
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        blob_client.upload_blob(data, overwrite=True, content_settings=ContentSettings(content_type=content_type))

    def upload_dataframe_as_csv(self, df: pd.DataFrame, blob_path: str):
        csv_buffer = BytesIO()
        df.to_csv(csv_buffer, index=False, encoding="utf-8")
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        blob_client.upload_blob(csv_buffer.getvalue(), overwrite=True, content_settings=ContentSettings(content_type="text/csv"))


class DocumentIntelligenceManager:
    def __init__(self, endpoint: str, key: str):
        self.client = DocumentIntelligenceClient(endpoint, AzureKeyCredential(key))

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def analyze_document(self, base64_data: str, file_extension: str) -> Dict[str, Any]:
        try:
            data_bytes = base64.b64decode(base64_data)
            content_type_map = {
                ".pdf": "application/pdf",
                ".png": "image/png",
                ".jpg": "image/jpeg",
                ".jpeg": "image/jpeg",
                ".tiff": "image/tiff",
                ".tif": "image/tiff",
                ".bmp": "image/bmp"
            }
            content_type = content_type_map.get(file_extension.lower(), "application/pdf")
            poller = self.client.begin_analyze_document("prebuilt-read", analyze_request=data_bytes, content_type=content_type)
            result = poller.result()
            text = ""
            pages = 0
            if hasattr(result, "pages") and result.pages:
                pages = len(result.pages)
                for page in result.pages:
                    if hasattr(page, "lines"):
                        for line in page.lines:
                            text += getattr(line, "content", "") + "\n"
            if not text:
                return {"text": "", "page_count": pages, "success": False, "error": "No text found"}
            return {"text": text.strip(), "page_count": pages, "success": True}
        except Exception as e:
            return {"text": "", "page_count": 0, "success": False, "error": str(e)}


class AzureOpenAIManager:
    def __init__(self, endpoint: str, api_key: str, api_version: str,
                 deployment_name: str, embedding_endpoint: str, embedding_key: str,
                 embedding_deployment_name: str, embedding_dimension: int):
        self.client = AzureOpenAI(azure_endpoint=endpoint, api_key=api_key, api_version=api_version)
        self.deployment_name = deployment_name
        self.embedding_endpoint = embedding_endpoint
        self.embedding_key = embedding_key
        self.embedding_deployment_name = embedding_deployment_name
        self.embedding_dimension = embedding_dimension
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def extract_fields(self, text: str, fields: List[str], source_document: str) -> Dict[str, Any]:
        try:
            system_prompt = f"You are an expert. Extract fields: {', '.join(fields)}. Return ONLY JSON with value & confidence."
            user_prompt = text[:8000]  # limit tokens
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                temperature=0,
                max_tokens=2000
            )
            self.total_tokens += response.usage.total_tokens
            self.prompt_tokens += response.usage.prompt_tokens
            self.completion_tokens += response.usage.completion_tokens
            content = response.choices[0].message.content.strip()
            if content.startswith("```"):
                content = re.sub(r"^```json\n", "", content)
                content = re.sub(r"\n```$", "", content)
            extracted = json.loads(content)
            # Normalize
            normalized = {}
            for f, val in extracted.items():
                if isinstance(val, dict):
                    normalized[f] = {"value": val.get("value", ""), "confidence": float(val.get("confidence", 0.0)), "source_document": source_document}
                else:
                    normalized[f] = {"value": str(val), "confidence": 0.5, "source_document": source_document}
            return {"extracted_fields": normalized, "raw_response": content, "success": True}
        except Exception as e:
            return {"extracted_fields": {}, "raw_response": "", "success": False, "error": str(e)}

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate_embeddings(self, text: str) -> List[float]:
        from openai import OpenAI
        try:
            client = AzureOpenAI(azure_endpoint=self.embedding_endpoint, api_key=self.embedding_key)
            if not text or len(text.strip()) < 3:
                text = "No content available"
            text = text[:30000]
            response = client.embeddings.create(model=self.embedding_deployment_name, input=text)
            return response.data[0].embedding
        except Exception as e:
            logger.warning(f"Embedding generation failed: {e}")
            return [0.0] * self.embedding_dimension

    def get_token_usage(self) -> Dict[str, int]:
        return {"total_tokens": self.total_tokens, "prompt_tokens": self.prompt_tokens, "completion_tokens": self.completion_tokens}

    def calculate_cost(self, costs_config: Dict[str, float]) -> Dict[str, float]:
        input_cost = (self.prompt_tokens / 1000) * costs_config.get("gpt4o_input_per_1k", 0.0025)
        output_cost = (self.completion_tokens / 1000) * costs_config.get("gpt4o_output_per_1k", 0.01)
        return {"input_cost": round(input_cost, 4), "output_cost": round(output_cost, 4), "total_cost": round(input_cost + output_cost, 4)}




---------


main.py

import os
import json
import pandas as pd
from datetime import datetime
from helper import ConfigManager, AzureBlobManager, DocumentIntelligenceManager, AzureOpenAIManager

CONFIG_PATH = "config.json"

def main():
    # -----------------------------
    # 1. Load Configuration
    # -----------------------------
    config_manager = ConfigManager(CONFIG_PATH)
    confidence_threshold = float(config_manager.config.get("confidence_threshold", 0.9))
    fields = config_manager.get("fields")
    
    # -----------------------------
    # 2. Initialize Managers
    # -----------------------------
    blob_cfg = config_manager.get("AzureBlob")
    blob_mgr = AzureBlobManager(blob_cfg["connection_string"], blob_cfg["inputcontainer"], blob_cfg["outputcontainer"])
    
    doc_intel_cfg = config_manager.get("DocumentIntelligence")
    doc_mgr = DocumentIntelligenceManager(doc_intel_cfg["endpoint"], doc_intel_cfg["key"])
    
    azure_cfg = config_manager.get("AzureOpenAI")
    embed_cfg = config_manager.get("AzureEmbedding")
    gpt_mgr = AzureOpenAIManager(
        endpoint=azure_cfg["endpoint"],
        api_key=azure_cfg["api_key"],
        api_version=azure_cfg["api_version"],
        deployment_name=azure_cfg["deployment_name"],
        embedding_endpoint=embed_cfg["endpoint"],
        embedding_key=embed_cfg["api_key"],
        embedding_deployment_name=embed_cfg["deployment_name"],
        embedding_dimension=int(embed_cfg.get("dimension", 3072))
    )
    
    # -----------------------------
    # 3. Process Providers
    # -----------------------------
    providers = blob_mgr.get_providers()
    all_results = []

    for provider in providers:
        print(f"Processing provider: {provider}")
        files = blob_mgr.get_provider_files(provider)
        for file_info in files:
            file_name = file_info["name"]
            print(f"  File: {file_name}")
            
            # Download as base64
            base64_data = blob_mgr.download_blob_as_base64(file_name)
            
            # OCR/Document extraction
            ocr_result = doc_mgr.analyze_document(base64_data, file_info["extension"])
            if not ocr_result["success"]:
                print(f"    OCR failed: {ocr_result.get('error')}")
                continue
            
            text_content = ocr_result["text"]
            page_count = ocr_r

