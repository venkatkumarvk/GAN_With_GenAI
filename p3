import base64
import os
import json
import tempfile
import streamlit as st
from dotenv import load_dotenv
from openai import AzureOpenAI
from pathlib import Path
import fitz  # PyMuPDF
import pandas as pd
from datetime import datetime
import io
import zipfile
from azure.storage.blob import BlobServiceClient, ContentSettings

# Load environment variables from .env file
load_dotenv()

# Azure OpenAI environment variables
aoai_endpoint = os.getenv("AOAI_ENDPOINT")
aoai_api_key = os.getenv("AOAI_API_KEY")
aoai_deployment_name = os.getenv("AOAI_DEPLOYMENT")

# Azure Blob Storage environment variables
azure_storage_connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
azure_storage_container_name = os.getenv("AZURE_STORAGE_CONTAINER_NAME", "pdf-extraction-results")

# Initialize the Azure OpenAI client
@st.cache_resource
def get_client():
    return AzureOpenAI(
        azure_endpoint=aoai_endpoint,
        api_key=aoai_api_key,
        api_version="2024-08-01-preview"
    )

# Initialize the Azure Blob Storage client
@st.cache_resource
def get_blob_service_client():
    return BlobServiceClient.from_connection_string(azure_storage_connection_string)

def image_to_data_url(image_bytes, mime_type='image/png'):
    """
    Convert image bytes to a data URL.
    """
    base64_encoded_data = base64.b64encode(image_bytes).decode('utf-8')
    return f"data:{mime_type};base64,{base64_encoded_data}"

def call_azure_openai_vision(prompt, image_data_url, client, deployment_name):
    """
    Call the Azure OpenAI Vision service to analyze an image.
    """
    try:
        completion = client.chat.completions.create(
            model=deployment_name,
            messages=[{
                "role": "system",
                "content": "You are an AI helpful assistant that extracts information from invoice documents. Your task is to extract the following fields from invoices: VendorName, InvoiceNumber, InvoiceDate, CustomerName, PurchaseOrder, StockCode, UnitPrice, InvoiceAmount, Freight, Salestax, and Total. Return a JSON object with these keys. For each field, also include a confidence score between 0 and 1. The response format should be: {\"VendorName\": {\"value\": \"ABC Corp\", \"confidence\": 0.95}, \"InvoiceNumber\": {\"value\": \"INV-12345\", \"confidence\": 0.87}, ...and so on for each field.}"
            }, {
                "role": "user",
                "content": [{
                    "type": "text",
                    "text": prompt
                }, {
                    "type": "image_url",
                    "image_url": {
                        "url": image_data_url
                    }
                }]
            }],
            max_tokens=2000,
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        # Extract and parse the response content
        response_content = completion.choices[0].message.content
        return json.loads(response_content)
    except Exception as e:
        st.error(f"Error calling Azure OpenAI: {str(e)}")
        return {"error": str(e)}

def process_pdf(pdf_file, prompt, client, deployment_name, progress_bar=None, progress_text=None):
    """
    Process a PDF file and extract information from all pages.
    """
    try:
        # Create a temporary file to store the uploaded PDF
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_path = tmp_file.name
        
        # Get the filename
        filename = pdf_file.name
        
        try:
            # Open the PDF file
            with fitz.open(tmp_path) as doc:
                page_count = len(doc)
                
                if progress_text:
                    progress_text.text(f"Processing {filename} - {page_count} pages...")
                
                # Create a list to store extracted data from all pages
                all_page_results = []
                
                # Process each page in the PDF
                for page_num in range(page_count):
                    try:
                        # Update progress
                        if progress_bar:
                            progress_bar.progress((page_num + 1) / page_count)
                        if progress_text:
                            progress_text.text(f"Processing {filename} - Page {page_num+1}/{page_count}")
                        
                        # Load the current page
                        page = doc.load_page(page_num)
                        
                        # Process image
                        zoom = 2  # Zoom factor for image quality
                        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
                        image_bytes = pix.tobytes()
                        
                        # Convert image to data URL
                        image_data_url = image_to_data_url(image_bytes)
                        
                        # Call Azure OpenAI Vision to extract structured information
                        extracted_info = call_azure_openai_vision(prompt, image_data_url, client, deployment_name)
                        
                        # Add page info to the collected results
                        extracted_info_with_page = {
                            "page": page_num + 1,
                            "data": extracted_info
                        }
                        
                        # Add to our collection of all page results
                        all_page_results.append(extracted_info_with_page)
                        
                    except Exception as e:
                        error_msg = f"Error processing page {page_num+1} of {filename}: {e}"
                        st.warning(error_msg)
                        all_page_results.append({
                            "page": page_num + 1,
                            "data": {"error": str(e)}
                        })
        finally:
            # Clean up the temporary file
            try:
                os.unlink(tmp_path)
            except Exception as e:
                st.warning(f"Could not remove temporary file {tmp_path}: {e}")
        
        # Create a result object that contains all pages' data
        final_result = {
            "filename": filename,
            "total_pages": page_count,
            "pages": all_page_results
        }
        
        return final_result
        
    except Exception as e:
        error_msg = f"Error processing {pdf_file.name}: {e}"
        st.error(error_msg)
        return {
            "filename": pdf_file.name,
            "error": str(e),
            "total_pages": 0,
            "pages": []
        }

def create_results_dataframe(all_pdf_results):
    """
    Create a pandas DataFrame from the extracted results for easy viewing.
    """
    rows = []
    
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    for pdf_result in all_pdf_results:
        filename = pdf_result["filename"]
        
        for page in pdf_result["pages"]:
            page_num = page["page"]
            data = page["data"]
            
            # Check for errors
            if "error" in data:
                row_data = {
                    "Filename": filename,
                    "Page": page_num
                }
                
                # Add placeholders for all fields and confidence values
                for field in fields:
                    row_data[field] = "N/A"
                    row_data[f"{field} Confidence"] = 0
                
                rows.append(row_data)
                continue
            
            # Initialize row data
            row_data = {
                "Filename": filename,
                "Page": page_num
            }
            
            # Process each field
            for field in fields:
                field_data = data.get(field, {})
                
                if isinstance(field_data, dict):
                    value = field_data.get("value", "N/A")
                    confidence = field_data.get("confidence", 0)
                else:
                    value = field_data if field_data else "N/A"
                    confidence = 0
                
                # Ensure values are strings to avoid PyArrow errors
                if isinstance(value, (list, dict)):
                    value = str(value)
                
                # Add to row data
                row_data[field] = value
                row_data[f"{field} Confidence"] = round(confidence * 100, 2)
            
            # Add completed row to rows
            rows.append(row_data)
    
    try:
        # First method: Try creating a DataFrame with string type
        # This avoids PyArrow conversion issues for mixed types
        return pd.DataFrame(rows, dtype=str)
    except Exception as e:
        st.warning(f"Error creating DataFrame: {e}. Trying alternative method...")
        
        try:
            # Second method: Try with pandas default types but disable PyArrow
            with pd.option_context('mode.dtype_backend', 'numpy'):  # Use NumPy instead of PyArrow
                return pd.DataFrame(rows)
        except Exception as e:
            st.warning(f"Second method failed: {e}. Using final fallback method...")
            
            try:
                # Third method: Convert all values to strings explicitly before creating DataFrame
                string_rows = []
                for row in rows:
                    string_row = {}
                    for key, value in row.items():
                        string_row[key] = str(value)
                    string_rows.append(string_row)
                return pd.DataFrame(string_rows)
            except Exception as e:
                st.error(f"All DataFrame creation methods failed: {e}")
                # Return empty DataFrame as absolute last resort
                return pd.DataFrame()

def create_text_files_zip(all_pdf_results):
    """
    Create a zip file containing text files for each PDF.
    """
    # Create a BytesIO object to store the zip file
    zip_buffer = io.BytesIO()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create a ZipFile object
    with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:
        for pdf_result in all_pdf_results:
            filename = pdf_result["filename"]
            base_filename = os.path.splitext(filename)[0]
            
            # Create the text content for this PDF (only key-value pairs)
            page_results_text = create_page_results_text(pdf_result)
            
            # Add structured data as a text file with timestamp
            zip_file.writestr(f"{base_filename}_{timestamp}.txt", page_results_text)
    
    # Seek to the beginning of the BytesIO object
    zip_buffer.seek(0)
    return zip_buffer

def create_page_results_text(pdf_result):
    """
    Create a text file containing only the key-value pairs from each page.
    Returns a string with the formatted key-value pairs.
    """
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    result_text = ""
    
    for page in pdf_result["pages"]:
        page_num = page["page"]
        data = page["data"]
        
        result_text += f"--- PAGE {page_num} ---\n"
        
        if "error" in data:
            result_text += f"error: {data['error']}\n\n"
            continue
            
        # Process fields with confidence scores
        for field in fields:
            display_field = ''.join(' ' + char if char.isupper() else char for char in field).strip().lower()
            
            field_data = data.get(field, {})
            if isinstance(field_data, dict):
                value = field_data.get("value", "N/A")
                confidence = field_data.get("confidence", 0)
                result_text += f"{display_field}: {value}\n"
                result_text += f"{display_field} confidence: {round(confidence * 100, 2)}%\n"
            else:
                result_text += f"{display_field}: {field_data}\n"
        
        result_text += "\n"
        
    return result_text

def evaluate_extraction_results(all_pdf_results):
    """
    Evaluate the quality and completeness of extraction results using the
    confidence scores provided by Azure AI Vision.
    """
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    evaluation_results = {
        "total_documents": len(all_pdf_results),
        "total_pages": 0,
        "successful_pages": 0,
        "failed_pages": 0,
        "field_confidence": {},
        "documents_with_errors": []
    }
    
    # Initialize field confidence data structure
    for field in fields:
        evaluation_results["field_confidence"][field] = {
            "total": 0,
            "average_confidence": 0,
            "pages_above_threshold": 0,
            "percent_above_threshold": 0
        }
    
    confidence_threshold = 0.7  # 70% confidence threshold
    
    # Collect all confidence scores by field
    all_confidences = {}
    for field in fields:
        all_confidences[field] = []
    
    for pdf_result in all_pdf_results:
        filename = pdf_result["filename"]
        document_has_error = False
        
        for page in pdf_result["pages"]:
            evaluation_results["total_pages"] += 1
            data = page["data"]
            
            if "error" in data:
                evaluation_results["failed_pages"] += 1
                document_has_error = True
                continue
                
            page_successful = True
            
            # Check each field for confidence scores
            for field in fields:
                field_data = data.get(field, {})
                
                if isinstance(field_data, dict) and "confidence" in field_data:
                    confidence = field_data.get("confidence", 0)
                    evaluation_results["field_confidence"][field]["total"] += 1
                    all_confidences[field].append(confidence)
                    
                    # Count pages above threshold
                    if confidence >= confidence_threshold:
                        evaluation_results["field_confidence"][field]["pages_above_threshold"] += 1
                    else:
                        page_successful = False
                else:
                    page_successful = False
            
            if page_successful:
                evaluation_results["successful_pages"] += 1
            else:
                evaluation_results["failed_pages"] += 1
                document_has_error = True
        
        if document_has_error:
            evaluation_results["documents_with_errors"].append(filename)
    
    # Calculate average confidence for each field
    for field in all_confidences:
        confidences = all_confidences[field]
        if confidences:
            avg_confidence = sum(confidences) / len(confidences)
            evaluation_results["field_confidence"][field]["average_confidence"] = round(avg_confidence * 100, 2)
            
            # Calculate percentage of pages above threshold
            total = evaluation_results["field_confidence"][field]["total"]
            if total > 0:
                above_threshold = evaluation_results["field_confidence"][field]["pages_above_threshold"]
                evaluation_results["field_confidence"][field]["percent_above_threshold"] = round((above_threshold / total) * 100, 2)
    
    # Calculate overall success rate
    if evaluation_results["total_pages"] > 0:
        evaluation_results["success_rate"] = round((evaluation_results["successful_pages"] / evaluation_results["total_pages"]) * 100, 2)
    
    # Calculate overall confidence score (average of field confidence)
    field_scores = [field_data["average_confidence"] for field_data in evaluation_results["field_confidence"].values() if "average_confidence" in field_data]
    if field_scores:
        evaluation_results["overall_confidence_score"] = round(sum(field_scores) / len(field_scores), 2)
    
    return evaluation_results

def upload_to_blob_storage(blob_service_client, container_name, blob_name, data, content_type):
    """
    Upload data to Azure Blob Storage.
    """
    try:
        # Get the blob client
        container_client = blob_service_client.get_container_client(container_name)
        
        # Create the container if it doesn't exist
        if not container_client.exists():
            container_client.create_container()
        
        # Upload blob
        blob_client = container_client.get_blob_client(blob_name)
        
        # Set content settings
        content_settings = ContentSettings(content_type=content_type)
        
        # Upload the file
        blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
        
        return True, blob_client.url
    except Exception as e:
        return False, str(e)

def main():
    st.set_page_config(
        page_title="PDF Financial Data Extractor",
        page_icon="ðŸ“Š",
        layout="wide"
    )
    
    st.title("PDF Financial Data Extractor")
    st.subheader("Upload financial documents to extract customer information")
    
    # Check if Azure OpenAI credentials are available
    if not all([aoai_endpoint, aoai_api_key, aoai_deployment_name]):
        st.error("Azure OpenAI credentials are missing. Please set AOAI_ENDPOINT, AOAI_API_KEY, and AOAI_DEPLOYMENT environment variables.")
        return
    
    # Check if Azure Blob Storage credentials are available
    if not azure_storage_connection_string:
        st.warning("Azure Blob Storage connection string is missing. Results will not be saved to the cloud. Please set AZURE_STORAGE_CONNECTION_STRING environment variable.")
    
    # Initialize the clients
    client = get_client()
    blob_service_client = get_blob_service_client() if azure_storage_connection_string else None
    
    # File uploader
    uploaded_files = st.file_uploader(
        "Upload PDF files", 
        type="pdf", 
        accept_multiple_files=True,
        help="Upload one or more PDF files containing financial statements"
    )
    
    # Advanced settings in an expandable section
    with st.expander("Advanced Settings"):
        prompt = st.text_area(
            "Extraction Prompt", 
            """Based on this image, extract the following information from the invoice:   
            1) What is the vendor name?
            2) What is the invoice number?
            3) What is the invoice date?
            4) What is the customer name?
            5) What is the purchase order number?
            6) What is the stock code?
            7) What is the unit price?
            8) What is the invoice amount?
            9) What is the freight cost?
            10) What is the sales tax?
            11) What is the total amount?""",
            help="Customize the prompt sent to Azure OpenAI Vision to extract information"
        )
    
    # Process button
    if uploaded_files and st.button("Process Documents", type="primary"):
        if not uploaded_files:
            st.warning("Please upload at least one PDF file.")
            return
            
        with st.spinner("Processing documents..."):
            # Create containers for progress tracking
            progress_container = st.container()
            with progress_container:
                progress_bar = st.progress(0)
                progress_text = st.empty()
                
                # Store all PDF results
                all_pdf_results = []
                
                # Process each uploaded PDF
                for i, pdf_file in enumerate(uploaded_files):
                    progress_text.text(f"Processing file {i+1}/{len(uploaded_files)}: {pdf_file.name}")
                    
                    # Process the PDF and get results
                    pdf_result = process_pdf(
                        pdf_file, 
                        prompt, 
                        client, 
                        aoai_deployment_name,
                        progress_bar,
                        progress_text
                    )
                    
                    # Add to our collection of all PDF results
                    all_pdf_results.append(pdf_result)
                    
                    # Update overall progress
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                # Create a timestamp for the filename
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # Save individual text files for each PDF and upload to Azure Blob Storage
                blob_upload_results = []
                text_files_info = []
                
                if blob_service_client:
                    for pdf_result in all_pdf_results:
                        filename = pdf_result["filename"]
                        base_filename = os.path.splitext(filename)[0]
                        
                        # Create the text content with key-value pairs
                        page_results_text = create_page_results_text(pdf_result)
                        
                        # Create timestamp filename
                        timestamp_filename = f"{base_filename}_{timestamp}"
                        
                        # Upload text file to blob storage
                        text_blob_name = f"{timestamp_filename}.txt"
                        text_success, text_url = upload_to_blob_storage(
                            blob_service_client,
                            azure_storage_container_name,
                            text_blob_name,
                            page_results_text,
                            "text/plain"
                        )
                        
                        # Upload JSON to blob storage
                        json_blob_name = f"{timestamp_filename}.json"
                        pdf_json = json.dumps(pdf_result, ensure_ascii=False, indent=2)
                        json_success, json_url = upload_to_blob_storage(
                            blob_service_client,
                            azure_storage_container_name,
                            json_blob_name,
                            pdf_json,
                            "application/json"
                        )
                        
                        # Store results
                        blob_upload_results.append({
                            "filename": filename,
                            "text_success": text_success,
                            "text_url": text_url if text_success else None,
                            "json_success": json_success,
                            "json_url": json_url if json_success else None
                        })
                        
                        # Store text file info for display
                        text_files_info.append({
                            "filename": filename,
                            "text_content": page_results_text,
                            "timestamp_filename": timestamp_filename
                        })
                
                progress_text.text("Processing complete!")
                progress_bar.progress(1.0)
                
            # 1. Display extraction results
            st.subheader("1. Extraction Results")
            
            # Create a DataFrame view
            if all_pdf_results:
                try:
                    results_df = create_results_dataframe(all_pdf_results)
                    
                    if not results_df.empty:
                        st.dataframe(results_df, use_container_width=True)
                    else:
                        st.warning("Could not create results table due to data format issues.")
                        
                except Exception as e:
                    st.error(f"Error displaying results table: {e}")
                    st.info("Continuing with other outputs despite table display error.")
                
                # Create zip file with text extractions
                text_zip = create_text_files_zip(all_pdf_results)
                
                # 2. Display extracted key-value pairs
                st.subheader("2. Extracted Key-Value Pairs")
                
                # Create tabs for each PDF
                if len(all_pdf_results) > 0:
                    pdf_tabs = st.tabs([pdf_result["filename"] for pdf_result in all_pdf_results])
                    
                    for i, tab in enumerate(pdf_tabs):
                        with tab:
                            pdf_result = all_pdf_results[i]
                            filename = pdf_result["filename"]
                            base_filename = os.path.splitext(filename)[0]
                            
                            # Generate or retrieve page results text
                            if text_files_info:
                                page_results_text = next((info["text_content"] for info in text_files_info if info["filename"] == filename), "")
                                timestamp_filename = next((info["timestamp_filename"] for info in text_files_info if info["filename"] == filename), "")
                            else:
                                page_results_text = create_page_results_text(pdf_result)
                                timestamp_filename = f"{base_filename}_{timestamp}"
                            
                            # Display the key-value page results
                            st.text_area(
                                f"Extracted Data for {filename}",
                                value=page_results_text,
                                height=300,
                                disabled=True
                            )
                
                # 3. Run evaluation and display results in table format
                evaluation_results = evaluate_extraction_results(all_pdf_results)
                st.subheader("3. Evaluation Results")
                
                # Create table of confidence levels by document
                confidence_by_document = {}
                
                for pdf_result in all_pdf_results:
                    filename = pdf_result["filename"]
                    confidence_by_document[filename] = {
                        "high": 0,  # 90-100%
                        "medium": 0,  # 70-90%
                        "low": 0,  # 60-70%
                        "poor": 0   # <60%
                    }
                    
                    # Count fields in each confidence range for this document
                    for page in pdf_result["pages"]:
                        data = page["data"]
                        
                        if "error" in data:
                            continue
                            
                        for field in ["VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
                                    "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
                                    "Freight", "Salestax", "Total"]:
                            field_data = data.get(field, {})
                            
                            if isinstance(field_data, dict) and "confidence" in field_data:
                                confidence = field_data.get("confidence", 0) * 100
                                
                                if confidence >= 90:
                                    confidence_by_document[filename]["high"] += 1
                                elif confidence >= 70:
                                    confidence_by_document[filename]["medium"] += 1
                                elif confidence >= 60:
                                    confidence_by_document[filename]["low"] += 1
                                else:
                                    confidence_by_document[filename]["poor"] += 1
                
                # Create table data
                doc_table_data = {
                    "Document": [],
                    "High Confidence (90-100%)": [],
                    "Medium Confidence (70-90%)": [],
                    "Low Confidence (60-70%)": [],
                    "Poor Confidence (<60%)": []
                }
                
                for filename, counts in confidence_by_document.items():
                    doc_table_data["Document"].append(filename)
                    doc_table_data["High Confidence (90-100%)"].append(counts["high"])
                    doc_table_data["Medium Confidence (70-90%)"].append(counts["medium"])
                    doc_table_data["Low Confidence (60-70%)"].append(counts["low"])
                    doc_table_data["Poor Confidence (<60%)"].append(counts["poor"])
                
                # Display the document confidence table
                doc_confidence_df = pd.DataFrame(doc_table_data)
                st.dataframe(doc_confidence_df, use_container_width=True)
                
                # 4. Documents that need manual verification
                st.subheader("4. Documents Needing Manual Verification")
                
                # Get documents with fields in medium or low confidence ranges
                docs_to_verify = []
                
                for filename, counts in confidence_by_document.items():
                    if counts["medium"] > 0 or counts["low"] > 0:
                        docs_to_verify.append({
                            "filename": filename,
                            "medium_count": counts["medium"],
                            "low_count": counts["low"]
                        })
                
                if docs_to_verify:
                    for doc in docs_to_verify:
                        if doc["low_count"] > 0:
                            st.warning(f"âš ï¸ {doc['filename']} - High priority for verification ({doc['low_count']} fields with 60-70% confidence)")
                        else:
                            st.info(f"â„¹ï¸ {doc['filename']} - Medium priority for verification ({doc['medium_count']} fields with 70-90% confidence)")
                else:
                    st.success("âœ… No documents need manual verification (all fields above 90% confidence)")
                
                # 5. Display Blob Storage upload results
                if blob_service_client and blob_upload_results:
                    st.subheader("5. Azure Blob Storage Upload Results")
                    
                    # Create a table to show upload results
                    upload_rows = []
                    for result in blob_upload_results:
                        upload_rows.append({
                            "Filename": result["filename"],
                            "Text File": "âœ… Uploaded" if result["text_success"] else "âŒ Failed",
                            "JSON File": "âœ… Uploaded" if result["json_success"] else "âŒ Failed"
                        })
                    
                    upload_df = pd.DataFrame(upload_rows)
                    st.dataframe(upload_df, use_container_width=True)
                
                # 6. Download options
                st.subheader("6. Download Options")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    # Download all text files as a zip
                    st.download_button(
                        label="Download All Text Files (ZIP)",
                        data=text_zip,
                        file_name=f"extracted_data_{timestamp}.zip",
                        mime="application/zip"
                    )
                
                with col2:
                    try:
                        # Download as CSV
                        if 'results_df' in locals() and not results_df.empty:
                            csv = results_df.to_csv(index=False)
                            st.download_button(
                                label="Download CSV Results",
                                data=csv,
                                file_name=f"financial_data_extraction_{timestamp}.csv",
                                mime="text/csv"
                            )
                        else:
                            st.info("CSV download is unavailable due to an error.")
                
                with col3:
                    # Download document verification report
                    if docs_to_verify:
                        verification_data = {
                            "Document": [doc["filename"] for doc in docs_to_verify],
                            "Medium Confidence Fields (70-90%)": [doc["medium_count"] for doc in docs_to_verify],
                            "Low Confidence Fields (60-70%)": [doc["low_count"] for doc in docs_to_verify],
                            "Verification Priority": ["High" if doc["low_count"] > 0 else "Medium" for doc in docs_to_verify]
                        }
                        verification_df = pd.DataFrame(verification_data)
                        verification_csv = verification_df.to_csv(index=False)
                        
                        st.download_button(
                            label="Download Verification Report",
                            data=verification_csv,
                            file_name=f"verification_report_{timestamp}.csv",
                            mime="text/csv"
                        )
                    else:
                        st.download_button(
                            label="Download Verification Report",
                            data="No documents require verification",
                            file_name=f"verification_report_{timestamp}.txt",
                            mime="text/plain",
                            disabled=True
                        )
            else:
                st.warning("No results were extracted from the PDFs.")

if __name__ == "__main__":
    main() download not available due to DataFrame creation issues.")
                    except Exception as e:
                        st.error(f"Error creating CSV: {e}")
                        st.info("CSV
