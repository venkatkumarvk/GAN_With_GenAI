# logger_helpers.py
import pyodbc
from datetime import datetime

def get_uid_from_conn_string(conn_str):
    for part in conn_str.split(";"):
        if part.strip().upper().startswith("UID="):
            return part.split("=")[1].strip()
    return "system"

def insert_begin_log(filename, source_path, config):
    if not config.get("logging", {}).get("enabled", True):
        print("[INFO] Logging disabled — skipping BEGIN insert.")
        return None
    
    conn_str = config["sql_server"]["connection_string"]
    table_name = config.get("logging", {}).get("table_name", "FileLogs")
    created_by = get_uid_from_conn_string(conn_str)
    created_on = datetime.utcnow()
    
    conn = None
    cursor = None
    
    try:
        print(f"[DEBUG] Attempting to connect to SQL Server...")
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()
        
        # Check if table exists first
        check_table_query = f"""
            SELECT COUNT(*) 
            FROM INFORMATION_SCHEMA.TABLES 
            WHERE TABLE_NAME = '{table_name}'
        """
        cursor.execute(check_table_query)
        table_exists = cursor.fetchone()[0] > 0
        
        if not table_exists:
            print(f"[ERROR] Table {table_name} does not exist!")
            return None
        
        insert_query = f"""
            INSERT INTO {table_name} (
                FileName, SourceFilePath,
                TargetFilePath, ArchiveFilePath,
                Status, StatusDesc,
                CREATED_ON, CREATED_BY
            )
            OUTPUT INSERTED.DocumentProcessor_Key
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """
        
        print(f"[DEBUG] Executing insert for file: {filename}")
        cursor.execute(insert_query, 
                      filename, source_path, "", "", 
                      "BEGIN", "Started processing file", 
                      created_on, created_by)
        
        result = cursor.fetchone()
        if result:
            document_id = result[0]
            conn.commit()
            print(f"[DEBUG] Successfully inserted BEGIN log with ID {document_id}")
            return document_id
        else:
            print("[ERROR] No ID returned from insert")
            return None
            
    except pyodbc.Error as db_error:
        print(f"[ERROR] Database error in BEGIN log: {db_error}")
        if "Invalid object name" in str(db_error):
            print(f"[ERROR] Table '{table_name}' does not exist or is not accessible")
        return None
    except Exception as e:
        print(f"[ERROR] General error in BEGIN log: {e}")
        return None
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def update_log(document_id, target_path, archive_path, status, status_desc, config):
    if not config.get("logging", {}).get("enabled", True):
        print("[INFO] Logging disabled — skipping UPDATE.")
        return
    
    if document_id is None:
        print("[WARNING] Cannot update log - document_id is None")
        return
    
    conn_str = config["sql_server"]["connection_string"]
    table_name = config.get("logging", {}).get("table_name", "FileLogs")
    updated_by = get_uid_from_conn_string(conn_str)
    updated_on = datetime.utcnow()
    
    conn = None
    cursor = None
    
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()
        
        update_query = f"""
            UPDATE {table_name}
            SET
                TargetFilePath = ?,
                ArchiveFilePath = ?,
                Status = ?,
                StatusDesc = ?,
                UPDATED_ON = ?,
                UPDATED_BY = ?
            WHERE DocumentProcessor_Key = ?
        """
        
        cursor.execute(update_query, 
                      target_path, archive_path, status, status_desc, 
                      updated_on, updated_by, document_id)
        
        rows_affected = cursor.rowcount
        conn.commit()
        
        if rows_affected > 0:
            print(f"[DEBUG] Successfully updated log ID {document_id} to status {status}")
        else:
            print(f"[WARNING] No rows updated for document_id {document_id} - record may not exist")
            
    except pyodbc.Error as db_error:
        print(f"[ERROR] Database error in UPDATE log: {db_error}")
    except Exception as e:
        print(f"[ERROR] General error in UPDATE log: {e}")
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def update_log_with_archive(document_id, archive_path, status, status_desc, config):
    """Update log with archive path while preserving existing target path"""
    if not config.get("logging", {}).get("enabled", True):
        print("[INFO] Logging disabled — skipping ARCHIVE UPDATE.")
        return
    
    if document_id is None:
        print("[WARNING] Cannot update archive log - document_id is None")
        return
    
    conn_str = config["sql_server"]["connection_string"]
    table_name = config.get("logging", {}).get("table_name", "FileLogs")
    updated_by = get_uid_from_conn_string(conn_str)
    updated_on = datetime.utcnow()
    
    conn = None
    cursor = None
    
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()
        
        # Get current target path first
        select_query = f"""
            SELECT TargetFilePath 
            FROM {table_name} 
            WHERE DocumentProcessor_Key = ?
        """
        cursor.execute(select_query, document_id)
        result = cursor.fetchone()
        current_target_path = result[0] if result else ""
        
        update_query = f"""
            UPDATE {table_name}
            SET
                TargetFilePath = ?,
                ArchiveFilePath = ?,
                Status = ?,
                StatusDesc = ?,
                UPDATED_ON = ?,
                UPDATED_BY = ?
            WHERE DocumentProcessor_Key = ?
        """
        
        cursor.execute(update_query, 
                      current_target_path, archive_path, status, status_desc, 
                      updated_on, updated_by, document_id)
        
        rows_affected = cursor.rowcount
        conn.commit()
        
        if rows_affected > 0:
            print(f"[DEBUG] Successfully updated archive path for log ID {document_id}: {archive_path}")
        else:
            print(f"[WARNING] No rows updated for document_id {document_id} during archive update")
            
    except pyodbc.Error as db_error:
        print(f"[ERROR] Database error in ARCHIVE UPDATE log: {db_error}")
    except Exception as e:
        print(f"[ERROR] General error in ARCHIVE UPDATE log: {e}")
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()
    """Test function to verify database connectivity and table structure"""
    if not config.get("logging", {}).get("enabled", True):
        print("[INFO] Logging disabled")
        return False
    
    conn_str = config["sql_server"]["connection_string"]
    table_name = config.get("logging", {}).get("table_name", "FileLogs")
    
    try:
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()
        
        # Check table structure
        cursor.execute(f"""
            SELECT COLUMN_NAME, DATA_TYPE 
            FROM INFORMATION_SCHEMA.COLUMNS 
            WHERE TABLE_NAME = '{table_name}'
            ORDER BY ORDINAL_POSITION
        """)
        
        columns = cursor.fetchall()
        print(f"[DEBUG] Table {table_name} structure:")
        for column in columns:
            print(f"  {column[0]}: {column[1]}")
        
        cursor.close()
        conn.close()
        return True
        
    except Exception as e:
        print(f"[ERROR] Database connection test failed: {e}")
        return False


----
def process_azure_pdf_files(config, api_type, azure_folder, logger):
    """
    Process PDF files from Azure Blob Storage with archiving support.
    
    Parameters:
    - config: Configuration dictionary
    - api_type: 'batch' or 'general'
    - azure_folder: Folder path in Azure Blob Storage
    - logger: Logger instance
    """
    # Import the logger helpers
    from logger_helpers import insert_begin_log, update_log, update_log_with_archive
    
    # Initialize helpers with archive container
    archive_container = config["azure_storage"].get("input_archive_container")
    logger.info(f"Initializing Azure Storage Helper with containers:")
    logger.info(f"  Input: {config['azure_storage']['input_container']}")
    logger.info(f"  Output: {config['azure_storage']['output_container']}")
    logger.info(f"  Archive: {archive_container}")
    
    storage_helper = AzureStorageHelper(
        config["azure_storage"]["connection_string"],
        config["azure_storage"]["input_container"],
        config["azure_storage"]["output_container"],
        archive_container,
        logger
    )
    
    pdf_processor = PDFProcessor(config, logger)
    
    logger.info(f"Initializing Azure OpenAI Client with {api_type} API")
    ai_client = AzureOpenAIClient(config, logger)
    
    # List PDF blobs in the specified folder
    logger.info(f"Listing PDF files in Azure folder: {azure_folder}")
    pdf_blobs = storage_helper.list_blobs_in_folder(azure_folder)
    
    if not pdf_blobs:
        logger.warning(f"No PDF files found in folder: {azure_folder}")
        return
    
    logger.info(f"Found {len(pdf_blobs)} PDF files to process")
    
    # Track processed and unprocessed files for archiving
    processed_files = []
    unprocessed_files = []
    file_document_ids = {}  # Track document IDs for each file
    
    # Process each PDF
    for i, blob_name in enumerate(pdf_blobs):
        file_processed_successfully = False
        document_id = None
        
        try:
            logger.info(f"Processing file {i+1}/{len(pdf_blobs)}: {blob_name}")
            
            # Get filename for logging
            filename = blob_name.split('/')[-1]
            
            # Insert BEGIN log entry
            document_id = insert_begin_log(filename, blob_name, config)
            if document_id is None:
                logger.warning(f"Failed to create log entry for {filename} - continuing without SQL logging")
            else:
                file_document_ids[blob_name] = document_id
            
            # Download blob to memory
            logger.debug(f"Downloading blob: {blob_name}")
            blob_content = storage_helper.download_blob_to_memory(blob_name)
            
            if blob_content is None:
                logger.error(f"Could not download blob: {blob_name}")
                if document_id:
                    update_log(document_id, "", "", "ERROR", "Could not download blob", config)
                unprocessed_files.append(blob_name)
                continue
            
            # Extract pages as base64 strings
            logger.info(f"Extracting pages from {filename}")
            pages = pdf_processor.extract_pdf_pages(blob_content)
            
            if not pages:
                logger.warning(f"No pages extracted from {filename}")
                if document_id:
                    update_log(document_id, "", "", "ERROR", "No pages extracted from PDF", config)
                unprocessed_files.append(blob_name)
                continue
            
            logger.info(f"Extracted {len(pages)} pages from {filename}")
            
            # Prepare batches for processing
            batch_size = config["processing"]["batch_size"]
            
            all_results = []
            batch_processing_successful = True
            
            for batch_start in range(0, len(pages), batch_size):
                batch_end = min(batch_start + batch_size, len(pages))
                batch_pages = pages[batch_start:batch_end]
                
                # Split into page numbers and base64 strings
                page_nums = [p[0] for p in batch_pages]
                base64_strings = [p[1] for p in batch_pages]
                
                # Create prompts
                prompts = [pdf_processor.create_extraction_prompt() for _ in range(len(batch_pages))]
                
                logger.info(f"Processing batch of {len(batch_pages)} pages (pages {batch_start+1}-{batch_end})")
                
                # Process batch using specified API type
                try:
                    if api_type == "batch":
                        logger.debug("Using batch API for processing")
                        raw_results = ai_client.process_batch(base64_strings, prompts)
                    else:
                        logger.debug("Using general API for processing")
                        raw_results = ai_client.process_general(base64_strings, prompts)
                    
                    # Process the results
                    logger.debug("Processing batch results")
                    processed_results = pdf_processor.process_batch_results(raw_results, page_nums)
                    all_results.extend(processed_results)
                    
                    logger.info(f"Processed batch {batch_start+1}-{batch_end}")
                except Exception as batch_error:
                    logger.error(f"Error processing batch: {str(batch_error)}")
                    if document_id:
                        update_log(document_id, "", "", "ERROR", f"Batch processing failed: {str(batch_error)}", config)
                    batch_processing_successful = False
                    break
            
            # Check if batch processing was successful
            if not batch_processing_successful:
                unprocessed_files.append(blob_name)
                continue
            
            # Log classification results
            for page_num, category, _ in all_results:
                logger.info(f"Page {page_num+1} classified as: {category}")
            
            # Create CSV and determine confidence level
            logger.info("Creating CSV from extraction results")
            csv_content, invoice_number, total_amount = pdf_processor.create_csv_for_results(
                all_results, filename
            )
            
            if csv_content:
                # Determine confidence level for folder structure
                is_high_confidence = pdf_processor.has_high_confidence(all_results)
                
                # Determine folder path based on confidence
                if is_high_confidence:
                    folder_path = config["azure_storage"]["high_confidence_folder"]
                    logger.info(f"{filename} has HIGH confidence (≥{config['processing']['confidence_threshold']}%)")
                else:
                    folder_path = config["azure_storage"]["low_confidence_folder"]
                    logger.info(f"{filename} has LOW confidence (<{config['processing']['confidence_threshold']}%)")
                
                # Prepare filenames for upload
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                base_filename = os.path.splitext(filename)[0]
                
                # Upload CSV to blob storage
                csv_blob_name = f"{folder_path}{base_filename}_{invoice_number}_{total_amount}_{timestamp}.csv"
                logger.info(f"Uploading CSV to {csv_blob_name}")
                csv_success, csv_url = storage_helper.upload_to_storage(
                    csv_blob_name,
                    csv_content,
                    "text/csv"
                )
                
                # Upload original PDF to appropriate folder
                source_folder = "source_documents/" + folder_path
                source_blob_name = f"{source_folder}{filename}"
                logger.info(f"Uploading source PDF to {source_blob_name}")
                source_success, source_url = storage_helper.upload_to_storage(
                    source_blob_name,
                    blob_content,
                    "application/pdf"
                )
                
                logger.info(f"CSV upload: {'Success' if csv_success else 'Failed'}")
                logger.info(f"Source PDF upload: {'Success' if source_success else 'Failed'}")
                
                if csv_success and source_success:
                    file_processed_successfully = True
                    processed_files.append(blob_name)
                    
                    # Update log with success
                    if document_id:
                        update_log(document_id, csv_blob_name, source_blob_name, "SUCCESS", "File processed successfully", config)
                    
                    if csv_success:
                        logger.info(f"CSV URL: {csv_url}")
                    if source_success:
                        logger.info(f"Source PDF URL: {source_url}")
                else:
                    if document_id:
                        update_log(document_id, csv_blob_name if csv_success else "", source_blob_name if source_success else "", "ERROR", "Upload failed", config)
                    unprocessed_files.append(blob_name)
            else:
                logger.warning(f"No extractable content found in {filename}")
                if document_id:
                    update_log(document_id, "", "", "ERROR", "No extractable content found", config)
                unprocessed_files.append(blob_name)
        
        except Exception as e:
            logger.error(f"Error processing {blob_name}: {str(e)}", exc_info=True)
            if document_id:
                update_log(document_id, "", "", "ERROR", f"Processing failed: {str(e)}", config)
            unprocessed_files.append(blob_name)
    
    # Handle archiving based on configuration
    blob_input_move_on = config.get("archive", {}).get("blob_input_move_on", False)
    
    if blob_input_move_on:
        logger.info("Starting archiving process...")
        logger.info(f"Files to archive - Processed: {len(processed_files)}, Unprocessed: {len(unprocessed_files)}")
        
        if processed_files or unprocessed_files:
            archive_config = config.get("archive", {})
            success, archive_info = storage_helper.move_files_to_archive(
                processed_files, 
                unprocessed_files, 
                archive_config
            )
            
            if success:
                logger.info(f"Successfully archived all files")
                
                # Update logs with archive paths for individual files
                if isinstance(archive_info, dict):
                    # If archive_info contains individual file paths
                    for blob_name in processed_files + unprocessed_files:
                        if blob_name in file_document_ids:
                            document_id = file_document_ids[blob_name]
                            archive_path = archive_info.get(blob_name, "archived")
                            
                            # Get current status from the file tracking
                            current_status = "SUCCESS" if blob_name in processed_files else "ERROR"
                            current_desc = "File processed and archived successfully" if blob_name in processed_files else "File failed processing but archived"
                            
                            # Update with archive path
                            update_log_with_archive(document_id, archive_path, current_status, current_desc, config)
                elif isinstance(archive_info, str):
                    # If archive_info is just a base URL/path
                    for blob_name in processed_files + unprocessed_files:
                        if blob_name in file_document_ids:
                            document_id = file_document_ids[blob_name]
                            filename = blob_name.split('/')[-1]
                            archive_path = f"{archive_info}/{filename}"
                            
                            current_status = "SUCCESS" if blob_name in processed_files else "ERROR"
                            current_desc = "File processed and archived successfully" if blob_name in processed_files else "File failed processing but archived"
                            
                            update_log_with_archive(document_id, archive_path, current_status, current_desc, config)
            else:
                logger.error("Failed to archive files")
                # Update logs to indicate archiving failed
                for blob_name in processed_files + unprocessed_files:
                    if blob_name in file_document_ids:
                        document_id = file_document_ids[blob_name]
                        current_status = "SUCCESS" if blob_name in processed_files else "ERROR"
                        current_desc = f"{'File processed successfully' if blob_name in processed_files else 'File processing failed'} - archiving failed"
                        update_log_with_archive(document_id, "", current_status, current_desc, config)
        else:
            logger.info("No files to archive")
    else:
        logger.info("Archiving is disabled (blob_input_move_on = False)")
    
    # Summary
    logger.info("Processing complete!")
    logger.info(f"Summary:")
    logger.info(f"  Total files processed: {len(pdf_blobs)}")
    logger.info(f"  Successfully processed: {len(processed_files)}")
    logger.info(f"  Failed to process: {len(unprocessed_files)}")
