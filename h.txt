# insert_helper.py
import os
import glob
import pandas as pd

class InsertSQLGenerator:
    def __init__(self, config: dict, input_folder: str, output_folder: str = "generated_inserts"):
        self.input_folder = input_folder
        self.output_folder = output_folder
        self.categories = config.get("categories", {})
        self.category_keys = list(self.categories.keys())

    def get_excel_files(self):
        patterns = ["*.xls", "*.xlsx", "*.xlsm"]
        files = []
        for p in patterns:
            files.extend(glob.glob(os.path.join(self.input_folder, p)))
        return files

    def load_all_sheets(self, file_path):
        try:
            xls = pd.ExcelFile(file_path)
            sheets = []
            for sheet_name in xls.sheet_names:
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    df.columns = df.columns.str.strip()
                    for cfg in self.categories.values():
                        for col_key in ["schema_col", "table_col", "column_col"]:
                            col = cfg.get(col_key)
                            if col in df.columns:
                                df[col] = df[col].ffill()
                    sheets.append(df)
                except Exception as e:
                    print(f"‚ö†Ô∏è Error reading sheet {sheet_name}: {e}")
            return pd.concat(sheets, ignore_index=True) if sheets else pd.DataFrame()
        except Exception as e:
            print(f"‚ùå Error loading {file_path}: {e}")
            return pd.DataFrame()

    def create_row_mappings(self, df):
        row_mappings = []
        valid_rows = df.dropna(how="all")
        for _, row in valid_rows.iterrows():
            mapping = {}
            for cat in self.category_keys:
                cfg = self.categories[cat]
                schema = str(row.get(cfg["schema_col"], "NA")).strip()
                table = str(row.get(cfg["table_col"], "NA")).strip()
                column = str(row.get(cfg["column_col"], "NA")).strip()
                table_comment = str(row.get(cfg.get("table_comment_col", ""), "")).strip()
                column_comment = str(row.get(cfg.get("column_comment_col", ""), "")).strip()

                schema = "NA" if schema in ["nan", "None", ""] else schema
                table = "NA" if table in ["nan", "None", ""] else table
                column = "NA" if column in ["nan", "None", ""] else column
                table_comment = "" if table_comment in ["nan", "None"] else table_comment
                column_comment = "" if column_comment in ["nan", "None"] else column_comment

                mapping[cat] = {
                    "table": (schema, table),
                    "column": column,
                    "table_comment": table_comment,
                    "column_comment": column_comment,
                    "has_na": schema == "NA" or table == "NA" or column == "NA"
                }
            row_mappings.append(mapping)
        return row_mappings

    def generate_sql_for_row(self, mapping):
        """Generate SQL statements per row with corrected logic."""
        edl = mapping.get("EDL", {})
        rdmof = mapping.get("RDMOF", {})
        original = mapping.get("Original_SSR", {})

        edl_na = edl.get("has_na", True)
        rdmof_na = rdmof.get("has_na", True)
        original_na = original.get("has_na", True)

        sql_statements = []

        # CASE 1: All present ‚Üí Original‚ÜíEDL + EDL‚ÜíRDMOF (2 statements)
        if not original_na and not edl_na and not rdmof_na:
            sql_statements.append(self.build_sql(original, edl, description="Original_SSR ‚Üí EDL"))
            sql_statements.append(self.build_sql(edl, rdmof, description="EDL ‚Üí RDMOF"))

        # CASE 2: Only Original‚ÜíRDMOF (EDL NA) (1 statement)
        elif not original_na and edl_na and not rdmof_na:
            sql_statements.append(self.build_sql(original, rdmof, description="Original_SSR ‚Üí RDMOF (EDL is NA)"))

        # CASE 3: Only Original‚ÜíEDL (RDMOF NA) (1 statement)  
        elif not original_na and not edl_na and rdmof_na:
            sql_statements.append(self.build_sql(original, edl, description="Original_SSR ‚Üí EDL (RDMOF is NA)"))

        # CASE 4: Only EDL‚ÜíRDMOF (Original NA) (1 statement)
        elif original_na and not edl_na and not rdmof_na:
            sql_statements.append(self.build_sql(edl, rdmof, description="EDL ‚Üí RDMOF (Original_SSR is NA)"))

        # CASE 5: Anything else ‚Üí commented placeholder (1 comment block)
        else:
            sql_statements.append(self.build_commented_placeholder(mapping, "Multiple categories have NA values"))

        return sql_statements

    def build_sql(self, src, tgt, description="", comment_all=False):
        src_schema, src_table = src.get("table", ("NA", "NA"))
        tgt_schema, tgt_table = tgt.get("table", ("NA", "NA"))
        src_col = src.get("column", "NA")
        tgt_col = tgt.get("column", "NA")

        comments = [f"-- {description}"] if description else []

        # Add additional comments if they exist
        for c in [src.get("table_comment", ""), src.get("column_comment", ""),
                  tgt.get("table_comment", ""), tgt.get("column_comment", "")]:
            if c:
                comments.extend([f"-- {line.strip()}" for line in str(c).splitlines() if line.strip()])

        comment_text = "\n".join(comments)
        if comment_text:
            comment_text += "\n"

        # Check if we should comment out the SQL
        if comment_all or src.get("has_na") or tgt.get("has_na"):
            return (
                f"{comment_text}"
                f"-- Mapping skipped due to NA values\n"
                f"-- {src_schema}.{src_table}.{src_col} ‚Üí {tgt_schema}.{tgt_table}.{tgt_col}\n"
            )
        else:
            return (
                f"{comment_text}"
                f"INSERT INTO {tgt_schema}.{tgt_table} ({tgt_col})\n"
                f"SELECT DISTINCT {src_col} FROM {src_schema}.{src_table};\n"
            )

    def build_commented_placeholder(self, mapping, reason):
        return (
            f"-- {reason}\n"
            f"-- Row skipped: No valid transformation possible\n"
            f"-- Original_SSR: {'NA' if mapping.get('Original_SSR', {}).get('has_na', True) else 'Valid'}\n"
            f"-- EDL: {'NA' if mapping.get('EDL', {}).get('has_na', True) else 'Valid'}\n"
            f"-- RDMOF: {'NA' if mapping.get('RDMOF', {}).get('has_na', True) else 'Valid'}\n"
        )

    def process_file(self, file_path):
        df = self.load_all_sheets(file_path)
        if df.empty:
            return [], set()

        row_maps = self.create_row_mappings(df)
        all_sql = []
        unique_set = set()

        print(f"üìä Processing {len(row_maps)} rows...")
        
        for i, row_map in enumerate(row_maps, 1):
            # Debug info for each row
            original_na = row_map.get('Original_SSR', {}).get('has_na', True)
            edl_na = row_map.get('EDL', {}).get('has_na', True)
            rdmof_na = row_map.get('RDMOF', {}).get('has_na', True)
            
            print(f"Row {i}: Original={'NA' if original_na else 'OK'}, "
                  f"EDL={'NA' if edl_na else 'OK'}, "
                  f"RDMOF={'NA' if rdmof_na else 'OK'}")
            
            sql_list = self.generate_sql_for_row(row_map)
            print(f"   Generated {len(sql_list)} statement(s)")
            
            for sql in sql_list:
                sql_normalized = " ".join(sql.split())
                if sql_normalized not in unique_set:
                    all_sql.append(sql)
                    unique_set.add(sql_normalized)
                else:
                    print(f"   Skipped duplicate statement")

        print(f"üìä Total unique statements: {len(all_sql)}")
        return all_sql, unique_set

    def run(self):
        os.makedirs(self.output_folder, exist_ok=True)
        files = self.get_excel_files()
        if not files:
            print(f"‚ùå No Excel files found in {self.input_folder}")
            return

        total_unique = 0
        print(f"üîÑ Processing {len(files)} Excel file(s)...")

        for f in files:
            print(f"\nüìÑ Processing: {os.path.basename(f)}")
            sql_list, unique_sql = self.process_file(f)

            if sql_list:
                out_file = os.path.splitext(os.path.basename(f))[0] + ".sql"
                out_path = os.path.join(self.output_folder, out_file)
                with open(out_path, "w", encoding="utf-8") as fw:
                    fw.write(f"-- Generated SQL from: {os.path.basename(f)}\n")
                    fw.write(f"-- Total unique statements: {len(sql_list)}\n\n")
                    fw.write("\n".join(sql_list))
                total_unique += len(unique_sql)
                print(f"‚úÖ {out_file}: {len(unique_sql)} unique statements")
            else:
                print(f"‚ö†Ô∏è No valid SQL generated for {os.path.basename(f)}")

        print(f"\nüìä Summary: Total unique statements: {total_unique}")


# main.py
import json
from insert_helper import InsertSQLGenerator

def main():
    try:
        with open("schema_config.json", 'r') as f:
            config = json.load(f)
        
        print("üöÄ Starting INSERT SQL Generation...")
        print(f"üìÑ Config loaded from: schema_config.json")
        
        generator = InsertSQLGenerator(
            config=config,
            input_folder=config.get("input_folder", "."),
            output_folder=config.get("output_folder", "generated_inserts")
        )
        generator.run()
        
    except FileNotFoundError:
        print("‚ùå Error: schema_config.json not found")
        print("Please ensure the config file exists in the same directory")
    except KeyError as e:
        print(f"‚ùå Error: Missing required config key: {e}")
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")

if __name__ == "__main__":
    main()


# Example schema_config.json
"""
{
  "input_folder": "excel_files",
  "output_folder": "generated_inserts",
  "categories": {
    "Original_SSR": {
      "schema_col": "Original SSR - Schema",
      "table_col": "Original SSR - Physical Table Name",
      "column_col": "Original SSR - Physical Column Name"
    },
    "EDL": {
      "schema_col": "EDL- Schema", 
      "table_col": "EDL - Physical Table Name",
      "column_col": "EDL - Physical Column Name"
    },
    "RDMOF": {
      "schema_col": "RDMOF - Schema",
      "table_col": "RDMOF - Physical Table Name", 
      "column_col": "RDMOF - Physical Column Name"
    }
  }
}
"""
