import os
import re
import shutil
import logging
import traceback
from datetime import datetime
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
import PyPDF2

# Configuration
INPUT_FOLDER = r"path/to/input/pdfs"  # Input PDF folder
OUTPUT_BASE_FOLDER = r"path/to/output"  # Base output folder
CONFIDENCE_THRESHOLD = 0.6  # 60% confidence threshold
ENDPOINT = "your_azure_endpoint"
KEY = "your_azure_key"
MODEL_ID = "your_custom_model_id"

# Setup logging
log_filename = f'document_classification_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
logging.basicConfig(
    filename=log_filename,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s: %(message)s',
    force=True  # Ensure logging works even if previous handlers exist
)
logger = logging.getLogger()
logger.addHandler(logging.StreamHandler())  # Add console output

# Create DocumentIntelligenceClient
document_intelligence_client = DocumentIntelligenceClient(
    endpoint=ENDPOINT,
    credential=AzureKeyCredential(KEY)
)

# Robust PDF validation
def is_valid_pdf(file_path):
    """
    Validate PDF file
    - Checks file extension
    - Attempts to open and read PDF
    """
    try:
        # Check file extension
        if not file_path.lower().endswith('.pdf'):
            return False
        
        # Attempt to open PDF
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            # Check if PDF has pages
            if len(pdf_reader.pages) == 0:
                logger.warning(f"Empty PDF: {file_path}")
                return False
            
            # Additional checks can be added here
            return True
    except Exception as e:
        logger.error(f"Invalid PDF {file_path}: {e}")
        return False

# Robust filename generation
def generate_unique_filename(base_path, filename, ext='.pdf'):
    """
    Generate a unique filename to prevent overwriting
    """
    base_name = os.path.splitext(filename)[0]
    counter = 1
    new_filename = filename
    
    while os.path.exists(os.path.join(base_path, new_filename)):
        new_filename = f"{base_name}_{counter}{ext}"
        counter += 1
    
    return new_filename

# Sanitize filename
def sanitize_filename(filename):
    """
    Sanitize filename to remove problematic characters
    """
    # Replace or remove problematic characters
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # Remove leading/trailing spaces and periods
    filename = filename.strip('. ')
    return filename

# Setup output folders
def setup_output_folders(base_folder):
    """
    Create source, classified, and unclassified folders
    """
    folders = {
        'source': os.path.join(base_folder, 'source'),
        'classified': os.path.join(base_folder, 'classified'),
        'unclassified': os.path.join(base_folder, 'unclassified')
    }
    
    # Create folders
    for folder in folders.values():
        os.makedirs(folder, exist_ok=True)
    
    return folders

# Extract specific pages from a PDF
def extract_pages(input_path, page_numbers):
    """
    Extract specific pages from a PDF
    
    :param input_path: Path to the source PDF
    :param page_numbers: List of page numbers to extract (1-indexed)
    :return: Path to the new PDF with extracted pages
    """
    try:
        # Read the input PDF
        with open(input_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            writer = PyPDF2.PdfWriter()

            # Add specified pages (adjusting for 0-indexing)
            for page_num in page_numbers:
                writer.add_page(reader.pages[page_num - 1])

            # Create a unique temporary output path
            base_dir = os.path.dirname(input_path)
            temp_filename = f"temp_extracted_pages_{datetime.now().strftime('%Y%m%d%H%M%S%f')}.pdf"
            output_path = os.path.join(base_dir, temp_filename)

            # Write the new PDF
            with open(output_path, 'wb') as output_file:
                writer.write(output_file)

        return output_path
    except Exception as e:
        logger.error(f"Error extracting pages: {e}")
        return None

# Comprehensive PDF processing
def process_pdf_folder(input_folder, output_base_folder):
    # Setup output folders
    output_folders = setup_output_folders(output_base_folder)
    
    # Logging variables
    stats = {
        'total_files_found': 0,
        'total_files_processed': 0,
        'classified_files': 0,
        'unclassified_files': 0,
        'total_pages_processed': 0,
        'classified_pages': 0,
        'document_types': {},
        'errors': []
    }
    
    # Log start of processing
    logger.info(f"Starting comprehensive document classification")
    logger.info(f"Input Folder: {input_folder}")
    logger.info(f"Output Folder: {output_base_folder}")
    logger.info(f"Confidence Threshold: {CONFIDENCE_THRESHOLD}")
    
    # Ensure input folder exists
    if not os.path.exists(input_folder):
        logger.error(f"Input folder {input_folder} does not exist!")
        return
    
    # Comprehensive file discovery
    all_files = []
    for root, dirs, files in os.walk(input_folder):
        for file in files:
            full_path = os.path.join(root, file)
            all_files.append(full_path)
    
    logger.info(f"Total files found: {len(all_files)}")
    
    # Process all discovered files
    for input_path in all_files:
        # Validate PDF
        if not is_valid_pdf(input_path):
            logger.warning(f"Skipping invalid PDF: {input_path}")
            continue
        
        try:
            # Increment total files found
            stats['total_files_found'] += 1
            
            # Generate safe filename
            filename = os.path.basename(input_path)
            sanitized_filename = sanitize_filename(filename)
            
            # Copy to source folder with unique name
            source_filename = generate_unique_filename(output_folders['source'], sanitized_filename)
            source_output_path = os.path.join(output_folders['source'], source_filename)
            shutil.copy2(input_path, source_output_path)
            
            # Open the PDF file
            with open(input_path, "rb") as file:
                # Read PDF to get total page count
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                stats['total_pages_processed'] += total_pages
            
            # Classify each page individually
            classified_page_groups = {}
            
            for page_num in range(1, total_pages + 1):
                # Reset file pointer
                with open(input_path, "rb") as file:
                    # Begin document classification for this page
                    poller = document_intelligence_client.begin_classify_document(
                        classifier_id=MODEL_ID,
                        body=file
                    )
                
                # Get classification result
                result = poller.result()
                
                # Check classification for this page
                for document in result.documents:
                    if document.confidence >= CONFIDENCE_THRESHOLD:
                        doc_type = document.doc_type.lower().replace(' ', '_')
                        
                        # Group pages by their document type
                        if doc_type not in classified_page_groups:
                            classified_page_groups[doc_type] = []
                        
                        classified_page_groups[doc_type].append({
                            'page_num': page_num,
                            'confidence': document.confidence
                        })
                        
                        # Update document type stats
                        if doc_type not in stats['document_types']:
                            stats['document_types'][doc_type] = 0
                        stats['document_types'][doc_type] += 1
                        
                        stats['classified_pages'] += 1
                        break
            
            # Process classified pages
            if classified_page_groups:
                stats['classified_files'] += 1
                stats['total_files_processed'] += 1
                
                # Process each document type
                for doc_type, page_info in classified_page_groups.items():
                    # Create type-specific folder in classified output
                    type_output_folder = os.path.join(output_folders['classified'], doc_type)
                    os.makedirs(type_output_folder, exist_ok=True)
                    
                    # Prepare classified page numbers
                    classified_nums = [page['page_num'] for page in page_info]
                    
                    # Extract classified pages
                    extracted_pdf = extract_pages(input_path, classified_nums)
                    
                    if extracted_pdf:
                        # Generate unique classified filename
                        output_filename = generate_unique_filename(
                            type_output_folder, 
                            f"{source_filename.replace('.pdf', '')}_pages{'_'.join(map(str, classified_nums))}_classified.pdf"
                        )
                        
                        # Full output path for classified PDF
                        output_path = os.path.join(type_output_folder, output_filename)
                        
                        # Move extracted PDF
                        shutil.move(extracted_pdf, output_path)
                        
                        # Logging
                        logger.info(f"Classified File: {filename}")
                        logger.info(f"  - Document Type: {doc_type}")
                        logger.info(f"  - Classified Pages: {classified_nums}")
                        logger.info(f"  - Output: {output_path}")
            else:
                # No pages classified - move to unclassified
                stats['unclassified_files'] += 1
                stats['total_files_processed'] += 1
                
                # Generate unique unclassified filename
                unclassified_filename = generate_unique_filename(output_folders['unclassified'], source_filename)
                
                # Move to unclassified folder
                unclassified_output_path = os.path.join(output_folders['unclassified'], unclassified_filename)
                shutil.copy2(input_path, unclassified_output_path)
                
                # Logging
                logger.info(f"Unclassified File: {filename}")
                logger.info(f"  - Moved to: {unclassified_output_path}")
        
        except Exception as e:
            # Comprehensive error handling
            error_info = {
                'file': input_path,
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            stats['errors'].append(error_info)
            logger.error(f"Error processing {input_path}: {e}")
            logger.error(traceback.format_exc())
    
    # Print and log processing summary
    summary_message = f"""
--- Processing Summary ---
Total Files Found: {stats['total_files_found']}
Total Files Processed: {stats['total_files_processed']}
Classified Files: {stats['classified_files']}
Unclassified Files: {stats['unclassified_files']}
Total Pages Processed: {stats['total_pages_processed']}
Classified Pages: {stats['classified_pages']}
Document Types:
{chr(10).join(f"  - {doc_type}: {count}" for doc_type, count in stats['document_types'].items())}

Errors: {len(stats['errors'])}
"""
    print(summary_message)
    logger.info(summary_message)
    
    # Log detailed errors if any
    if stats['errors']:
        error_log_path = os.path.join(output_base_folder, 'processing_errors.log')
        with open(error_log_path, 'w') as error_file:
            for error in stats['errors']:
                error_file.write(f"File: {error['file']}\n")
                error_file.write(f"Error: {error['error']}\n")
                error_file.write(f"Traceback:\n{error['traceback']}\n\n")
        logger.info(f"Detailed error log saved to {error_log_path}")

# Run the processing
process_pdf_folder(INPUT_FOLDER, OUTPUT_BASE_FOLDER)
```

Key Improvements:
1. Comprehensive File Discovery
   - Walks through entire input folder and subfolders
   - Finds PDFs in nested directories

2. Robust PDF Validation
   - Checks file extension
   - Verifies PDF readability
   - Ensures PDF has pages

3. Unique Filename Generation
   - Prevents overwriting files
   - Handles duplicate filenames
   - Maintains original filename structure

4. Enhanced Error Handling
   - Captures and logs detailed errors
   - Creates a separate error log file
   - Continues processing even if some files fail

5. Improved Logging
   - Console and file logging
   - Detailed processing statistics
   - Tracks processed, classified, and unclassified files

Output Structure:
```
output/
├── source/
│   ├── original_document1.pdf
│   ├── original_document2_1.pdf  # Unique naming if duplicates exist
│   └── ...
│
├── classified/
│   ├── invoice/
│   │   ├── original_document1_pages2_4_classified.pdf
│   │   └── original_document2_pages6_classified.pdf
│   ├── receipt/
│   │   └── original_document3_pages7_classified.pdf
│   └── contract/
│       └── original_document4_pages9_classified.pdf
│
├── unclassified/
│   ├── original_document5.pdf
│   └── ...
│
└── processing_errors.log  # Detailed error log
