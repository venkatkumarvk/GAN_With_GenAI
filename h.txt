import pandas as pd
import re
import os
from typing import List, Tuple, Dict
from collections import defaultdict


class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str, output_base_folder: str = "generated_schemas"):
        self.excel_file_path = excel_file_path
        self.output_base_folder = output_base_folder
        self.tables_created = 0  # Track number of tables created

    def load_excel_data(self) -> pd.DataFrame:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            sheets = excel_file.sheet_names
            print(f"ðŸ“„ Available sheets: {sheets}")
            sheet = next((s for s in sheets if 'BROKER_GROUP_RELATION' in s.upper()), sheets[0])
            df = pd.read_excel(excel_file, sheet_name=sheet)
            print(f"âœ… Loaded sheet: {sheet} with {df.shape[0]} rows and {df.shape[1]} columns.")
            return df
        except Exception as e:
            print(f"âŒ Error loading Excel file: {e}")
            return None

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype):
            return 'VARCHAR(255)'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'VARCHAR(1)'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'VARCHAR(255)'

    def extract_tables_by_physical_name(self, df: pd.DataFrame) -> Dict[str, Dict[str, List[Tuple[str, str]]]]:
        result = {
            "EDL": defaultdict(list),
            "Original_SSR": defaultdict(list),
            "RDMOF": defaultdict(list)
        }
        seen_columns = {
            "EDL": defaultdict(set),
            "Original_SSR": defaultdict(set),
            "RDMOF": defaultdict(set)
        }

        column_map = {}
        for col in df.columns:
            col_upper = col.upper()
            if 'EDL' in col_upper and 'PHYSICAL' in col_upper and 'TABLE' in col_upper:
                column_map['EDL_table'] = col
            elif ('SSR' in col_upper or 'ORIGINAL' in col_upper) and 'PHYSICAL' in col_upper and 'TABLE' in col_upper:
                column_map['SSR_table'] = col
            elif 'RDMOF' in col_upper and 'PHYSICAL' in col_upper and 'TABLE' in col_upper:
                column_map['RDMOF_table'] = col
            elif 'EDL' in col_upper and 'COLUMN' in col_upper:
                column_map['EDL_column'] = col
            elif ('SSR' in col_upper or 'ORIGINAL' in col_upper) and 'COLUMN' in col_upper:
                column_map['SSR_column'] = col
            elif 'RDMOF' in col_upper and 'COLUMN' in col_upper:
                column_map['RDMOF_column'] = col
            elif 'RDMOF' in col_upper and 'DATA' in col_upper:
                column_map['datatype'] = col

        print(f"ðŸ” Found column mappings: {column_map}")

        for _, row in df.iterrows():
            rd_dtype = row.get(column_map.get('datatype', ''), None)

            # EDL
            if 'EDL_table' in column_map and 'EDL_column' in column_map:
                table_name = row.get(column_map['EDL_table'])
                column_name = row.get(column_map['EDL_column'])
                if pd.notna(table_name) and pd.notna(column_name):
                    table_name = str(table_name).strip()
                    column_name = str(column_name).strip()
                    if column_name.lower() not in seen_columns["EDL"][table_name]:
                        result["EDL"][table_name].append((column_name, 'VARCHAR(255)'))
                        seen_columns["EDL"][table_name].add(column_name.lower())

            # SSR
            if 'SSR_table' in column_map and 'SSR_column' in column_map:
                table_name = row.get(column_map['SSR_table'])
                column_name = row.get(column_map['SSR_column'])
                if pd.notna(table_name) and pd.notna(column_name):
                    table_name = str(table_name).strip()
                    column_name = str(column_name).strip()
                    if column_name.lower() not in seen_columns["Original_SSR"][table_name]:
                        result["Original_SSR"][table_name].append((column_name, 'VARCHAR(255)'))
                        seen_columns["Original_SSR"][table_name].add(column_name.lower())

            # RDMOF
            if 'RDMOF_table' in column_map and 'RDMOF_column' in column_map:
                table_name = row.get(column_map['RDMOF_table'])
                column_name = row.get(column_map['RDMOF_column'])
                if pd.notna(table_name) and pd.notna(column_name):
                    table_name = str(table_name).strip()
                    column_name = str(column_name).strip()
                    if column_name.lower() not in seen_columns["RDMOF"][table_name]:
                        dtype = self.map_datatype(rd_dtype)
                        result["RDMOF"][table_name].append((column_name, dtype))
                        seen_columns["RDMOF"][table_name].add(column_name.lower())

        return result

    def create_folder_structure(self):
        folders = ["EDL", "Original_SSR", "RDMOF"]
        created_folders = []
        if not os.path.exists(self.output_base_folder):
            os.makedirs(self.output_base_folder)
            print(f"ðŸ“ Created base folder: {self.output_base_folder}")

        for folder in folders:
            folder_path = os.path.join(self.output_base_folder, folder)
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)
                created_folders.append(folder)
                print(f"ðŸ“ Created folder: {folder_path}")

        if created_folders:
            print(f"âœ… Created {len(created_folders)} category folders")

        return True

    def generate_schema_sql(self, table_name: str, columns: List[Tuple[str, str]], category: str) -> str:
        if not columns:
            return f"-- No columns found for {table_name}"
        sql = f"-- {category} - {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.EDM_Reporting.{table_name} (\n"
        sql += ",\n".join([f"    [{col}] {dtype}" for col, dtype in columns])
        sql += "\n);"
        return sql

    def run(self):
        print("ðŸš€ Starting schema generation by Physical Table Names...")
        self.create_folder_structure()
        df = self.load_excel_data()
        if df is None:
            return

        tables_by_category = self.extract_tables_by_physical_name(df)
        total_tables = 0
        category_stats = {}

        for category, tables in tables_by_category.items():
            category_tables = 0
            category_stats[category] = {}
            print(f"\n{'='*60}")
            print(f"ðŸ“Š {category} CATEGORY")
            print(f"{'='*60}")
            if not tables:
                print(f"âš ï¸ No tables found for {category}")
                continue

            category_folder = os.path.join(self.output_base_folder, category)

            for table_name, columns in tables.items():
                if columns:
                    sql = self.generate_schema_sql(table_name, columns, category)
                    print(f"\nðŸ“„ {table_name} ({len(columns)} columns):\n{sql}")
                    filename = f"{table_name.lower()}.sql"
                    file_path = os.path.join(category_folder, filename)
                    with open(file_path, "w") as f:
                        f.write(sql)
                    print(f"âœ… Saved to: {file_path}")
                    category_tables += 1
                    category_stats[category][table_name] = len(columns)
                else:
                    print(f"âš ï¸ No columns found for {table_name} - skipping")

            total_tables += category_tables
            print(f"\nðŸ“ˆ {category} Summary: {category_tables} tables created in {category_folder}")

        self.tables_created = total_tables

        print(f"\n{'='*60}")
        print(f"ðŸŽ¯ FINAL SUMMARY")
        print(f"{'='*60}")
        print(f"ðŸ—ï¸  Total tables created: {self.tables_created}")
        print(f"ðŸ“ Total SQL files generated: {self.tables_created}")
        print(f"ðŸ“‚ Output folder: {self.output_base_folder}\n")

        print(f"ðŸ“‚ Folder Structure:")
        print(f"   {self.output_base_folder}/")
        for category, tables in category_stats.items():
            if tables:
                print(f"   â”œâ”€â”€ {category}/")
                for table_name, column_count in tables.items():
                    print(f"   â”‚   â”œâ”€â”€ {table_name.lower()}.sql ({column_count} columns)")
            else:
                print(f"   â”œâ”€â”€ {category}/ (empty)")
        print(f"{'='*60}")

    def get_table_count(self) -> int:
        return self.tables_created


# Usage
def main():
    excel_path = "your_excel_file.xlsx"  # Replace with your actual Excel file path
    output_folder = "generated_schemas"  # Replace with desired output folder

    generator = DatabricksSchemaGenerator(excel_path, output_folder)
    generator.run()

    print(f"\nðŸ”¢ Total tables created: {generator.get_table_count()}")


if __name__ == "__main__":
    main()


#test (pyspark and pytest)
import os
import pytest
from pyspark.sql import SparkSession

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder \
        .appName("SchemaValidationTest") \
        .master("local[*]") \
        .config("spark.sql.catalogImplementation", "in-memory") \
        .getOrCreate()
    yield spark
    spark.stop()


def run_sql_file(spark, file_path):
    with open(file_path, "r") as f:
        sql_query = f.read()
    spark.sql(sql_query)


def test_schema_creation(spark):
    # Replace with a valid path to one of your generated .sql files
    sql_file_path = "generated_schemas/RDMOF/sample_table.sql"
    assert os.path.exists(sql_file_path), f"SQL file not found: {sql_file_path}"

    run_sql_file(spark, sql_file_path)

    # Extract table name from file name
    table_name = os.path.basename(sql_file_path).replace(".sql", "")
    db = "external_catalog.edm_reporting"

    # Validate the table exists
    tables = spark.sql(f"SHOW TABLES IN {db}").collect()
    found = any(row.tableName.lower() == table_name.lower() for row in tables)

    assert found, f"Table '{table_name}' not found in database '{db}'"

    # Clean up (optional)
    spark.sql(f"DROP TABLE IF EXISTS {db}.{table_name}")

