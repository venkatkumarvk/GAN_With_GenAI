def create_batch_jsonl_file(image_data_urls, prompts):
    """
    Create a JSONL file for batch processing.
    
    Parameters:
    - image_data_urls: List of image data URLs
    - prompts: List of prompts matching the data URLs
    
    Returns:
    - Path to the created JSONL file
    """
    import json
    import tempfile
    
    # Create a temporary file to store the JSONL data
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.jsonl') as f:
        filepath = f.name
        
        # Write each entry as a JSON line
        for i in range(len(image_data_urls)):
            entry = {
                "messages": [
                    {"role": "system", "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."},
                    {"role": "user", "content": [
                        {"type": "text", "text": prompts[i]},
                        {"type": "image_url", "image_url": {"url": image_data_urls[i]}}
                    ]}
                ],
                "max_tokens": 2000,
                "temperature": 0.5,
                "response_format": {"type": "json_object"}
            }
            f.write(json.dumps(entry) + '\n')
    
    return filepath

#batch
def process_batch_with_azure_openai(client, deployment_name, jsonl_filepath):
    """
    Process a batch using Azure OpenAI's batch API.
    
    Parameters:
    - client: Azure OpenAI client
    - deployment_name: Name of the deployment
    - jsonl_filepath: Path to the JSONL file
    
    Returns:
    - List of results
    """
    import time
    import datetime
    import json
    import os
    
    # Upload the file
    with open(jsonl_filepath, "rb") as f:
        file = client.files.create(
            file=f,
            purpose="batch"
        )
    
    file_id = file.id
    
    # Create the batch job
    batch_response = client.batches.create(
        model=deployment_name,
        input_file_id=file_id,
        endpoint="/chat/completions",
        completion_window="24h"
    )
    
    batch_id = batch_response.id
    
    # Track the batch job
    status = "validating"
    with st.spinner("Processing batch..."):
        status_placeholder = st.empty()
        while status not in ("completed", "failed", "canceled"):
            time.sleep(10)  # Check every 10 seconds - adjust as needed
            batch_response = client.batches.retrieve(batch_id)
            status = batch_response.status
            status_time = datetime.datetime.now().strftime("%H:%M:%S")
            status_placeholder.text(f"{status_time} Batch Id: {batch_id}, Status: {status}")
    
    # Check for errors
    if batch_response.status == "failed":
        error_messages = []
        for error in batch_response.errors.data:
            error_messages.append(f"Error code {error.code}: {error.message}")
        
        error_str = "\n".join(error_messages)
        st.error(f"Batch processing failed:\n{error_str}")
        return []
    
    # Retrieve results
    output_file_id = batch_response.output_file_id
    
    if not output_file_id:
        output_file_id = batch_response.error_file_id
        st.warning("Using error file for results as output file is not available")
    
    if output_file_id:
        file_response = client.files.content(output_file_id)
        raw_responses = file_response.text.strip().split('\n')
        
        results = []
        for raw_response in raw_responses:
            try:
                result = json.loads(raw_response)
                results.append(result)
            except json.JSONDecodeError as e:
                st.warning(f"Could not parse response line: {e}")
        
        # Clean up
        try:
            os.remove(jsonl_filepath)
        except Exception as e:
            st.warning(f"Could not remove temporary JSONL file: {e}")
        
        return results
    
    return []

def process_pdf(pdf_file, prompt_template, client, deployment_name, progress_bar=None, progress_text=None):
    """
    Process a PDF file using Azure OpenAI batch API with JSONL files.
    """
    tmp_path = None
    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_path = tmp_file.name
        
        filename = pdf_file.name
        
        # Open the PDF
        with fitz.open(tmp_path) as doc:
            page_count = len(doc)
            
            if progress_text:
                progress_text.text(f"Processing {filename} - {page_count} pages...")
            
            # Prepare for batch processing
            image_data_urls = []
            page_numbers = []
            prompts = []
            
            # Extract all pages
            for page_num in range(page_count):
                try:
                    # Update progress
                    if progress_bar:
                        progress_bar.progress((page_num + 1) / page_count)
                    
                    # Load page and convert to image
                    page = doc.load_page(page_num)
                    zoom = 2
                    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
                    image_bytes = pix.tobytes()
                    image_data_url = image_to_data_url(image_bytes)
                    
                    # Add to batch lists
                    image_data_urls.append(image_data_url)
                    page_numbers.append(page_num)
                    
                    # Create the prompt for this page
                    classification_prompt = """First, classify this document into one of these categories:
- Terms & Conditions
- General Terms and Conditions
- Sale Order
- Delivery
- Price and Payment
- Warranty
- Other

If and ONLY if the document is in the "Other" category, extract the following information:
1) Vendor name
2) Invoice number
3) Invoice date
4) Customer name
5) Purchase order number
6) Stock code
7) Unit price
8) Invoice amount
9) Freight cost
10) Sales tax
11) Total amount

Format your response as a JSON object with these fields:
{
  "category": "the category name",
  "shouldExtract": true/false,
  "extractedData": {
    // Only include if shouldExtract is true
    "VendorName": {"value": "value", "confidence": 0.95},
    "InvoiceNumber": {"value": "value", "confidence": 0.95},
    ...and so on for all fields
  }
}"""
                    
                    prompts.append(classification_prompt)
                    
                    # Clean memory
                    del image_bytes
                    
                except Exception as e:
                    st.warning(f"Error preparing page {page_num+1}: {e}")
            
            # Create JSONL file for batch processing
            jsonl_filepath = create_batch_jsonl_file(image_data_urls, prompts)
            
            # Process the batch
            all_page_results = []
            
            with st.spinner(f"Processing {len(image_data_urls)} pages from {filename} as a batch..."):
                batch_results = process_batch_with_azure_openai(
                    client,
                    deployment_name,
                    jsonl_filepath
                )
                
                # Match results to pages
                for i, result in enumerate(batch_results):
                    if i < len(page_numbers):
                        page_num = page_numbers[i]
                        
                        try:
                            # Extract the content
                            if 'choices' in result and len(result['choices']) > 0:
                                content_str = result['choices'][0]['message']['content']
                                content = json.loads(content_str)
                                
                                category = content.get("category", "Unknown")
                                
                                # Only process "Other" category for extraction
                                if category == "Other" and content.get("shouldExtract", False):
                                    extracted_info = content.get("extractedData", {})
                                    
                                    # Add to results
                                    extracted_info_with_page = {
                                        "page": page_num + 1,
                                        "data": extracted_info,
                                        "extraction_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                    }
                                    
                                    all_page_results.append(extracted_info_with_page)
                                    
                                    if progress_text:
                                        progress_text.text(f"Extracted data from {filename} - Page {page_num+1} (Category: {category})")
                                elif progress_text:
                                    progress_text.text(f"Skipped extraction for {filename} - Page {page_num+1} (Category: {category})")
                        except Exception as e:
                            st.warning(f"Error processing result for page {page_num+1}: {e}")
            
            # Create final result
            final_result = {
                "filename": filename,
                "total_pages": page_count,
                "pages": all_page_results
            }
            
            return final_result
            
    except Exception as e:
        st.error(f"Error processing {pdf_file.name}: {e}")
        return {
            "filename": pdf_file.name,
            "error": str(e),
            "total_pages": 0,
            "pages": []
        }
    finally:
        # Clean up
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except Exception as e:
                st.warning(f"Could not remove temp file: {e}")
        
        gc.collect()

def process_pdf(pdf_file, prompt_template, client, deployment_name, progress_bar=None, progress_text=None):
    """
    Process a PDF file using Azure OpenAI batch API with JSONL files.
    """
    tmp_path = None
    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_path = tmp_file.name
        
        filename = pdf_file.name
        
        # Open the PDF
        with fitz.open(tmp_path) as doc:
            page_count = len(doc)
            
            if progress_text:
                progress_text.text(f"Processing {filename} - {page_count} pages...")
            
            # Prepare for batch processing
            image_data_urls = []
            page_numbers = []
            prompts = []
            
            # Extract all pages
            for page_num in range(page_count):
                try:
                    # Update progress
                    if progress_bar:
                        progress_bar.progress((page_num + 1) / page_count)
                    
                    # Load page and convert to image
                    page = doc.load_page(page_num)
                    zoom = 2
                    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
                    image_bytes = pix.tobytes()
                    image_data_url = image_to_data_url(image_bytes)
                    
                    # Add to batch lists
                    image_data_urls.append(image_data_url)
                    page_numbers.append(page_num)
                    
                    # Create the prompt for this page
                    classification_prompt = """First, classify this document into one of these categories:
- Terms & Conditions
- General Terms and Conditions
- Sale Order
- Delivery
- Price and Payment
- Warranty
- Other

If and ONLY if the document is in the "Other" category, extract the following information:
1) Vendor name
2) Invoice number
3) Invoice date
4) Customer name
5) Purchase order number
6) Stock code
7) Unit price
8) Invoice amount
9) Freight cost
10) Sales tax
11) Total amount

Format your response as a JSON object with these fields:
{
  "category": "the category name",
  "shouldExtract": true/false,
  "extractedData": {
    // Only include if shouldExtract is true
    "VendorName": {"value": "value", "confidence": 0.95},
    "InvoiceNumber": {"value": "value", "confidence": 0.95},
    ...and so on for all fields
  }
}"""
                    
                    prompts.append(classification_prompt)
                    
                    # Clean memory
                    del image_bytes
                    
                except Exception as e:
                    st.warning(f"Error preparing page {page_num+1}: {e}")
            
            # Create JSONL file for batch processing
            jsonl_filepath = create_batch_jsonl_file(image_data_urls, prompts)
            
            # Process the batch
            all_page_results = []
            
            with st.spinner(f"Processing {len(image_data_urls)} pages from {filename} as a batch..."):
                batch_results = process_batch_with_azure_openai(
                    client,
                    deployment_name,
                    jsonl_filepath
                )
                
                # Match results to pages
                for i, result in enumerate(batch_results):
                    if i < len(page_numbers):
                        page_num = page_numbers[i]
                        
                        try:
                            # Extract the content
                            if 'choices' in result and len(result['choices']) > 0:
                                content_str = result['choices'][0]['message']['content']
                                content = json.loads(content_str)
                                
                                category = content.get("category", "Unknown")
                                
                                # Only process "Other" category for extraction
                                if category == "Other" and content.get("shouldExtract", False):
                                    extracted_info = content.get("extractedData", {})
                                    
                                    # Add to results
                                    extracted_info_with_page = {
                                        "page": page_num + 1,
                                        "data": extracted_info,
                                        "extraction_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                    }
                                    
                                    all_page_results.append(extracted_info_with_page)
                                    
                                    if progress_text:
                                        progress_text.text(f"Extracted data from {filename} - Page {page_num+1} (Category: {category})")
                                elif progress_text:
                                    progress_text.text(f"Skipped extraction for {filename} - Page {page_num+1} (Category: {category})")
                        except Exception as e:
                            st.warning(f"Error processing result for page {page_num+1}: {e}")
            
            # Create final result
            final_result = {
                "filename": filename,
                "total_pages": page_count,
                "pages": all_page_results
            }
            
            return final_result
            
    except Exception as e:
        st.error(f"Error processing {pdf_file.name}: {e}")
        return {
            "filename": pdf_file.name,
            "error": str(e),
            "total_pages": 0,
            "pages": []
        }
    finally:
        # Clean up
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except Exception as e:
                st.warning(f"Could not remove temp file: {e}")
        
        gc.collect()
