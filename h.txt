def prepare_batch_jsonl(image_base64_strings, prompts, model_deployment_name):
    """
    Prepares a JSONL file for batch processing.
    
    Parameters:
    - image_base64_strings: List of base64-encoded image strings
    - prompts: List of corresponding prompts
    - model_deployment_name: The deployment name for the model
    
    Returns:
    - BytesIO object containing the JSONL content
    """
    import json
    from io import BytesIO
    
    jsonl_file = BytesIO()
    
    for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
        # Create the request object with proper data URL format
        request = {
            "custom_id": f"request-{i+1}",
            "method": "POST",
            "url": "/chat/completions",
            "body": {
                "model": model_deployment_name,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_img}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 2000
            }
        }
        
        # Write the JSON line to the file
        jsonl_file.write((json.dumps(request) + "\n").encode('utf-8'))
    
    # Reset the file pointer to the beginning
    jsonl_file.seek(0)
    return jsonl_file

def image_to_data_url(image_bytes, mime_type='image/png'):
    """
    Convert image bytes to a base64 string (without data URL prefix for batch API).
    """
    base64_encoded_data = base64.b64encode(image_bytes).decode('utf-8')
    # Return just the base64 data without the prefix
    return base64_encoded_data

def process_pdf(pdf_file, client, deployment_name, progress_bar=None, progress_text=None):
    """
    Process a PDF file using Azure OpenAI batch API.
    Optimized to minimize temporary file usage.
    """
    try:
        # Get PDF content directly from uploaded file
        pdf_content = pdf_file.getvalue()
        filename = pdf_file.name
        
        # Create BytesIO object from content
        pdf_io = io.BytesIO(pdf_content)
        
        # Open the PDF directly from memory
        doc = fitz.open(stream=pdf_io, filetype="pdf")
        page_count = len(doc)
        
        if progress_text:
            progress_text.text(f"Processing {filename} - {page_count} pages...")
        
        # Prepare data for batch processing
        image_base64_strings = []
        page_numbers = []
        prompts = []
        
        # Extract all pages
        for page_num in range(page_count):
            try:
                # Update progress
                if progress_bar:
                    progress_bar.progress((page_num + 1) / page_count)
                if progress_text:
                    progress_text.text(f"Preparing {filename} - Page {page_num+1}/{page_count}")
                
                # Load page and convert to image
                page = doc.load_page(page_num)
                zoom = 1.5  # Reduced zoom to save memory
                pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
                
                # Convert directly to base64
                image_bytes = pix.tobytes()
                base64_string = base64.b64encode(image_bytes).decode('utf-8')
                
                # Create prompt for this page
                classification_prompt = """First, classify this document into one of these categories:
- Terms & Conditions
- General Terms and Conditions
- Sale Order
- Delivery
- Price and Payment
- Warranty
- Other

If and ONLY if the document is in the "Other" category, extract the following information:
1) Vendor name
2) Invoice number
3) Invoice date
4) Customer name
5) Purchase order number
6) Stock code
7) Unit price
8) Invoice amount
9) Freight cost
10) Sales tax
11) Total amount

Format your response as a JSON object with these fields:
{
  "category": "the category name",
  "shouldExtract": true/false,
  "extractedData": {
    // Only include if shouldExtract is true
    "VendorName": {"value": "value", "confidence": 0.95},
    "InvoiceNumber": {"value": "value", "confidence": 0.95},
    ...and so on for all fields
  }
}"""
                
                # Add to batch data
                image_base64_strings.append(base64_string)
                page_numbers.append(page_num)
                prompts.append(classification_prompt)
                
                # Clean memory immediately
                del image_bytes
                del pix
                del base64_string
                
            except Exception as e:
                st.warning(f"Error preparing page {page_num+1}: {e}")
        
        # Close the document to free memory
        doc.close()
        del doc
        
        # Prepare the JSONL batch file
        jsonl_file = prepare_batch_jsonl(image_base64_strings, prompts, deployment_name)
        
        # Clear these large lists to free memory
        del image_base64_strings
        
        # Process the batch
        all_page_results = []
        status_placeholder = st.empty()
        
        # We need to create a temporary JSONL file as the API might have issues with direct BytesIO
        tmp_jsonl_path = None
        
        try:
            # Create a temporary file for the JSONL content
            with tempfile.NamedTemporaryFile(suffix='.jsonl', delete=False) as tmp_jsonl:
                tmp_jsonl.write(jsonl_file.getvalue())
                tmp_jsonl_path = tmp_jsonl.name
            
            # Upload with explicit file open
            status_placeholder.text("Uploading batch file to Azure...")
            with open(tmp_jsonl_path, 'rb') as f:
                file = client.files.create(
                    file=f,
                    purpose="batch"
                )
            
            # We can delete the temp file immediately after upload
            if tmp_jsonl_path:
                os.unlink(tmp_jsonl_path)
                tmp_jsonl_path = None
            
            file_id = file.id
            status_placeholder.text(f"File uploaded (ID: {file_id}). Creating batch job...")
            
            # Submit batch job
            batch_response = client.batches.create(
                input_file_id=file_id,
                endpoint="/chat/completions",
                completion_window="24h"
            )
            
            batch_id = batch_response.id
            status_placeholder.text(f"Batch job created (ID: {batch_id}). Waiting for processing...")
            
            # Track batch job status
            status = "validating"
            import time
            import datetime
            
            while status not in ("completed", "failed", "canceled"):
                time.sleep(10)  # Check every 10 seconds for faster feedback
                batch_response = client.batches.retrieve(batch_id)
                status = batch_response.status
                status_message = f"{datetime.datetime.now()} Batch Id: {batch_id}, Status: {status}"
                status_placeholder.text(status_message)
            
            if batch_response.status == "failed":
                error_message = "Batch processing failed:"
                if hasattr(batch_response, 'errors') and hasattr(batch_response.errors, 'data'):
                    for error in batch_response.errors.data:
                        error_message += f"\nError code {error.code} Message {error.message}"
                else:
                    error_message += " Unknown error occurred"
                st.error(error_message)
                raise Exception(error_message)
            
            # Retrieve results
            output_file_id = batch_response.output_file_id
            
            if not output_file_id:
                output_file_id = batch_response.error_file_id
                if not output_file_id:
                    st.error("No output or error file was produced by the batch job.")
                    raise Exception("No output file produced")
            
            status_placeholder.text(f"Batch completed. Retrieving results...")
            file_response = client.files.content(output_file_id)
            raw_responses = file_response.text.strip().split('\n')
            
            # Process the batch results
            for raw_response in raw_responses:
                try:
                    json_response = json.loads(raw_response)
                    
                    # Extract the request ID to identify the page
                    request_id = json_response.get("custom_id", "")
                    if request_id.startswith("request-"):
                        idx = int(request_id.split("-")[1]) - 1
                        if idx < len(page_numbers):
                            page_num = page_numbers[idx]
                        else:
                            page_num = -1
                    else:
                        page_num = -1
                    
                    # Process the actual content
                    if "response" in json_response and "body" in json_response["response"]:
                        content = json_response["response"]["body"]
                        if isinstance(content, str):
                            content = json.loads(content)
                        
                        if "choices" in content and len(content["choices"]) > 0:
                            message_content = content["choices"][0]["message"]["content"]
                            result = json.loads(message_content)
                            
                            category = result.get("category", "Unknown")
                            
                            if category == "Other" and result.get("shouldExtract", False):
                                extracted_info = result.get("extractedData", {})
                                
                                # Add to results
                                extracted_info_with_page = {
                                    "page": page_num + 1,
                                    "data": extracted_info,
                                    "extraction_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                }
                                
                                all_page_results.append(extracted_info_with_page)
                                
                                if progress_text:
                                    progress_text.text(f"Extracted data from {filename} - Page {page_num+1} (Category: {category})")
                            elif progress_text:
                                progress_text.text(f"Skipped extraction for {filename} - Page {page_num+1} (Category: {category})")
                except Exception as e:
                    st.warning(f"Error processing result: {str(e)}")
        
        except Exception as e:
            st.error(f"Error during batch processing: {str(e)}")
            raise  # Re-raise to be caught by the outer exception handler
        finally:
            # Ensure temporary file is deleted if it exists
            if tmp_jsonl_path and os.path.exists(tmp_jsonl_path):
                os.unlink(tmp_jsonl_path)
        
        # Create final result
        final_result = {
            "filename": filename,
            "total_pages": page_count,
            "pages": all_page_results
        }
        
        return final_result
            
    except Exception as e:
        st.error(f"Error processing {pdf_file.name}: {str(e)}")
        return {
            "filename": pdf_file.name,
            "error": str(e),
            "total_pages": 0,
            "pages": []
        }
    finally:
        # Clean up resources
        if 'pdf_io' in locals():
            del pdf_io
        if 'jsonl_file' in locals():
            del jsonl_file
        
        # Force garbage collection
        gc.collect()

def process_blob_pdfs(blob_service_client, container_name, pdf_blobs, client, deployment_name, progress_bar=None, progress_text=None):
    """
    Process PDF blobs from Azure Blob Storage using the Azure OpenAI batch API.
    Optimized to minimize temporary file usage.
    """
    all_pdf_results = []
    
    # Create progress tracking
    if progress_bar is None:
        progress_bar = st.progress(0)
    if progress_text is None:
        progress_text = st.empty()
    
    for i, blob_name in enumerate(pdf_blobs):
        progress_text.text(f"Processing file {i+1}/{len(pdf_blobs)}: {blob_name}")
        
        try:
            # Download blob to memory
            blob_content = download_blob_to_memory(blob_service_client, container_name, blob_name)
            
            if blob_content is None:
                st.warning(f"Could not download blob: {blob_name}")
                continue
            
            # Create a BytesIO object from the blob content
            blob_file = io.BytesIO(blob_content)
            filename = blob_name.split('/')[-1]  # Set the filename to the blob name without folder path
            
            # Open the PDF directly from memory
            doc = fitz.open(stream=blob_file, filetype="pdf")
            page_count = len(doc)
            
            progress_text.text(f"Processing {filename} - {page_count} pages...")
            
            # Prepare data for batch processing
            image_base64_strings = []
            page_numbers = []
            prompts = []
            
            # Extract all pages
            for page_num in range(page_count):
                try:
                    # Update progress
                    sub_progress = (i + (page_num + 1) / page_count) / len(pdf_blobs)
                    progress_bar.progress(sub_progress)
                    progress_text.text(f"Preparing {filename} - Page {page_num+1}/{page_count}")
                    
                    # Load page and convert to image
                    page = doc.load_page(page_num)
                    zoom = 1.5  # Reduced zoom to save memory
                    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
                    
                    # Convert directly to base64
                    image_bytes = pix.tobytes()
                    base64_string = base64.b64encode(image_bytes).decode('utf-8')
                    
                    # Create prompt for this page
                    classification_prompt = """First, classify this document into one of these categories:
- Terms & Conditions
- General Terms and Conditions
- Sale Order
- Delivery
- Price and Payment
- Warranty
- Other

If and ONLY if the document is in the "Other" category, extract the following information:
1) Vendor name
2) Invoice number
3) Invoice date
4) Customer name
5) Purchase order number
6) Stock code
7) Unit price
8) Invoice amount
9) Freight cost
10) Sales tax
11) Total amount

Format your response as a JSON object with these fields:
{
  "category": "the category name",
  "shouldExtract": true/false,
  "extractedData": {
    // Only include if shouldExtract is true
    "VendorName": {"value": "value", "confidence": 0.95},
    "InvoiceNumber": {"value": "value", "confidence": 0.95},
    ...and so on for all fields
  }
}"""
                    
                    # Add to batch data
                    image_base64_strings.append(base64_string)
                    page_numbers.append(page_num)
                    prompts.append(classification_prompt)
                    
                    # Clean memory immediately
                    del image_bytes
                    del pix
                    del base64_string
                    
                except Exception as e:
                    st.warning(f"Error preparing page {page_num+1}: {e}")
            
            # Close the document to free memory
            doc.close()
            del doc
            
            # Prepare the JSONL batch file
            jsonl_file = prepare_batch_jsonl(image_base64_strings, prompts, deployment_name)
            
            # Clear these large lists to free memory
            del image_base64_strings
            
            # Process the batch
            all_page_results = []
            status_placeholder = st.empty()
            
            # We need to create a temporary JSONL file as the API might have issues with direct BytesIO
            tmp_jsonl_path = None
            
            try:
                # Create a temporary file for the JSONL content
                with tempfile.NamedTemporaryFile(suffix='.jsonl', delete=False) as tmp_jsonl:
                    tmp_jsonl.write(jsonl_file.getvalue())
                    tmp_jsonl_path = tmp_jsonl.name
                
                try:
                    # Upload with explicit file open
                    status_placeholder.text("Uploading batch file to Azure...")
                    with open(tmp_jsonl_path, 'rb') as f:
                        file = client.files.create(
                            file=f,
                            purpose="batch"
                        )
                    
                    # We can delete the temp file immediately after upload
                    if tmp_jsonl_path:
                        os.unlink(tmp_jsonl_path)
                        tmp_jsonl_path = None
                    
                    file_id = file.id
                    status_placeholder.text(f"File uploaded (ID: {file_id}). Creating batch job...")
                    
                    # Submit batch job
                    batch_response = client.batches.create(
                        input_file_id=file_id,
                        endpoint="/chat/completions",
                        completion_window="24h"
                    )
                    
                    batch_id = batch_response.id
                    status_placeholder.text(f"Batch job created (ID: {batch_id}). Waiting for processing...")
                    
                    # Track batch job status
                    status = "validating"
                    import time
                    import datetime
                    
                    while status not in ("completed", "failed", "canceled"):
                        time.sleep(10)  # Check every 10 seconds for faster feedback
                        batch_response = client.batches.retrieve(batch_id)
                        status = batch_response.status
                        status_message = f"{datetime.datetime.now()} Batch Id: {batch_id}, Status: {status}"
                        status_placeholder.text(status_message)
                    
                    if batch_response.status == "failed":
                        error_message = "Batch processing failed:"
                        if hasattr(batch_response, 'errors') and hasattr(batch_response.errors, 'data'):
                            for error in batch_response.errors.data:
                                error_message += f"\nError code {error.code} Message {error.message}"
                        else:
                            error_message += " Unknown error occurred"
                        st.error(error_message)
                        raise Exception(error_message)
                    
                    # Retrieve results
                    output_file_id = batch_response.output_file_id
                    
                    if not output_file_id:
                        output_file_id = batch_response.error_file_id
                        if not output_file_id:
                            st.error("No output or error file was produced by the batch job.")
                            raise Exception("No output file produced")
                    
                    status_placeholder.text(f"Batch completed. Retrieving results...")
                    file_response = client.files.content(output_file_id)
                    raw_responses = file_response.text.strip().split('\n')
                    
                    # Process the batch results
                    for raw_response in raw_responses:
                        try:
                            json_response = json.loads(raw_response)
                            
                            # Extract the request ID to identify the page
                            request_id = json_response.get("custom_id", "")
                            if request_id.startswith("request-"):
                                idx = int(request_id.split("-")[1]) - 1
                                if idx < len(page_numbers):
                                    page_num = page_numbers[idx]
                                else:
                                    page_num = -1
                            else:
                                page_num = -1
                            
                            # Process the actual content
                            if "response" in json_response and "body" in json_response["response"]:
                                content = json_response["response"]["body"]
                                if isinstance(content, str):
                                    content = json.loads(content)
                                
                                if "choices" in content and len(content["choices"]) > 0:
                                    message_content = content["choices"][0]["message"]["content"]
                                    result = json.loads(message_content)
                                    
                                    category = result.get("category", "Unknown")
                                    
                                    if category == "Other" and result.get("shouldExtract", False):
                                        extracted_info = result.get("extractedData", {})
                                        
                                        # Add to results
                                        extracted_info_with_page = {
                                            "page": page_num + 1,
                                            "data": extracted_info,
                                            "extraction_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                        }
                                        
                                        all_page_results.append(extracted_info_with_page)
                                        
                                        if progress_text:
                                            progress_text.text(f"Extracted data from {filename} - Page {page_num+1} (Category: {category})")
                                    elif progress_text:
                                        progress_text.text(f"Skipped extraction for {filename} - Page {page_num+1} (Category: {category})")
                        except Exception as e:
                            st.warning(f"Error processing result: {str(e)}")
                
                except Exception as e:
                    st.error(f"Error during batch processing: {str(e)}")
                    raise  # Re-raise to be caught by the outer try/except
            
            finally:
                # Ensure temporary file is deleted if it exists
                if tmp_jsonl_path and os.path.exists(tmp_jsonl_path):
                    try:
                        os.unlink(tmp_jsonl_path)
                    except Exception as e:
                        st.warning(f"Could not delete temporary file: {str(e)}")
            
            # Create result object for this PDF
            final_result = {
                "filename": filename,
                "total_pages": page_count,
                "pages": all_page_results
            }
            
            # Add to our collection of all PDF results
            all_pdf_results.append(final_result)
        
        except Exception as e:
            st.error(f"Error processing blob {blob_name}: {str(e)}")
            all_pdf_results.append({
                "filename": blob_name.split('/')[-1],
                "error": str(e),
                "total_pages": 0,
                "pages": []
            })
        finally:
            # Clean up blob content
            if 'blob_content' in locals():
                del blob_content
            if 'blob_file' in locals():
                del blob_file
            if 'jsonl_file' in locals():
                del jsonl_file
            
            # Update overall progress
            progress_bar.progress((i + 1) / len(pdf_blobs))
            
            # Force garbage collection
            gc.collect()
    
    progress_text.text("Processing complete!")
    progress_bar.progress(1.0)
    
    return all_pdf_results
