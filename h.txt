üìÅ PROJECT STRUCTURE
project/
‚îÇ
‚îú‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ helper.py
‚îú‚îÄ‚îÄ process_doc.py
‚îú‚îÄ‚îÄ documentintelligence_ocr.py
‚îú‚îÄ‚îÄ Embedding.py
‚îú‚îÄ‚îÄ AIsearch.py
‚îú‚îÄ‚îÄ rag.py
‚îú‚îÄ‚îÄ prompt.py
‚îú‚îÄ‚îÄ cost_tracking.py

1Ô∏è‚É£ config.json
{
  "Azureblob": {
    "connection_string": "YOUR_BLOB_CONNECTION_STRING",
    "inputcontainer": "testinputcontainer",
    "outputcontainer": "testoutputcontainer"
  },

  "AzureOpenAI": {
    "endpoint": "https://YOUR-OPENAI.openai.azure.com/",
    "api_key": "YOUR_OPENAI_KEY",
    "chat_model": "gpt-5",
    "embedding_model": "text-embedding-3-large",
    "api_version": "2024-02-01"
  },

  "AzureAISearch": {
    "endpoint": "https://YOUR-SEARCH.search.windows.net/",
    "api_key": "YOUR_SEARCH_KEY"
  },

  "DocumentIntelligence": {
    "endpoint": "https://YOUR-DOCINT.cognitiveservices.azure.com/",
    "api_key": "YOUR_DOCINT_KEY"
  },

  "required_fields": [
    "name",
    "dob",
    "passport_number",
    "license_number",
    "expiry_date"
  ],

  "confidence_threshold": 0.90
}

2Ô∏è‚É£ helper.py
import json
import os
from datetime import datetime

SUPPORTED_EXTENSIONS = {
    ".pdf", ".doc", ".docx",
    ".png", ".jpg", ".jpeg", ".tiff", ".bmp"
}

def load_config():
    with open("config.json") as f:
        return json.load(f)

def utc_timestamp():
    return datetime.utcnow().strftime("%Y-%m-%dT%H-%M-%S")

def is_supported(filename):
    return os.path.splitext(filename.lower())[1] in SUPPORTED_EXTENSIONS

3Ô∏è‚É£ prompt.py
def field_extraction_prompt(field, context):
    return f"""
You are a strict information extraction system.

Extract the field: "{field}"

Rules:
- Use ONLY the provided text
- Do NOT guess
- If not found, return null and confidence 0
- Confidence must be between 0 and 1

Return STRICT JSON:
{{
  "value": null,
  "confidence": 0.0,
  "source_document": ""
}}

TEXT:
{context}
"""

4Ô∏è‚É£ documentintelligence_ocr.py
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

class OCRClient:
    def __init__(self, endpoint, key):
        self.client = DocumentAnalysisClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(key)
        )

    def read(self, file_bytes):
        poller = self.client.begin_analyze_document(
            "prebuilt-read", file_bytes
        )
        result = poller.result()

        pages = []
        for page in result.pages:
            pages.append(" ".join([l.content for l in page.lines]))
        return pages

5Ô∏è‚É£ Embedding.py
from openai import AzureOpenAI

class EmbeddingClient:
    def __init__(self, cfg, cost_tracker):
        self.client = AzureOpenAI(
            api_key=cfg["api_key"],
            azure_endpoint=cfg["endpoint"],
            api_version=cfg["api_version"]
        )
        self.model = cfg["embedding_model"]
        self.cost = cost_tracker

    def embed(self, text):
        tokens = max(len(text) // 4, 1)
        self.cost.add_embedding(tokens)

        return self.client.embeddings.create(
            model=self.model,
            input=text
        ).data[0].embedding

6Ô∏è‚É£ AIsearch.py
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import *
from azure.core.credentials import AzureKeyCredential

class AISearch:
    def __init__(self, endpoint, key):
        self.endpoint = endpoint
        self.credential = AzureKeyCredential(key)
        self.index_client = SearchIndexClient(endpoint, self.credential)

    def ensure_index(self, index_name):
        if index_name in [i.name for i in self.index_client.list_indexes()]:
            return

        fields = [
            SimpleField("id", SearchFieldDataType.String, key=True),
            SearchableField("content"),
            SimpleField("document", SearchFieldDataType.String),
            SearchField(
                name="embedding",
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                vector_search_dimensions=3072,
                vector_search_configuration="default"
            )
        ]

        vector = VectorSearch(
            algorithms=[HnswAlgorithmConfiguration(name="default")]
        )

        self.index_client.create_index(
            SearchIndex(name=index_name, fields=fields, vector_search=vector)
        )

    def upload(self, index_name, docs):
        SearchClient(
            self.endpoint, index_name, self.credential
        ).upload_documents(docs)

    def search(self, index_name, vector, k=5):
        client = SearchClient(self.endpoint, index_name, self.credential)
        return list(client.search(
            search_text="",
            vector_queries=[{
                "vector": vector,
                "fields": "embedding",
                "k": k
            }]
        ))

7Ô∏è‚É£ rag.py
import json
from openai import AzureOpenAI
from prompt import field_extraction_prompt

class RAG:
    def __init__(self, cfg, cost_tracker):
        self.client = AzureOpenAI(
            api_key=cfg["api_key"],
            azure_endpoint=cfg["endpoint"],
            api_version=cfg["api_version"]
        )
        self.model = cfg["chat_model"]
        self.cost = cost_tracker

    def extract(self, field, context):
        tokens = max(len(context) // 4, 1)
        self.cost.add_llm(tokens)

        prompt = field_extraction_prompt(field, context)
        res = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}]
        )
        return json.loads(res.choices[0].message.content)

8Ô∏è‚É£ process_doc.py
from azure.storage.blob import BlobServiceClient

def list_providers(conn_str, container, inputfolder):
    client = BlobServiceClient.from_connection_string(conn_str)
    blobs = client.get_container_client(container).list_blobs(
        name_starts_with=inputfolder + "/"
    )

    providers = {}
    for b in blobs:
        parts = b.name.split("/")
        if len(parts) >= 2:
            provider = parts[1]
            providers.setdefault(provider, []).append(b.name)

    return providers

9Ô∏è‚É£ cost_tracking.py
class CostTracker:
    def __init__(self):
        self.embedding_tokens = 0
        self.llm_tokens = 0
        self.ocr_pages = 0

    def add_embedding(self, tokens):
        self.embedding_tokens += tokens

    def add_llm(self, tokens):
        self.llm_tokens += tokens

    def add_ocr_pages(self, pages):
        self.ocr_pages += pages

    def summary(self):
        embedding_cost = (self.embedding_tokens / 1000) * 0.00013
        llm_cost = (self.llm_tokens / 1000) * 0.01
        ocr_cost = (self.ocr_pages / 1000) * 1.50

        total = embedding_cost + llm_cost + ocr_cost

        return f"""
RAG COST SUMMARY
========================
Embedding Tokens : {self.embedding_tokens}
LLM Tokens       : {self.llm_tokens}
OCR Pages        : {self.ocr_pages}

Embedding Cost   : ${embedding_cost:.4f}
LLM Cost         : ${llm_cost:.4f}
OCR Cost         : ${ocr_cost:.4f}
------------------------
TOTAL COST       : ${total:.4f}
"""

üîü main.py (FULL PIPELINE + COST FILE)
import argparse, io, csv, json
from azure.storage.blob import BlobServiceClient
from helper import load_config, utc_timestamp, is_supported
from process_doc import list_providers
from documentintelligence_ocr import OCRClient
from Embedding import EmbeddingClient
from AIsearch import AISearch
from rag import RAG
from cost_tracking import CostTracker

parser = argparse.ArgumentParser()
parser.add_argument("--inputfolder", required=True)
parser.add_argument("--outputfolder", required=True)
parser.add_argument("--ocr", default="true")
args = parser.parse_args()

OCR_ENABLED = args.ocr.lower() == "true"
cfg = load_config()
timestamp = utc_timestamp()

blob_cfg = cfg["Azureblob"]
container_in = blob_cfg["inputcontainer"]
container_out = blob_cfg["outputcontainer"]

blob_service = BlobServiceClient.from_connection_string(
    blob_cfg["connection_string"]
)

providers = list_providers(
    blob_cfg["connection_string"],
    container_in,
    args.inputfolder
)

cost = CostTracker()

ocr = OCRClient(
    cfg["DocumentIntelligence"]["endpoint"],
    cfg["DocumentIntelligence"]["api_key"]
)
embedder = EmbeddingClient(cfg["AzureOpenAI"], cost)
search = AISearch(
    cfg["AzureAISearch"]["endpoint"],
    cfg["AzureAISearch"]["api_key"]
)
rag = RAG(cfg["AzureOpenAI"], cost)

for provider, files in providers.items():
    log_lines = [f"[{timestamp}] Provider {provider} START"]
    index_name = f"{provider.lower()}-index"
    search.ensure_index(index_name)

    chunks = []

    for path in files:
        filename = path.split("/")[-1]

        if not is_supported(filename):
            log_lines.append(f"[WARN] Unsupported file skipped: {filename}")
            continue

        data = blob_service.get_blob_client(
            container_in, path
        ).download_blob().readall()

        pages = ocr.read(data) if OCR_ENABLED else [data.decode(errors="ignore")]
        cost.add_ocr_pages(len(pages))

        for i, text in enumerate(pages):
            chunks.append({
                "id": f"{filename}_{i}",
                "content": text,
                "document": filename,
                "embedding": embedder.embed(text)
            })

    if chunks:
        search.upload(index_name, chunks)

    results = {}
    low_conf = False

    for field in cfg["required_fields"]:
        vec = embedder.embed(field)
        hits = search.search(index_name, vec)
        context = "\n".join([h["content"] for h in hits])
        res = rag.extract(field, context)

        if res["confidence"] < cfg["confidence_threshold"]:
            low_conf = True

        results[field] = res

    bucket = "Lowconfidence" if low_conf else "Highconfidence"

    csv_buf = io.StringIO()
    writer = csv.writer(csv_buf)
    header = ["id", "extraction_datetime"]
    row = [provider, timestamp]

    for f, v in results.items():
        header += [f, f"{f}_confidence", f"{f}_sourcedocument"]
        row += [v["value"], v["confidence"], v["source_document"]]

    writer.writerow(header)
    writer.writerow(row)

    blob_service.get_blob_client(
        container_out,
        f"{args.outputfolder}/{bucket}/processedcsvresult/{provider}.csv"
    ).upload_blob(csv_buf.getvalue(), overwrite=True)

    blob_service.get_blob_client(
        container_out,
        f"{args.outputfolder}/{bucket}/processedjsonresult/{provider}.json"
    ).upload_blob(json.dumps(results, indent=2), overwrite=True)

    log_lines.append(f"[{timestamp}] Provider {provider} END")
    blob_service.get_blob_client(
        container_out,
        f"{args.outputfolder}/logs/{provider}_{timestamp}.log"
    ).upload_blob("\n".join(log_lines), overwrite=True)

# COST FILE (per run)
blob_service.get_blob_client(
    container_out,
    f"{args.outputfolder}/cost/run_{timestamp}.txt"
).upload_blob(cost.summary(), overwrite=True)
