{
  "document_intelligence": {
    "endpoint": "https://<your-doc-intelligence-resource>.cognitiveservices.azure.com/",
    "api_key": "<your-doc-intelligence-key>",
    "project_name": "custom-doc-classifier",
    "model_id": "custom-classifier-v1"
  },
  "storage": {
    "connection_string": "<your-azure-storage-connection-string>",
    "container_name": "document-training-data"
  },
  "paths": {
    "reference_dir": "reference/",
    "input_dir": "input/",
    "output_dir": "output/",
    "classified_dir": "output/classified/",
    "unclassified_dir": "output/unclassified/",
    "source_dir": "output/source/"
  },
  "runtime": {
    "confidence_threshold": 0.9,
    "auto_retrain": true
  }
}


----

helper.py

import os
import hashlib
from azure.storage.blob import BlobServiceClient

def upload_reference_to_blob(config):
    """Uploads reference data to Azure Blob Storage."""
    conn_str = config["storage"]["connection_string"]
    container_name = config["storage"]["container_name"]
    reference_dir = config["paths"]["reference_dir"]

    blob_service_client = BlobServiceClient.from_connection_string(conn_str)
    container_client = blob_service_client.get_container_client(container_name)

    # Create container if not exists
    try:
        container_client.create_container()
    except Exception:
        pass  # already exists

    uploaded = False
    for root, _, files in os.walk(reference_dir):
        for file in files:
            local_path = os.path.join(root, file)
            blob_path = os.path.relpath(local_path, reference_dir).replace("\\", "/")
            blob_client = container_client.get_blob_client(blob_path)

            with open(local_path, "rb") as data:
                blob_data = data.read()
                blob_hash = hashlib.md5(blob_data).hexdigest()
                exists = False

                # Check if already uploaded (compare hashes)
                if blob_client.exists():
                    props = blob_client.get_blob_properties()
                    if "content_md5" in props and props["content_md5"]:
                        exists = True

                if not exists:
                    blob_client.upload_blob(blob_data, overwrite=True)
                    uploaded = True
    if uploaded:
        print("‚úÖ New reference data uploaded to Blob Storage.")
    else:
        print("‚ÑπÔ∏è No new reference files found.")
    return uploaded


def split_pdf_to_pages(input_pdf, output_dir):
    """Split PDF into pages and save as temporary files."""
    from PyPDF2 import PdfReader, PdfWriter
    os.makedirs(output_dir, exist_ok=True)
    reader = PdfReader(input_pdf)
    total_pages = len(reader.pages)

    for i, page in enumerate(reader.pages, start=1):
        writer = PdfWriter()
        writer.add_page(page)
        page_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(input_pdf))[0]}_page{i}.pdf")
        with open(page_path, "wb") as out_file:
            writer.write(out_file)
    return total_pages


----
main.py

import os
import json
import shutil
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import (
    DocumentIntelligenceClient,
    DocumentIntelligenceAdministrationClient,
)
from azure.ai.documentintelligence.models import (
    BuildDocumentClassifierRequest,
    AzureBlobSource,
)
from helper import upload_reference_to_blob, split_pdf_to_pages


def train_model(config):
    """
    Trains a custom classifier using Azure Document Intelligence
    with reference data uploaded to Blob Storage.
    """
    endpoint = config["document_intelligence"]["endpoint"]
    key = config["document_intelligence"]["api_key"]
    container_name = config["storage"]["container_name"]

    print("üöÄ Starting model training ...")
    admin_client = DocumentIntelligenceAdministrationClient(
        endpoint, AzureKeyCredential(key)
    )

    build_request = BuildDocumentClassifierRequest(
        azure_blob_source=AzureBlobSource(
            container_url=f"https://<your-storage-account>.blob.core.windows.net/{container_name}"
        ),
        description="Custom document classification model (auto-generated)",
    )

    poller = admin_client.begin_build_document_classifier(build_request)
    model = poller.result()

    print(f"‚úÖ Model training completed successfully.")
    print(f"üÜî New model ID: {model.model_id}")

    # Save model_id back into config.json
    config["document_intelligence"]["model_id"] = model.model_id
    with open("config.json", "w") as f:
        json.dump(config, f, indent=4)

    return model.model_id


def classify_documents(config):
    """
    Classifies PDFs and images into folders based on model predictions.
    """
    endpoint = config["document_intelligence"]["endpoint"]
    key = config["document_intelligence"]["api_key"]
    model_id = config["document_intelligence"]["model_id"]

    input_dir = config["paths"]["input_dir"]
    output_dir = config["paths"]["output_dir"]
    classified_dir = config["paths"]["classified_dir"]
    unclassified_dir = config["paths"]["unclassified_dir"]
    source_dir = config["paths"]["source_dir"]
    confidence_threshold = config["runtime"]["confidence_threshold"]

    # Ensure all directories exist
    for path in [output_dir, classified_dir, unclassified_dir, source_dir]:
        os.makedirs(path, exist_ok=True)

    client = DocumentIntelligenceClient(endpoint, AzureKeyCredential(key))

    for file in os.listdir(input_dir):
        file_path = os.path.join(input_dir, file)
        if not os.path.isfile(file_path):
            continue
        if not file.lower().endswith((".pdf", ".png", ".jpg", ".jpeg")):
            continue

        print(f"üìÑ Processing {file} ...")

        # Copy original file to source_dir
        shutil.copy(file_path, os.path.join(source_dir, file))

        temp_dir = "temp_pages"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir, exist_ok=True)

        if file.lower().endswith(".pdf"):
            total_pages = split_pdf_to_pages(file_path, temp_dir)

            for page_file in os.listdir(temp_dir):
                page_path = os.path.join(temp_dir, page_file)
                with open(page_path, "rb") as f:
                    poller = client.begin_classify_document(
                        model_id=model_id, document=f
                    )
                result = poller.result()

                if (
                    result.documents
                    and result.documents[0].confidence >= confidence_threshold
                ):
                    category = result.documents[0].doc_type
                else:
                    category = "unclassified"

                dest_folder = (
                    unclassified_dir
                    if category == "unclassified"
                    else os.path.join(classified_dir, category)
                )
                os.makedirs(dest_folder, exist_ok=True)

                dest_path = os.path.join(dest_folder, page_file)
                shutil.move(page_path, dest_path)

                print(f"   ‚Üí {page_file} ‚Üí {category}")

            print(f"‚úÖ {file}: {total_pages} pages classified.")
        else:
            with open(file_path, "rb") as f:
                poller = client.begin_classify_document(model_id=model_id, document=f)
            result = poller.result()

            if (
                result.documents
                and result.documents[0].confidence >= confidence_threshold
            ):
                category = result.documents[0].doc_type
            else:
                category = "unclassified"

            dest_folder = (
                unclassified_dir
                if category == "unclassified"
                else os.path.join(classified_dir, category)
            )
            os.makedirs(dest_folder, exist_ok=True)
            shutil.move(file_path, os.path.join(dest_folder, file))

            print(f"‚úÖ {file} classified as {category}")

    print("üéØ Classification complete.")


if __name__ == "__main__":
    with open("config.json", "r") as f:
        config = json.load(f)

    print("üîß Checking reference data ...")
    new_data_uploaded = upload_reference_to_blob(config)

    # Retrain only if new data found
    if config["runtime"]["auto_retrain"] and new_data_uploaded:
        model_id = train_model(config)
    else:
        model_id = config["document_intelligence"]["model_id"]
        print(f"‚ÑπÔ∏è Using existing model ID: {model_id}")

    classify_documents(config)
