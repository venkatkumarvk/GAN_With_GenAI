config.json

  {
  "azure_openai": {
    "api_key": "YOUR_AZURE_OPENAI_KEY",
    "endpoint": "https://YOUR-RESOURCE-NAME.openai.azure.com/",
    "deployment_name": "gpt-4o",
    "api_version": "2024-05-01-preview"
  },
  "categories": {
    "cms1500": ["cadwell", "rhymlink", "medical"],
    "invoice": ["tesla", "amazon", "utility"],
    "scheduling": ["email", "iomrequest", "appointment"],
    "contract": ["employment", "service", "nda"],
    "financial": ["bank_statement", "tax_document", "invoice"]
  },
  "paths": {
    "reference_dir": "reference",
    "input_dir": "input_docs",
    "output_dir": "output"
  },
  "last_reference_snapshot": ""
}

-----------------------

  helper.py

  import os
import json
import hashlib
import logging
from openai import AzureOpenAI

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='w'
)

def load_config(config_path="config.json"):
    """
    Load configuration from JSON file
    """
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error(f"Config file not found: {config_path}")
        raise
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON in config file: {config_path}")
        raise

def compute_reference_hash(ref_dir):
    """
    Compute a hash of reference directory contents
    """
    hasher = hashlib.sha256()
    
    for root, _, files in os.walk(ref_dir):
        for f in sorted(files):
            path = os.path.join(root, f)
            
            # Update hash with file path and modification time
            hasher.update(path.encode())
            hasher.update(str(os.path.getmtime(path)).encode())
            
            # Include file contents for precise tracking
            try:
                with open(path, 'rb') as file:
                    hasher.update(file.read())
            except Exception as e:
                logging.warning(f"Could not read file {path} for hashing: {e}")
    
    return hasher.hexdigest()

def fine_tune_if_new_reference():
    """
    Check if reference data has changed
    """
    cfg = load_config()
    ref_dir = cfg["paths"]["reference_dir"]
    
    # Ensure reference directory exists
    os.makedirs(ref_dir, exist_ok=True)
    
    hash_now = compute_reference_hash(ref_dir)

    # Compare current hash with last snapshot
    if hash_now == cfg.get("last_reference_snapshot", ""):
        logging.info("âœ… No new references. Skipping fine-tuning.")
        return False

    logging.info("ðŸš€ New reference data detected â€” preparing fine-tune dataset...")
    
    try:
        # Simulate fine-tuning (replace with actual fine-tuning logic if needed)
        fine_tuned_model = f"ft:gpt-4o:doc_classifier"
        
        # Update config
        cfg["azure_openai"]["deployment_name"] = fine_tuned_model
        cfg["last_reference_snapshot"] = hash_now
        
        with open("config.json", "w") as f:
            json.dump(cfg, f, indent=2)
        
        logging.info(f"âœ… Fine-tune complete. Using model: {fine_tuned_model}")
        return True
    
    except Exception as e:
        logging.error(f"Fine-tuning failed: {e}")
        return False

def get_azure_client():
    """
    Initialize Azure OpenAI client
    """
    cfg = load_config()
    try:
        client = AzureOpenAI(
            api_key=cfg["azure_openai"]["api_key"],
            api_version=cfg["azure_openai"]["api_version"],
            azure_endpoint=cfg["azure_openai"]["endpoint"],
        )
        return client, cfg["azure_openai"]["deployment_name"]
    except Exception as e:
        logging.error(f"Failed to initialize Azure client: {e}")
        raise


----------------------

main.py

import os
import shutil
import fitz  # PyMuPDF
import logging
from PIL import Image
from io import BytesIO
from helper import load_config, fine_tune_if_new_reference, get_azure_client

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='w'
)

def extract_page_for_classification(file_path):
    """
    Automatically extract the most suitable page for classification
    """
    ext = os.path.splitext(file_path)[1].lower()
    
    try:
        if ext == '.pdf':
            # Open PDF and determine best page for classification
            doc = fitz.open(file_path)
            
            # If document has fewer pages, use first page
            # If 3+ pages, use middle page
            if len(doc) <= 2:
                page_num = 0
            else:
                page_num = len(doc) // 2  # Middle page
            
            page = doc.load_page(page_num)
            pix = page.get_pixmap()
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # Extract the specific page as a new PDF
            pdf_writer = fitz.open()
            pdf_writer.insert_pdf(doc, from_page=page_num, to_page=page_num)
            
            doc.close()
            return img.tobytes(), pdf_writer, page_num
        
        else:
            raise ValueError(f"Unsupported file type: {ext}")
    
    except Exception as e:
        logging.error(f"Error extracting page for {file_path}: {e}")
        # Fallback to a blank image
        img = Image.new('RGB', (800, 200), color='white')
        return img.tobytes(), None, 0

def classify_page(image_bytes, client, deployment_name, cfg):
    """
    Automatically classify page with predefined categories
    """
    prompt = f"""
    You are an advanced document classifier. 
    Classify this document page into one main category and one subcategory.
    Possible categories: {list(cfg['categories'].keys())}.
    Subcategories: {cfg['categories']}.
    If unsure, choose the most likely category.
    Respond strictly in format: <main_category>|<sub_category>
    """
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": [{"type": "image", "image_data": image_bytes}]}
            ]
        )
        result = response.choices[0].message.content.strip()
        main_cat, sub_cat = result.split("|")
        return main_cat.strip(), sub_cat.strip()
    except Exception as e:
        logging.error(f"Classification error: {e}")
        return "unknown", "unknown"

def process_documents():
    """
    Fully automated document processing and classification
    """
    # Automatic fine-tuning trigger
    fine_tune_if_new_reference()
    
    # Load configuration and initialize Azure client
    cfg = load_config()
    client, deployment = get_azure_client()

    # Automatic directory setup
    input_dir = cfg["paths"]["input_dir"]
    output_dir = cfg["paths"]["output_dir"]

    # Create necessary directories
    src_dir = os.path.join(output_dir, "source")
    classified_dir = os.path.join(output_dir, "classified")
    unclassified_dir = os.path.join(output_dir, "unclassified")
    
    os.makedirs(src_dir, exist_ok=True)
    os.makedirs(classified_dir, exist_ok=True)
    os.makedirs(unclassified_dir, exist_ok=True)

    # Tracking statistics
    processing_stats = {
        'total_files': 0,
        'processed_files': 0,
        'unclassified_files': 0,
        'classification_breakdown': {}
    }

    # Automated processing of all files
    for fname in os.listdir(input_dir):
        fpath = os.path.join(input_dir, fname)
        
        # Skip directories and hidden files
        if not os.path.isfile(fpath) or fname.startswith('.'):
            continue

        # Increment total files processed
        processing_stats['total_files'] += 1
        logging.info(f"Processing: {fname}")

        # Supported file extensions
        ext = fname.lower().split(".")[-1]
        supported_exts = ['pdf']
        
        if ext not in supported_exts:
            logging.warning(f"Unsupported file type: {fname}")
            continue

        try:
            # Preserve original file in source directory
            shutil.copy(fpath, os.path.join(src_dir, fname))

            # Automatically extract page for classification
            image_bytes, extracted_pdf, page_num = extract_page_for_classification(fpath)

            # Automatic classification
            main_cat, sub_cat = classify_page(image_bytes, client, deployment, cfg)
            
            # Track classification
            if main_cat not in processing_stats['classification_breakdown']:
                processing_stats['classification_breakdown'][main_cat] = {}
            processing_stats['classification_breakdown'][main_cat][sub_cat] = \
                processing_stats['classification_breakdown'][main_cat].get(sub_cat, 0) + 1

            # Determine destination based on classification
            if main_cat == 'unknown':
                # Unclassified handling
                dest_dir = unclassified_dir
                processing_stats['unclassified_files'] += 1
            else:
                # Classified document handling
                dest_dir = os.path.join(classified_dir, main_cat, sub_cat)
                processing_stats['processed_files'] += 1

            # Create destination directory
            os.makedirs(dest_dir, exist_ok=True)

            # Save classified document
            if ext == 'pdf' and extracted_pdf:
                # For PDF, save extracted page
                page_fname = f"{os.path.splitext(fname)[0]}_page{page_num+1}.pdf"
                dest_path = os.path.join(dest_dir, page_fname)
                extracted_pdf.save(dest_path)
                extracted_pdf.close()

        except Exception as e:
            logging.error(f"Error processing {fname}: {e}")
            continue

    # Log processing summary
    logging.info("\n--- Processing Summary ---")
    logging.info(f"Total Files: {processing_stats['total_files']}")
    logging.info(f"Processed Files: {processing_stats['processed_files']}")
    logging.info(f"Unclassified Files: {processing_stats['unclassified_files']}")
    logging.info("Classification Breakdown:")
    for main_cat, subcats in processing_stats['classification_breakdown'].items():
        logging.info(f"  {main_cat}:")
        for subcat, count in subcats.items():
            logging.info(f"    {subcat}: {count}")

    print("\nâœ… Automated Document Processing Complete")

if __name__ == "__main__":
    process_documents()
```

### 4. Reference Folder Structure
```
reference/
â”œâ”€â”€ cms1500/
â”‚   â”œâ”€â”€ cadwell/
â”‚   â”‚   â”œâ”€â”€ medical_form1.pdf
â”‚   â”‚   â””â”€â”€ medical_form2.pdf
â”‚   â””â”€â”€ rhymlink/
â”‚       â”œâ”€â”€ insurance_doc1.pdf
â”‚       â””â”€â”€ insurance_doc2.pdf
â”‚
â”œâ”€â”€ invoice/
â”‚   â”œâ”€â”€ tesla/
â”‚   â”‚   â”œâ”€â”€ invoice1.pdf
â”‚   â”‚   â””â”€â”€ invoice2.pdf
â”‚   â””â”€â”€ amazon/
â”‚       â”œâ”€â”€ order_invoice1.pdf
â”‚       â””â”€â”€ order_invoice2.pdf
â”‚
â””â”€â”€ scheduling/
    â”œâ”€â”€ email/
    â”‚   â”œâ”€â”€ appointment_request1.pdf
    â”‚   â””â”€â”€ appointment_request2.pdf
    â””â”€â”€ iomrequest/
        â”œâ”€â”€ scheduling_request1.pdf
        â””â”€â”€ scheduling_request2.pdf
