def process_and_classify_page(image_data_url, client, deployment_name):
    """
    Single function to both classify a page and extract data if it's an "Other" category.
    Returns a tuple of (category, extracted_info)
    """
    try:
        prompt = """First, classify this document into one of these categories:
- Terms & Conditions
- General Terms and Conditions
- Sale Order
- Delivery
- Price and Payment
- Warranty
- Other

If and ONLY if the document is in the "Other" category, extract the following information:
1) Vendor name
2) Invoice number
3) Invoice date
4) Customer name
5) Purchase order number
6) Stock code
7) Unit price
8) Invoice amount
9) Freight cost
10) Sales tax
11) Total amount

If the document is NOT in the "Other" category, just provide the category name and do not attempt extraction.

Format your response as a JSON object with these fields:
{
  "category": "the category name",
  "shouldExtract": true/false,
  "extractedData": {
    // Only include if shouldExtract is true
    "VendorName": {"value": "value", "confidence": 0.95},
    "InvoiceNumber": {"value": "value", "confidence": 0.95},
    ...and so on for all fields
  }
}
"""

        completion = client.chat.completions.create(
            model=deployment_name,
            messages=[{
                "role": "system",
                "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
            }, {
                "role": "user",
                "content": [{
                    "type": "text",
                    "text": prompt
                }, {
                    "type": "image_url",
                    "image_url": {
                        "url": image_data_url
                    }
                }]
            }],
            max_tokens=2000,
            temperature=0.5,
            response_format={"type": "json_object"}
        )
        
        # Parse the response
        response_content = completion.choices[0].message.content
        result = json.loads(response_content)
        
        # Extract the category and data
        category = result.get("category", "Unknown")
        
        # If we shouldn't extract, return empty data
        if not result.get("shouldExtract", False):
            return category, {"skipped": True, "reason": f"Document classified as '{category}'"}
        
        # Otherwise return the extracted data
        return category, result.get("extractedData", {})
    
    except Exception as e:
        st.error(f"Error processing page: {str(e)}")
        return "Error", {"error": str(e)}

def process_pdf(pdf_file, prompt, client, deployment_name, progress_bar=None, progress_text=None):
    """
    Process a PDF file, classify pages, and extract information only from "Other" pages.
    Memory-optimized version.
    """
    tmp_path = None
    try:
        # Create a temporary file to store the uploaded PDF
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_path = tmp_file.name
        
        # Get the filename
        filename = pdf_file.name
        
        # Open the PDF file
        with fitz.open(tmp_path) as doc:
            page_count = len(doc)
            
            if progress_text:
                progress_text.text(f"Processing {filename} - {page_count} pages...")
            
            # Create a list to store extracted data from all pages
            all_page_results = []
            
            # Create a list to store classification results for all pages
            page_classifications = []
            
            # Process each page in the PDF
            for page_num in range(page_count):
                try:
                    # Update progress
                    if progress_bar:
                        progress_bar.progress((page_num + 1) / page_count)
                    if progress_text:
                        progress_text.text(f"Processing {filename} - Page {page_num+1}/{page_count}")
                    
                    # Load the current page
                    page = doc.load_page(page_num)
                    
                    # Process image
                    zoom = 2  # Zoom factor for image quality
                    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
                    image_bytes = pix.tobytes()
                    
                    # Convert image to data URL
                    image_data_url = image_to_data_url(image_bytes)
                    
                    # Clean up memory immediately
                    del image_bytes
                    
                    # Classify and process the page in one step
                    category, extracted_info = process_and_classify_page(image_data_url, client, deployment_name)
                    
                    # Clean up memory
                    del image_data_url
                    
                    # Store classification result for all pages
                    page_classifications.append({
                        "page": page_num + 1,
                        "category": category,
                        "processed": category == "Other" and "skipped" not in extracted_info
                    })
                    
                    # Log the category
                    if progress_text:
                        if category == "Other":
                            progress_text.text(f"Extracted data from {filename} - Page {page_num+1}/{page_count} (Category: {category})")
                        else:
                            progress_text.text(f"Skipped extraction for {filename} - Page {page_num+1}/{page_count} (Category: {category})")
                    
                    # Only add "Other" pages to our results
                    if category == "Other" and "skipped" not in extracted_info:
                        # Add page info to the collected results
                        extracted_info_with_page = {
                            "page": page_num + 1,
                            "data": extracted_info
                        }
                        
                        # Add to our collection of all page results
                        all_page_results.append(extracted_info_with_page)
                    
                    # Force garbage collection after each page
                    gc.collect()
                    
                except Exception as e:
                    error_msg = f"Error processing page {page_num+1} of {filename}: {e}"
                    st.warning(error_msg)
                    
                    # Add error to classification list
                    page_classifications.append({
                        "page": page_num + 1,
                        "category": "Error",
                        "processed": False
                    })
                    
                    all_page_results.append({
                        "page": page_num + 1,
                        "data": {"error": str(e)}
                    })
        
        # Create a result object that contains all pages' data
        final_result = {
            "filename": filename,
            "total_pages": page_count,
            "pages": all_page_results,
            "page_classifications": page_classifications
        }
        
        return final_result
        
    except Exception as e:
        error_msg = f"Error processing {pdf_file.name}: {e}"
        st.error(error_msg)
        return {
            "filename": pdf_file.name,
            "error": str(e),
            "total_pages": 0,
            "pages": [],
            "page_classifications": []
        }
    finally:
        # Clean up the temporary file
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except Exception as cleanup_error:
                st.warning(f"Could not remove temporary file {tmp_path}: {cleanup_error}")
        
        # Force garbage collection
        gc.collect()

# In the View Results mode section:
view_tabs = st.tabs(["Results Table", "Data by Document", "Classification Report", "Confidence Analysis", "Download Options"])

# ... classification report

with view_tabs[2]:  # Classification Report
    st.subheader("Page Classification Report")
    
    if 'all_pdf_results' in st.session_state and st.session_state.all_pdf_results:
        all_pdf_results = st.session_state.all_pdf_results
        
        # Create tabs for each PDF
        pdf_tabs = st.tabs([pdf_result["filename"] for pdf_result in all_pdf_results])
        
        for i, tab in enumerate(pdf_tabs):
            with tab:
                pdf_result = all_pdf_results[i]
                filename = pdf_result["filename"]
                
                # Get classification data
                if "page_classifications" in pdf_result:
                    # Create DataFrame for classification results
                    class_data = {
                        "Page": [],
                        "Category": [],
                        "Processed for Extraction": []
                    }
                    
                    for page_class in pdf_result["page_classifications"]:
                        class_data["Page"].append(page_class["page"])
                        class_data["Category"].append(page_class["category"])
                        class_data["Processed for Extraction"].append("✅ Yes" if page_class["processed"] else "❌ No")
                    
                    # Display as DataFrame
                    class_df = pd.DataFrame(class_data)
                    st.dataframe(class_df, use_container_width=True)
                    
                    # Show summary statistics
                    st.subheader("Classification Summary")
                    
                    # Count categories
                    category_counts = {}
                    for page_class in pdf_result["page_classifications"]:
                        category = page_class["category"]
                        if category not in category_counts:
                            category_counts[category] = 0
                        category_counts[category] += 1
                    
                    # Display as a bar chart
                    chart_data = pd.DataFrame({
                        "Category": list(category_counts.keys()),
                        "Pages": list(category_counts.values())
                    })
                    
                    # Only show chart if we have data
                    if not chart_data.empty:
                        st.bar_chart(chart_data, x="Category", y="Pages")
                    
                else:
                    st.info(f"No classification data available for {filename}")
    else:
        st.warning("No extraction results available. Please run extraction first.")
