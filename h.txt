import os
import json
import pandas as pd
import re
from typing import List, Dict, Tuple, Optional, Union
from collections import defaultdict

def load_config_from_file(config_file: str) -> Dict:
    try:
        with open(config_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading config file: {e}")
        return {}

class DatabricksSchemaGenerator:
    def __init__(self,
                 excel_file_path: str,
                 output_base_folder: str = "generated_schemas",
                 sheet_name: Optional[str] = None,
                 categories: Optional[Union[List[str], Dict[str, Dict[str, str]]]] = None,
                 default_datatype_col_suffix: str = " - Data Type"): # New parameter for default suffix
        self.excel_file_path = excel_file_path
        self.output_base_folder = output_base_folder
        self.sheet_name = sheet_name
        
        # New logic to handle simplified categories list
        self.categories_config = self._process_categories_input(categories, default_datatype_col_suffix)
        
        self.tables_created = 0

    def _process_categories_input(self, categories_input: Optional[Union[List[str], Dict[str, Dict[str, str]]]], default_datatype_col_suffix: str) -> Dict[str, Dict[str, str]]:
        """
        Processes the categories input.
        If it's a list of strings, it infers the column names.
        If it's already a dictionary (old format), it uses it as is.
        """
        processed_config = {}
        if isinstance(categories_input, list):
            for category_name in categories_input:
                # Standardize category name for column inference (e.g., "Original SSR" -> "Original SSR")
                base_name = category_name.strip()

                cfg = {
                    "schema_col": f"{base_name} - Schema",
                    "table_col": f"{base_name} - Physical Table Name",
                    "column_col": f"{base_name} - Physical Column Name"
                }
                
                # Check for an explicit datatype column if a suffix is provided
                # We'll need to check if this column actually exists in the DataFrame later
                cfg["datatype_col"] = f"{base_name}{default_datatype_col_suffix}"
                
                processed_config[category_name] = cfg
        elif isinstance(categories_input, dict):
            # If the old dictionary format is provided, use it directly
            processed_config = categories_input
        else:
            print("Warning: 'categories' in config.json is neither a list nor a dictionary. No categories will be processed.")
        return processed_config

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype) or not str(datatype).strip(): # Check for NaN and empty strings
            return 'VARCHAR(255)'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'VARCHAR(1)'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'VARCHAR(255)'

    def load_excel_data(self) -> pd.DataFrame:
        try:
            df = pd.read_excel(self.excel_file_path, sheet_name=self.sheet_name)
            
            # Collect all expected column names from all categories
            all_expected_cols = set()
            for cfg in self.categories_config.values():
                all_expected_cols.add(cfg.get("schema_col"))
                all_expected_cols.add(cfg.get("table_col"))
                all_expected_cols.add(cfg.get("column_col"))
                if cfg.get("datatype_col"): # Only add if it's explicitly set in the inferred config
                    all_expected_cols.add(cfg.get("datatype_col"))

            # Filter out None and ensure columns exist in DataFrame
            cols_to_ffill = [col for col in all_expected_cols if col and col in df.columns]
            
            # Apply ffill only to the columns that actually exist and are relevant
            if cols_to_ffill:
                df[cols_to_ffill] = df[cols_to_ffill].ffill()

            # Filter out rows that are entirely empty across all crucial columns
            initial_rows = len(df)
            valid_rows_mask = pd.Series([False] * len(df), index=df.index)

            for cat, cfg in self.categories_config.items():
                schema_col = cfg.get("schema_col")
                table_col = cfg.get("table_col")
                column_col = cfg.get("column_col")

                # Check if all critical columns for this category are present in the DataFrame
                if all(col in df.columns for col in [schema_col, table_col, column_col] if col):
                    current_category_mask = (df[schema_col].notna()) & \
                                            (df[table_col].notna()) & \
                                            (df[column_col].notna()) & \
                                            (df[schema_col].astype(str).str.upper() != "NA") & \
                                            (df[table_col].astype(str).str.upper() != "NA") & \
                                            (df[column_col].astype(str).str.upper() != "NA")
                    valid_rows_mask = valid_rows_mask | current_category_mask
                else:
                    print(f"Warning: Category '{cat}''s essential columns ({schema_col}, {table_col}, {column_col}) not fully found in Excel. Data for this category might be incomplete or skipped.")

            df = df[valid_rows_mask]
            
            if len(df) < initial_rows:
                print(f"Info: {initial_rows - len(df)} rows were dropped due to missing essential data across all configured categories.")
            
            if df.empty:
                print("Warning: After filtering, the DataFrame is empty. No schemas will be generated.")

            return df
        except FileNotFoundError:
            print(f"Error: Excel file not found at '{self.excel_file_path}'. Please check the path in config.json.")
            return pd.DataFrame()
        except KeyError as e:
            print(f"Error: A column expected by config was not found in the Excel file: {e}. Please check your Excel headers and config.json.")
            return pd.DataFrame()
        except Exception as e:
            print(f"Error loading Excel file: {e}")
            return pd.DataFrame()

    def extract_tables(self, df: pd.DataFrame) -> Dict[str, Dict[Tuple[str, str], List[Tuple[str, str]]]]:
        result = {}
        for cat, cfg in self.categories_config.items():
            schema_col = cfg.get('schema_col')
            table_col = cfg.get('table_col')
            column_col = cfg.get('column_col')
            datatype_col = cfg.get('datatype_col') # This will be the inferred name like "RDMOF - Data Type"

            tables = defaultdict(list)
            seen = defaultdict(set)

            # Essential columns must exist for the category to be processed
            if not all(col and col in df.columns for col in [schema_col, table_col, column_col]):
                print(f"Warning: Skipping category '{cat}' because not all essential columns ({schema_col}, {table_col}, {column_col}) were found in the loaded Excel data.")
                continue # Skip this category if its essential columns are missing

            # Filter df specifically for this category's relevant rows if needed (though global filter tries to catch most)
            # This inner loop iterates over the *entire* filtered df, relying on get() to return '' if data is not in columns
            # or if the data is irrelevant to the current category.
            for _, row in df.iterrows():
                schema = str(row.get(schema_col, '')).strip()
                table = str(row.get(table_col, '')).strip()
                column = str(row.get(column_col, '')).strip()
                
                # Only use datatype_col if it exists in the DataFrame, otherwise default to empty string
                dtype_val = str(row.get(datatype_col, '')) if datatype_col and datatype_col in df.columns else ''

                # Ensure schema, table, and column are not empty or 'NA' after stripping
                if not schema or schema.upper() == "NA":
                    schema = 'default_schema' # Assign default if schema is invalid
                if not table or table.upper() == "NA":
                    continue # Skip if table name is missing or 'NA'
                if not column or column.upper() == "NA":
                    continue # Skip if column name is missing or 'NA'

                if column.lower() not in seen[(schema, table)]:
                    dtype = self.map_datatype(dtype_val)
                    tables[(schema, table)].append((column, dtype))
                    seen[(schema, table)].add(column.lower())

            result[cat] = tables
        return result

    def generate_schema_sql(self, schema: str, table: str, columns: List[Tuple[str, str]], category: str) -> str:
        sql = f"-- Category: {category}\n"
        sql += f"-- Table: {schema}.{table}\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.{schema}.{table} (\n"
        sql += ",\n".join([f"    `{col}` {dtype}" for col, dtype in columns]) # Using backticks for column names
        sql += "\n);"
        return sql

    def create_folder_structure(self):
        folders = list(self.categories_config.keys()) + ["consolidated"]
        os.makedirs(self.output_base_folder, exist_ok=True)
        for folder in folders:
            os.makedirs(os.path.join(self.output_base_folder, folder), exist_ok=True)

    def generate_category_consolidated_schema(self, category: str, tables: Dict[Tuple[str, str], List[Tuple[str, str]]]) -> str:
        sql = f"-- {category} CATEGORY - CONSOLIDATED SCHEMA\n\n"
        count = 0
        for (schema, table), cols in sorted(tables.items()): # Sort for consistent output
            if cols:
                sql += self.generate_schema_sql(schema, table, cols, category) + "\n\n"
                count += 1
        sql += f"-- Total tables in {category} consolidated schema: {count}\n"
        return sql, count

    def generate_master_consolidated_schema(self, all_tables: Dict[str, Dict[Tuple[str, str], List[Tuple[str, str]]]]) -> str:
        sql = "-- MASTER CONSOLIDATED SCHEMA\n\n"
        total = 0
        # Sort categories and then tables for consistent output
        for cat in sorted(all_tables.keys()):
            tables = all_tables[cat]
            if tables:
                sql += f"-- CATEGORY: {cat}\n\n"
                for (schema, table), cols in sorted(tables.items()):
                    if cols:
                        sql += self.generate_schema_sql(schema, table, cols, cat) + "\n\n"
                        total += 1
        sql += f"-- Total tables in master consolidated schema: {total}\n"
        return sql

    def run(self):
        print(f"Loading Excel data from: {self.excel_file_path}, sheet: {self.sheet_name}...")
        df = self.load_excel_data()
        if df.empty:
            print("No data processed. Exiting.")
            return

        self.create_folder_structure()
        print("Extracting table schemas from DataFrame...")
        all_tables = self.extract_tables(df)
        total_tables_generated = 0

        for cat, tables in all_tables.items():
            if not tables:
                print(f"No tables found for category '{cat}'. Skipping individual and consolidated schema generation for this category.")
                continue

            folder = os.path.join(self.output_base_folder, cat)
            current_category_table_count = 0
            for (schema, table), cols in tables.items():
                if cols:
                    sql = self.generate_schema_sql(schema, table, cols, cat)
                    # Ensure table name is safe for filename
                    safe_table_name = re.sub(r'[^\w\-_\.]', '_', table).lower() 
                    file_path = os.path.join(folder, f"{safe_table_name}.sql")
                    try:
                        with open(file_path, "w") as f:
                            f.write(sql)
                        current_category_table_count += 1
                    except IOError as e:
                        print(f"Error writing file {file_path}: {e}")
            
            consolidated_sql, cat_count = self.generate_category_consolidated_schema(cat, tables)
            consolidated_file_path = os.path.join(folder, f"{cat.replace(' ', '_').lower()}_consolidated_{cat_count}.sql") # Handle spaces in category names
            try:
                with open(consolidated_file_path, "w") as f:
                    f.write(consolidated_sql)
            except IOError as e:
                print(f"Error writing consolidated file {consolidated_file_path}: {e}")

            total_tables_generated += current_category_table_count
            print(f"Processed {current_category_table_count} tables for category '{cat}'.")

        master_sql = self.generate_master_consolidated_schema(all_tables)
        master_consolidated_folder = os.path.join(self.output_base_folder, "consolidated")
        master_consolidated_file_path = os.path.join(master_consolidated_folder, "all_tables_master_consolidated.sql")
        try:
            with open(master_consolidated_file_path, "w") as f:
                f.write(master_sql)
        except IOError as e:
            print(f"Error writing master consolidated file {master_consolidated_file_path}: {e}")

        print(f"\nSchema generation complete. Total tables generated across all categories: {total_tables_generated}")
