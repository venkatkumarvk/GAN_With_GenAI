{
    "azure_openai": {
        "api_key": "YOUR_AZURE_OPENAI_KEY",
        "endpoint": "https://your-resource-name.openai.azure.com/",
        "deployment_name": "your-gpt-4o-deployment-name",
        "api_version": "2024-02-15-preview"
    },
    "azure_storage": {
        "connection_string": "YOUR_AZURE_STORAGE_CONNECTION_STRING",
        "input_container": "document-input",
        "output_container": "document-output",
        "reference_container": "document-reference"
    },
    "paths": {
        "input_dir": "./input",
        "output_dir": "./output",
        "reference_dir": "./reference"
    },
    "categories": {
        "medical": [
            "insurance_claim", 
            "prescription", 
            "medical_report"
        ],
        "financial": [
            "invoice", 
            "bank_statement", 
            "tax_document"
        ],
        "legal": [
            "contract", 
            "agreement", 
            "legal_notice"
        ]
    },
    "classification": {
        "confidence_threshold": 0.6
    },
    "token_pricing": {
        "gpt-4o": {
            "input": {
                "price_per_million": 2.50,
                "description": "Input token pricing"
            },
            "output": {
                "price_per_million": 10.00,
                "description": "Output token pricing"
            },
            "cached_input": {
                "price_per_million": 1.25,
                "description": "Cached input token pricing"
            }
        }
    }
}


-----------------------------


  helper.py

import os
import json
import logging
from datetime import datetime
from openai import AzureOpenAI
from azure.storage.blob import BlobServiceClient

def load_config(config_path="config.json"):
    """
    Load configuration from JSON file
    """
    try:
        with open(config_path, "r") as f:
            cfg = json.load(f)
        return cfg
    except FileNotFoundError:
        logging.error(f"Config file not found: {config_path}")
        raise
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON in config file: {config_path}")
        raise

def setup_logging(log_dir='logs'):
    """
    Configure logging with timestamped log file
    """
    # Ensure log directory exists
    os.makedirs(log_dir, exist_ok=True)
    
    # Create timestamped log filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f'documentclassification_{timestamp}.log')
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(levelname)s: %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='a'),
            logging.StreamHandler()
        ]
    )
    
    logging.info(f"Log file created: {log_file}")
    return log_file

def calculate_azure_openai_cost(input_tokens, output_tokens, cfg, model='gpt-4o', use_cached=False):
    """
    Calculate the cost of Azure OpenAI API usage
    """
    try:
        pricing = cfg['token_pricing'].get(model, cfg['token_pricing']['gpt-4o'])
        
        # Calculate input token cost
        input_cost = (input_tokens / 1_000_000) * (
            pricing['cached_input']['price_per_million'] if use_cached 
            else pricing['input']['price_per_million']
        )
        
        # Calculate output token cost
        output_cost = (output_tokens / 1_000_000) * pricing['output']['price_per_million']
        
        # Total cost
        total_cost = input_cost + output_cost
        
        return {
            'model': model,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'input_cost': round(input_cost, 4),
            'output_cost': round(output_cost, 4),
            'total_cost': round(total_cost, 4),
            'use_cached': use_cached
        }
    except Exception as e:
        logging.error(f"Token cost calculation error: {e}")
        return {
            'model': model,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'input_cost': 0,
            'output_cost': 0,
            'total_cost': 0,
            'error': str(e)
        }

def get_azure_client(cfg):
    """
    Initialize Azure OpenAI client 
    """
    try:
        client = AzureOpenAI(
            api_key=cfg["azure_openai"]["api_key"],
            api_version=cfg["azure_openai"]["api_version"],
            azure_endpoint=cfg["azure_openai"]["endpoint"]
        )
        return client, cfg["azure_openai"]["deployment_name"]
    except Exception as e:
        logging.critical(f"Azure client initialization error: {e}")
        raise

def prepare_directories(cfg):
    """
    Prepare necessary directories for processing
    """
    directories = [
        cfg['paths']['input_dir'],
        cfg['paths']['output_dir'],
        cfg['paths']['reference_dir'],
        os.path.join(cfg['paths']['output_dir'], 'source'),
        os.path.join(cfg['paths']['output_dir'], 'classified'),
        os.path.join(cfg['paths']['output_dir'], 'unclassified'),
        os.path.join(cfg['paths']['output_dir'], 'logs')
    ]
    
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)

def download_azure_blobs(cfg, container_name, local_dir):
    """
    Download blobs from an Azure Storage container to local directory
    """
    try:
        blob_service_client = BlobServiceClient.from_connection_string(
            cfg['azure_storage']['connection_string']
        )
        container_client = blob_service_client.get_container_client(container_name)
        
        # Ensure local directory exists
        os.makedirs(local_dir, exist_ok=True)
        
        # List and download blobs
        downloaded_files = 0
        for blob in container_client.list_blobs():
            blob_client = container_client.get_blob_client(blob.name)
            local_file_path = os.path.join(local_dir, blob.name)
            
            # Create subdirectories if needed
            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
            
            # Download blob
            with open(local_file_path, "wb") as download_file:
                download_file.write(blob_client.download_blob().readall())
            
            downloaded_files += 1
            logging.info(f"Downloaded: {blob.name}")
        
        logging.info(f"Total files downloaded: {downloaded_files}")
        return downloaded_files
    
    except Exception as e:
        logging.error(f"Azure Blob download error: {e}")
        raise

def upload_azure_blobs(cfg, local_dir, container_name):
    """
    Upload files from local directory to Azure Storage container
    """
    try:
        blob_service_client = BlobServiceClient.from_connection_string(
            cfg['azure_storage']['connection_string']
        )
        container_client = blob_service_client.get_container_client(container_name)
        
        # Walk through local directory
        uploaded_files = 0
        for root, _, files in os.walk(local_dir):
            for file in files:
                local_path = os.path.join(root, file)
                
                # Create blob path (preserve directory structure)
                relative_path = os.path.relpath(local_path, local_dir)
                blob_name = relative_path
                
                # Upload blob
                blob_client = container_client.get_blob_client(blob_name)
                with open(local_path, "rb") as data:
                    blob_client.upload_blob(data, overwrite=True)
                
                uploaded_files += 1
                logging.info(f"Uploaded: {blob_name}")
        
        logging.info(f"Total files uploaded: {uploaded_files}")
        return uploaded_files
    
    except Exception as e:
        logging.error(f"Azure Blob upload error: {e}")
        raise

def validate_azure_containers(cfg):
    """
    Validate existence of Azure Blob Storage containers
    """
    try:
        from azure.storage.blob import BlobServiceClient
        
        # Create blob service client
        blob_service_client = BlobServiceClient.from_connection_string(
            cfg['azure_storage']['connection_string']
        )
        
        # Containers to check
        containers_to_check = [
            cfg['azure_storage']['input_container'],
            cfg['azure_storage']['reference_container'],
            cfg['azure_storage']['output_container']
        ]
        
        # Check each container
        for container_name in containers_to_check:
            try:
                container_client = blob_service_client.get_container_client(container_name)
                
                # Check if container exists
                if not container_client.exists():
                    logging.error(f"Container does not exist: {container_name}")
                    raise ValueError(f"Container {container_name} does not exist")
                
                logging.info(f"Container verified: {container_name}")
            
            except Exception as container_error:
                logging.error(f"Error accessing container {container_name}: {container_error}")
                raise
        
        return True
    
    except Exception as e:
        logging.critical(f"Azure container validation error: {e}")
        raise
-----

  document.py

 import os
import io
import json
import base64
import shutil
import logging
import traceback
import fitz  # PyMuPDF
from PIL import Image
from datetime import datetime

from helper import (
    load_config, 
    setup_logging,
    get_azure_client, 
    prepare_directories,
    calculate_azure_openai_cost
)

def convert_to_pdf(input_file):
    """
    Convert various file types to PDF
    """
    try:
        # Get file extension
        _, ext = os.path.splitext(input_file)
        ext = ext.lower()

        # PDF files are already in the right format
        if ext == '.pdf':
            return input_file

        # Image files conversion
        if ext in ['.jpg', '.jpeg', '.png']:
            img = Image.open(input_file)
            pdf_path = input_file.replace(ext, '.pdf')
            img.save(pdf_path, 'PDF', resolution=100.0)
            return pdf_path

        # For other file types like .doc, .docx, use PyMuPDF
        try:
            doc = fitz.open(input_file)
            pdf_path = input_file.replace(ext, '.pdf')
            doc.save(pdf_path)
            doc.close()
            return pdf_path
        except Exception as conversion_error:
            logging.error(f"Conversion error for {input_file}: {conversion_error}")
            return None

    except Exception as e:
        logging.error(f"PDF conversion error for {input_file}: {e}")
        return None

def preprocess_image_for_classification(image_bytes):
    """
    Preprocess image to ensure compatibility with GPT-4o
    """
    try:
        # Open image from bytes
        img = Image.open(io.BytesIO(image_bytes))
        
        # Convert to RGB if needed
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # Resize image
        img = img.resize((800, 600), Image.LANCZOS)
        
        # Compress image
        buffer = io.BytesIO()
        img.save(buffer, format="PNG", optimize=True, quality=85)
        
        # Convert to base64
        base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        return base64_image
    
    except Exception as e:
        logging.error(f"Image preprocessing error: {e}")
        return None

def prepare_reference_prompt(cfg, reference_dir):
    """
    Generate a detailed prompt using reference document metadata
    """
    reference_details = {}
    
    for main_category in os.listdir(reference_dir):
        main_path = os.path.join(reference_dir, main_category)
        if not os.path.isdir(main_path):
            continue
        
        reference_details[main_category] = {}
        
        for subcategory in os.listdir(main_path):
            subcat_path = os.path.join(main_path, subcategory)
            if not os.path.isdir(subcat_path):
                continue
            
            # Count and list reference documents
            reference_docs = [f for f in os.listdir(subcat_path) 
                              if os.path.isfile(os.path.join(subcat_path, f))]
            
            reference_details[main_category][subcategory] = {
                'document_count': len(reference_docs),
                'document_types': list(set(os.path.splitext(doc)[1] for doc in reference_docs))
            }
    
    return reference_details

def extract_page_image(file_path, page_number):
    """
    Extract image for a specific page from various document types
    """
    try:
        # PDF handling
        if file_path.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            
            # Validate page number
            if page_number < 0 or page_number >= len(doc):
                logging.error(f"Invalid page number {page_number} for {file_path}")
                return _create_blank_image()
            
            page = doc.load_page(page_number)
            pix = page.get_pixmap()
            
            # Validate pixmap
            if not pix or pix.width <= 0 or pix.height <= 0:
                logging.error(f"Invalid page {page_number} in {file_path}")
                return _create_blank_image()
            
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            doc.close()
        
        # Image handling
        elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):
            img = Image.open(file_path)
        
        else:
            logging.error(f"Unsupported file type: {file_path}")
            return _create_blank_image()
        
        # Resize and convert to bytes
        img = img.resize((800, 600), Image.LANCZOS)
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        return buf.getvalue()
    
    except Exception as e:
        logging.error(f"Error extracting page {page_number} from {file_path}: {e}")
        logging.error(traceback.format_exc())
        return _create_blank_image()

def _create_blank_image():
    """
    Create a blank white image for error cases
    """
    img = Image.new('RGB', (800, 600), color='white')
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()

def classify_page(document_image, client, deployment_name, cfg):
    """
    Enhanced page-level classification with reference-based validation
    """
    try:
        # Validate and preprocess image
        base64_image = preprocess_image_for_classification(document_image)
        
        if not base64_image:
            logging.error("Failed to preprocess image")
            raise ValueError("Image preprocessing failed")

        # Prepare reference details
        reference_dir = cfg['paths']['reference_dir']
        reference_details = prepare_reference_prompt(cfg, reference_dir)

        # Prepare reference categories for prompt
        categories = cfg['categories']
        
        # Create a comprehensive prompt for detailed classification
        prompt = f"""
        You are an advanced document classification AI with expertise in precise document categorization.

        CLASSIFICATION OBJECTIVES:
        - Carefully analyze the document page
        - Identify the main document category
        - Determine the specific subcategory
        - Provide a confidence score
        - Explain your reasoning

        AVAILABLE DOCUMENT CATEGORIES:
        {json.dumps(categories, indent=2)}

        REFERENCE DOCUMENT INSIGHTS:
        {json.dumps(reference_details, indent=2)}

        CLASSIFICATION GUIDELINES:
        1. Examine the document's visual structure
        2. Match against reference document characteristics
        3. Be rigorous in category assignment
        4. Use context and visual cues for classification
        5. If no clear match exists, return 'unknown'

        RESPONSE FORMAT:
        {{
            "main_category": "Exact main category (medical/financial/legal) or 'unknown'",
            "subcategory": "Exact subcategory or 'unknown'",
            "confidence_score": 0.0-1.0,
            "reasoning": "Detailed explanation of classification decision"
        }}

        CRITICAL EVALUATION CRITERIA:
        - Assess document layout
        - Analyze text and visual elements
        - Compare against reference document structure
        - Provide objective, data-driven classification
        """

        # Make API call with explicit error handling
        try:
            response = client.chat.completions.create(
                model=deployment_name,
                messages=[
                    {"role": "system", "content": prompt},
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Classify this document page based on the provided guidelines."
                            },
                            {
                                "type": "image",
                                "image_base64": base64_image
                            }
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                max_tokens=300  # Limit response length
            )

            # Token usage tracking
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens

            # Calculate cost 
            token_cost = calculate_azure_openai_cost(
                input_tokens, 
                output_tokens, 
                cfg
            )

            # Extract and parse response
            result = json.loads(response.choices[0].message.content)

            # Extract classification details
            main_category = result.get('main_category', 'unknown').lower()
            subcategory = result.get('subcategory', 'unknown').lower()
            confidence_score = float(result.get('confidence_score', 0.0))
            reasoning = result.get('reasoning', 'No reasoning provided')

            # Validate main category
            if main_category not in categories:
                main_category = 'unknown'
                subcategory = 'unknown'
                confidence_score = 0.0

            # Validate subcategory
            if main_category != 'unknown':
                valid_subcategories = categories.get(main_category, [])
                if subcategory not in valid_subcategories:
                    subcategory = 'unknown'
                    confidence_score = min(confidence_score, 0.3)

            # Additional confidence adjustment based on reference documents
            reference_document_count = reference_details.get(main_category, {}).get(subcategory, {}).get('document_count', 0)
            if reference_document_count > 0:
                confidence_score *= 1.2  # Boost confidence if reference documents exist
            
            # Cap confidence score
            confidence_score = min(confidence_score, 1.0)

            # Log detailed classification
            logging.info(f"Classification Details:\n" + json.dumps({
                'main_category': main_category,
                'subcategory': subcategory,
                'confidence_score': confidence_score,
                'reasoning': reasoning,
                'reference_document_count': reference_document_count
            }, indent=2))

            return main_category, subcategory, confidence_score, reasoning, token_cost

        except Exception as api_error:
            logging.error(f"API Classification Error: {api_error}")
            logging.error(traceback.format_exc())
            
            return 'unknown', 'unknown', 0.0, f"Classification error: {str(api_error)}", None

    except Exception as e:
        logging.error(f"Page classification error: {e}")
        logging.error(traceback.format_exc())
        
        return 'unknown', 'unknown', 0.0, f"Error in page classification: {str(e)}", None


def process_documents(
    source='local', 
    config_path='config.json', 
    output_dir='./output', 
    input_dir='./input', 
    confidence_threshold=0.6
):
    """
    Comprehensive document processing pipeline with enhanced error handling
    """
    try:
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Create necessary subdirectories
        source_dir = os.path.join(output_dir, 'source')
        classified_dir = os.path.join(output_dir, 'classified')
        unclassified_dir = os.path.join(output_dir, 'unclassified')
        unprocessed_dir = os.path.join(output_dir, 'unprocessed')
        logs_dir = os.path.join(output_dir, 'logs')
        
        # Create all necessary directories
        for directory in [source_dir, classified_dir, unclassified_dir, unprocessed_dir, logs_dir]:
            os.makedirs(directory, exist_ok=True)

        # Setup logging
        log_file = os.path.join(logs_dir, f'documentclassification_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        logging.basicConfig(
            level=logging.INFO, 
            format='%(asctime)s - %(levelname)s: %(message)s',
            handlers=[
                logging.FileHandler(log_file, mode='a'),
                logging.StreamHandler()
            ]
        )
        logging.info(f"Log file created: {log_file}")
        
        # Load configuration
        cfg = load_config(config_path)
        
        # Initialize Azure client
        client, deployment = get_azure_client(cfg)
        
        # Processing statistics
        stats = {
            'total_documents': 0,
            'total_pages': 0,
            'classified_pages': 0,
            'unclassified_pages': 0,
            'total_token_cost': 0.0,
            'total_input_tokens': 0,
            'total_output_tokens': 0,
            'processed_files': [],
            'failed_files': []
        }

        # Process each document
        for fname in os.listdir(input_dir):
            try:
                fpath = os.path.join(input_dir, fname)
                
                # Skip directories and hidden files
                if not os.path.isfile(fpath) or fname.startswith('.'):
                    continue

                # Determine file type
                file_type = os.path.splitext(fname)[1].lower()
                
                # Process only supported file types
                if file_type not in ['.pdf', '.jpg', '.jpeg', '.png']:
                    # Move unprocessable files to unprocessed folder
                    unprocessed_path = os.path.join(unprocessed_dir, fname)
                    shutil.copy2(fpath, unprocessed_path)
                    logging.warning(f"Unsupported file type, moved to unprocessed: {fname}")
                    stats['failed_files'].append(fname)
                    continue

                # Convert to PDF
                pdf_path = convert_to_pdf(fpath)
                if not pdf_path:
                    raise ValueError(f"Failed to convert {fname} to PDF")

                # Copy source PDF
                shutil.copy2(pdf_path, os.path.join(source_dir, os.path.basename(pdf_path)))

                # Open PDF
                doc = fitz.open(pdf_path)
                total_pages = len(doc)
                
                # Update statistics
                stats['total_documents'] += 1
                stats['total_pages'] += total_pages
                stats['processed_files'].append(fname)

                # Classified and unclassified page tracking
                classified_pages = []
                unclassified_pages = []

                # Process each page
                for page_num in range(total_pages):
                    try:
                        # Extract page image
                        page_image = extract_page_image(pdf_path, page_num)
                        
                        # Classify page
                        main_cat, sub_cat, confidence, reasoning, token_cost = classify_page(
                            page_image, client, deployment, cfg
                        )

                        # Update token cost tracking
                        if token_cost:
                            stats['total_token_cost'] += token_cost['total_cost']
                            stats['total_input_tokens'] += token_cost['input_tokens']
                            stats['total_output_tokens'] += token_cost['output_tokens']

                        # Check classification confidence
                        if confidence >= confidence_threshold and main_cat != 'unknown':
                            classified_pages.append({
                                'page': page_num,
                                'main_category': main_cat,
                                'subcategory': sub_cat,
                                'confidence': confidence
                            })
                            stats['classified_pages'] += 1
                        else:
                            unclassified_pages.append(page_num)
                            stats['unclassified_pages'] += 1

                    except Exception as page_error:
                        logging.error(f"Error processing page {page_num} in {fname}: {page_error}")
                        unclassified_pages.append(page_num)

                # Close original document
                doc.close()

                # Process classified pages
                if classified_pages:
                    # Group by subcategory
                    subcategory_pages = {}
                    for page_info in classified_pages:
                        key = (page_info['main_category'], page_info['subcategory'])
                        if key not in subcategory_pages:
                            subcategory_pages[key] = []
                        subcategory_pages[key].append(page_info['page'])

                    # Create PDFs for each subcategory
                    for (main_cat, sub_cat), pages in subcategory_pages.items():
                        # Convert page numbers to 1-based for filename
                        page_nums_str = '_'.join(str(p+1) for p in pages)
                        
                        # Open original PDF
                        doc = fitz.open(pdf_path)
                        classified_doc = fitz.open()
                        
                        # Add classified pages
                        for page_num in pages:
                            classified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                        
                        # Prepare output path
                        category_path = os.path.join(classified_dir, main_cat, sub_cat)
                        os.makedirs(category_path, exist_ok=True)
                        
                        # Save classified PDF
                        output_filename = f"{os.path.splitext(fname)[0]}_classified_{page_nums_str}.pdf"
                        output_path = os.path.join(category_path, output_filename)
                        
                        classified_doc.save(output_path)
                        classified_doc.close()
                        doc.close()

                # Process unclassified pages
                if unclassified_pages:
                    # Convert page numbers to 1-based for filename
                    page_nums_str = '_'.join(str(p+1) for p in unclassified_pages)
                    
                    # Open original PDF
                    doc = fitz.open(pdf_path)
                    unclassified_doc = fitz.open()
                    
                    # Add unclassified pages
                    for page_num in unclassified_pages:
                        unclassified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                    
                    # Save unclassified PDF
                    output_filename = f"{os.path.splitext(fname)[0]}_unclassified_{page_nums_str}.pdf"
                    output_path = os.path.join(unclassified_dir, output_filename)
                    
                    unclassified_doc.save(output_path)
                    unclassified_doc.close()
                    doc.close()

                # Log processing results
                logging.info(f"Document {fname} Processing Summary:")
                logging.info(f"Total Pages: {total_pages}")
                logging.info(f"Classified Pages: {len(classified_pages)}")
                logging.info(f"Unclassified Pages: {len(unclassified_pages)}")

            except Exception as doc_error:
                logging.error(f"Error processing document {fname}: {doc_error}")
                logging.error(traceback.format_exc())
                
                # Move source document to unprocessed folder
                unprocessed_path = os.path.join(unprocessed_dir, fname)
                try:
                    shutil.copy2(fpath, unprocessed_path)
                    logging.info(f"Moved unprocessed document to: {unprocessed_path}")
                except Exception as move_error:
                    logging.error(f"Could not move unprocessed document {fname}: {move_error}")
                
                stats['failed_files'].append(fname)

        # Log overall processing statistics
        logging.info("Processing Session Statistics:")
        logging.info(json.dumps(stats, indent=2))

        # Print summary
        print("Document Processing Complete")
        print(f"Total Documents: {stats['total_documents']}")
        print(f"Total Pages: {stats['total_pages']}")
        print(f"Classified Pages: {stats['classified_pages']}")
        print(f"Unclassified Pages: {stats['unclassified_pages']}")
        print(f"Total Token Cost: ${stats['total_token_cost']:.4f}")
        print(f"Processed Files:", ', '.join(stats['processed_files']))
        print(f"Failed Files:", ', '.join(stats['failed_files']) if stats['failed_files'] else "None")

    except Exception as critical_error:
        logging.critical(f"Critical processing error: {critical_error}")
        logging.critical(traceback.format_exc())
        print(f"Document Processing Failed: {critical_error}")

-----
  main.py

import os
import argparse
from document_process import process_documents
from helper import (
    load_config, 
    download_azure_blobs, 
    upload_azure_blobs
)

def parse_arguments():
    """
    Parse command-line arguments for document processing
    """
    parser = argparse.ArgumentParser(description='Document Classification Pipeline')
    parser.add_argument(
        '--source', 
        choices=['local', 'azure'], 
        default='local',
        help='Source of documents: local filesystem or Azure Blob Storage'
    )
    parser.add_argument(
        '--inputfolder', 
        default='./input',
        help='Input folder path or Azure Blob Storage container name'
    )
    parser.add_argument(
        '--output', 
        default='./output', 
        help='Path to output directory'
    )
    parser.add_argument(
        '--config', 
        default='config.json', 
        help='Path to configuration file'
    )
    parser.add_argument(
        '--confidence', 
        type=float, 
        default=0.6, 
        help='Confidence threshold for classification'
    )
    return parser.parse_args()
def main():
    """
    Main entry point for document processing
    """
    # Parse command-line arguments
    args = parse_arguments()
    
    # Load configuration
    cfg = load_config(args.config)
    
    # Ensure output directory exists
    os.makedirs(args.output, exist_ok=True)
    
    # Determine input source and process documents
    try:
        if args.source == 'local':
            # Process documents from local input folder
            input_folder = args.inputfolder
            os.makedirs(input_folder, exist_ok=True)
            
            # Process documents in the input folder
            process_documents(
                source='local', 
                config_path=args.config,
                output_dir=args.output,
                input_dir=input_folder,
                confidence_threshold=args.confidence
            )
        
        elif args.source == 'azure':
            # Validate Azure containers first
            try:
                validate_azure_containers(cfg)
            except Exception as container_error:
                logging.critical(f"Azure container validation failed: {container_error}")
                print(f"Error: {container_error}")
                return
            
            # Use input container from config or command-line input folder
            input_container = args.inputfolder or cfg['azure_storage']['input_container']
            
            # Download blobs from Azure input container
            local_input_dir = cfg['paths']['input_dir']
            os.makedirs(local_input_dir, exist_ok=True)
            
            # Download blobs from Azure
            downloaded_files = download_azure_blobs(
                cfg, 
                input_container, 
                local_input_dir
            )
            
            # Process downloaded files
            process_documents(
                source='azure', 
                config_path=args.config,
                output_dir=args.output,
                input_dir=local_input_dir,
                confidence_threshold=args.confidence
            )
            
            # Upload processed files to Azure output container
            upload_azure_blobs(
                cfg, 
                cfg['paths']['output_dir'], 
                cfg['azure_storage']['output_container']
            )
    
    except Exception as e:
        print(f"Error processing documents: {e}")
        import traceback
        traceback.print_exc()
