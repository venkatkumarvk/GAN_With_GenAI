import os
import re
import shutil
import logging
import traceback
from datetime import datetime
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
import PyPDF2

# Configuration
INPUT_FOLDER = r"path/to/input/pdfs"  # Input PDF folder
OUTPUT_BASE_FOLDER = r"path/to/output"  # Base output folder
CONFIDENCE_THRESHOLD = 0.6  # 60% confidence threshold
ENDPOINT = "your_azure_endpoint"
KEY = "your_azure_key"
MODEL_ID = "your_custom_model_id"

# Setup logging
def setup_logging():
    log_dir = os.path.join(OUTPUT_BASE_FOLDER, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    log_filename = os.path.join(log_dir, f'document_classification_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger()

# Logger setup
logger = setup_logging()

# Create DocumentIntelligenceClient
document_intelligence_client = DocumentIntelligenceClient(
    endpoint=ENDPOINT,
    credential=AzureKeyCredential(KEY)
)

# Robust PDF validation
def is_valid_pdf(file_path):
    try:
        if not file_path.lower().endswith('.pdf'):
            return False
        
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            if len(pdf_reader.pages) == 0:
                logger.warning(f"Empty PDF: {file_path}")
                return False
            
            return True
    except Exception as e:
        logger.error(f"Invalid PDF {file_path}: {e}")
        return False

# Sanitize filename
def sanitize_filename(filename):
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    return filename.strip('. ')

# Extract specific pages from a PDF
def extract_pages(input_path, page_numbers, original_filename):
    """
    Extract specific pages from a PDF
    
    :param input_path: Path to the source PDF
    :param page_numbers: List of page numbers to extract (1-indexed)
    :param original_filename: Original filename to preserve naming
    :return: Path to the new PDF with extracted pages
    """
    try:
        # Create necessary directories
        temp_dir = os.path.join(OUTPUT_BASE_FOLDER, 'temp')
        os.makedirs(temp_dir, exist_ok=True)
        
        # Generate output filename maintaining original name
        base_name = os.path.splitext(original_filename)[0]
        pages_str = '_'.join(map(str, page_numbers))
        output_filename = f"{base_name}_pages{pages_str}_extracted.pdf"
        
        # Full output path
        output_path = os.path.join(temp_dir, output_filename)
        
        # Read the input PDF
        with open(input_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            writer = PyPDF2.PdfWriter()

            # Add specified pages (adjusting for 0-indexing)
            for page_num in page_numbers:
                writer.add_page(reader.pages[page_num - 1])

            # Write the new PDF
            with open(output_path, 'wb') as output_file:
                writer.write(output_file)

        return output_path, output_filename
    except Exception as e:
        logger.error(f"Error extracting pages from {input_path}: {e}")
        logger.error(traceback.format_exc())
        return None, None

# Process a single PDF file
def process_single_pdf(input_path, output_folders):
    """
    Process a single PDF file with comprehensive error handling
    """
    # Tracking variables for this file
    file_stats = {
        'processed': False,
        'classified_pages': [],
        'unclassified_pages': [],
        'errors': []
    }
    
    try:
        # Generate safe filename
        filename = os.path.basename(input_path)
        sanitized_filename = sanitize_filename(filename)
        
        # Copy to source folder
        source_output_path = os.path.join(output_folders['source'], sanitized_filename)
        shutil.copy2(input_path, source_output_path)
        
        # Open the PDF file
        with open(input_path, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
        
        # Classify each page individually
        page_classifications = {}
        
        for page_num in range(1, total_pages + 1):
            try:
                # Reset file pointer
                with open(input_path, "rb") as file:
                    # Begin document classification for this page
                    poller = document_intelligence_client.begin_classify_document(
                        classifier_id=MODEL_ID,
                        body=file
                    )
                
                # Get classification result
                result = poller.result()
                
                # Check classification for this page
                classified = False
                for document in result.documents:
                    logger.info(f"Page {page_num}: Confidence = {document.confidence}")
                    
                    if document.confidence >= CONFIDENCE_THRESHOLD:
                        doc_type = document.doc_type.lower().replace(' ', '_')
                        page_classifications[page_num] = {
                            'type': doc_type,
                            'confidence': document.confidence
                        }
                        file_stats['classified_pages'].append(page_num)
                        classified = True
                        break
                
                # Track unclassified pages
                if not classified:
                    file_stats['unclassified_pages'].append(page_num)
            
            except Exception as classification_error:
                logger.error(f"Classification error for {input_path}, page {page_num}: {classification_error}")
                file_stats['unclassified_pages'].append(page_num)
        
        # Process classified and unclassified pages
        if file_stats['classified_pages']:
            # Group classified pages by type
            classified_groups = {}
            for page_num in file_stats['classified_pages']:
                doc_type = page_classifications[page_num]['type']
                if doc_type not in classified_groups:
                    classified_groups[doc_type] = []
                classified_groups[doc_type].append(page_num)
            
            # Extract and save classified page groups
            for doc_type, pages in classified_groups.items():
                # Create type-specific folder in classified output
                type_output_folder = os.path.join(output_folders['classified'], doc_type)
                os.makedirs(type_output_folder, exist_ok=True)
                
                # Extract classified pages
                extracted_pdf, output_filename = extract_pages(
                    input_path, 
                    pages, 
                    sanitized_filename
                )
                
                if extracted_pdf:
                    # Full output path for classified PDF
                    output_path = os.path.join(type_output_folder, output_filename)
                    
                    # Move extracted PDF
                    shutil.move(extracted_pdf, output_path)
                    
                    # Logging
                    logger.info(f"Classified Pages: {pages}")
                    logger.info(f"  - Document Type: {doc_type}")
                    logger.info(f"  - Output: {output_path}")
        
        # Handle unclassified pages
        if file_stats['unclassified_pages']:
            # Extract unclassified pages
            extracted_pdf, output_filename = extract_pages(
                input_path, 
                file_stats['unclassified_pages'], 
                sanitized_filename
            )
            
            if extracted_pdf:
                # Move to unclassified folder
                unclassified_output_path = os.path.join(output_folders['unclassified'], output_filename)
                shutil.move(extracted_pdf, unclassified_output_path)
                
                # Logging
                logger.info(f"Unclassified Pages: {file_stats['unclassified_pages']}")
                logger.info(f"  - Moved to: {unclassified_output_path}")
        
        # Mark as processed if any pages were handled
        file_stats['processed'] = bool(file_stats['classified_pages'] or file_stats['unclassified_pages'])
    
    except Exception as e:
        # Catch any unexpected errors
        logger.error(f"Unexpected error processing {input_path}: {e}")
        logger.error(traceback.format_exc())
        file_stats['errors'].append({
            'error': 'Unexpected processing error',
            'details': str(e),
            'traceback': traceback.format_exc()
        })
    
    return file_stats

# Main processing function remains the same as in the previous full script
def process_pdf_folder(input_folder, output_base_folder):
    # Setup output folders
    output_folders = {
        'source': os.path.join(output_base_folder, 'source'),
        'classified': os.path.join(output_base_folder, 'classified'),
        'unclassified': os.path.join(output_base_folder, 'unclassified')
    }
    
    # Create output folders
    for folder in output_folders.values():
        os.makedirs(folder, exist_ok=True)
    
    # Create temporary folder
    temp_folder = os.path.join(output_base_folder, 'temp')
    os.makedirs(temp_folder, exist_ok=True)
    
    # Logging variables
    stats = {
        'total_files_found': 0,
        'total_files_processed': 0,
        'classified_files': 0,
        'unclassified_files': 0,
        'total_pages_processed': 0,
        'classified_pages': 0,
        'document_types': {},
        'errors': []
    }
    
    # Log start of processing
    logger.info(f"Starting comprehensive document classification")
    logger.info(f"Input Folder: {input_folder}")
    logger.info(f"Output Folder: {output_base_folder}")
    logger.info(f"Confidence Threshold: {CONFIDENCE_THRESHOLD}")
    
    # Ensure input folder exists
    if not os.path.exists(input_folder):
        logger.error(f"Input folder {input_folder} does not exist!")
        return
    
    # Comprehensive file discovery
    all_files = []
    for root, dirs, files in os.walk(input_folder):
        for file in files:
            full_path = os.path.join(root, file)
            if is_valid_pdf(full_path):
                all_files.append(full_path)
    
    logger.info(f"Total PDF files found: {len(all_files)}")
    
    # Process all discovered files
    for input_path in all_files:
        # Increment total files found
        stats['total_files_found'] += 1
        
        # Process each PDF
        file_stats = process_single_pdf(input_path, output_folders)
        
        # Update overall stats
        if file_stats['processed']:
            stats['total_files_processed'] += 1
            
            # Track classified pages
            if file_stats['classified_pages']:
                stats['classified_files'] += 1
                stats['classified_pages'] += len(file_stats['classified_pages'])
            
            # Track unclassified pages
            if file_stats['unclassified_pages']:
                stats['unclassified_files'] += 1
        
        # Track errors
        if file_stats['errors']:
            stats['errors'].extend(file_stats['errors'])
    
    # Print and log processing summary
    summary_message = f"""
--- Processing Summary ---
Total Files Found: {stats['total_files_found']}
Total Files Processed: {stats['total_files_processed']}
Classified Files: {stats['classified_files']}
Unclassified Files: {stats['unclassified_files']}
Classified Pages: {stats['classified_pages']}
Errors: {len(stats['errors'])}
"""
    print(summary_message)
    logger.info(summary_message)
    
    # Log detailed errors if any
    if stats['errors']:
        error_log_path = os.path.join(output_base_folder, 'logs', 'processing_errors.log')
        with open(error_log_path, 'w') as error_file:
            for error in stats['errors']:
                error_file.write(f"Error: {error.get('error', 'Unknown')}\n")
                error_file.write(f"Details: {error.get('details', 'No details')}\n")
                if 'traceback' in error:
                    error_file.write(f"Traceback:\n{error['traceback']}\n")
                error_file.write("\n" + "="*50 + "\n\n")
        logger.info(f"Detailed error log saved to {error_log_path}")
    
    # Clean up temporary folder
    try:
        # Remove temporary folder contents
        if os.path.exists(temp_folder):
            for item in os.listdir(temp_folder):
                item_path = os.path.join(temp_folder, item)
                if os.path.isfile(item_path):
                    os.unlink(item_path)
                elif os.path.isdir(item_path):
                    shutil.rmtree(item_path)
            
            # Remove the temp folder itself
            os.rmdir(temp_folder)
        
        logger.info(f"Temporary folder {temp_folder} cleaned up")
    except Exception as cleanup_error:
        logger.error(f"Error cleaning up temporary folder: {cleanup_error}")

# Run the processing
process_pdf_folder(INPUT_FOLDER, OUTPUT_BASE_FOLDER)
