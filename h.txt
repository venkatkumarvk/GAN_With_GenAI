import pandas as pd
import re
from typing import Dict, List, Tuple


class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str):
        self.excel_file_path = excel_file_path
        self.datatype_mapping = {
            'TIMESTAMP(6)': 'TIMESTAMP',
            'DATE': 'DATE',
            'CHAR(1)': 'CHAR(1)',
            'CLOB': 'STRING',
            'BLOB': 'BINARY'
        }

    def load_excel_data(self) -> pd.DataFrame:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            available_sheets = excel_file.sheet_names
            print(f"Available sheets: {available_sheets}")

            target_sheet = None
            for sheet in available_sheets:
                if 'BROKER_GROUP_RELATION' in sheet.upper() or 'BROKER GROUP RELATION' in sheet.upper():
                    target_sheet = sheet
                    break

            if target_sheet:
                print(f"Loading sheet: {target_sheet}")
                df = pd.read_excel(self.excel_file_path, sheet_name=target_sheet)
            else:
                print("Sheet not found. Using first sheet instead.")
                df = pd.read_excel(self.excel_file_path, sheet_name=available_sheets[0])

            print(f"Loaded {len(df)} rows and {len(df.columns)} columns.")
            return df
        except Exception as e:
            print(f"Error loading Excel file: {e}")
            return None

    def map_datatype(self, original_datatype: str) -> str:
        if pd.isna(original_datatype):
            return 'STRING'

        datatype_str = str(original_datatype).upper().strip()

        # Exact known mappings
        if datatype_str in self.datatype_mapping:
            return self.datatype_mapping[datatype_str]

        # Handle VARCHAR2(n BYTE) ‚Üí VARCHAR(n)
        varchar_match = re.match(r'VARCHAR2\((\d+)\s*BYTE\)', datatype_str)
        if varchar_match:
            return f'VARCHAR({varchar_match.group(1)})'

        # Handle generic VARCHAR2(n)
        varchar_match_simple = re.match(r'VARCHAR2\((\d+)\)', datatype_str)
        if varchar_match_simple:
            return f'VARCHAR({varchar_match_simple.group(1)})'

        # Handle NUMBER(p,s) ‚Üí DECIMAL(p,s)
        number_decimal_match = re.match(r'NUMBER\((\d+),\s*(\d+)\)', datatype_str)
        if number_decimal_match:
            p, s = number_decimal_match.groups()
            return f'DECIMAL({p},{s})'

        # Handle NUMBER(p) ‚Üí INT or BIGINT
        number_int_match = re.match(r'NUMBER\((\d+)\)', datatype_str)
        if number_int_match:
            precision = int(number_int_match.group(1))
            return 'INT' if precision <= 10 else 'BIGINT'

        # TIMESTAMP
        if 'TIMESTAMP' in datatype_str:
            return 'TIMESTAMP'

        # DATE
        if 'DATE' in datatype_str:
            return 'DATE'

        # CHAR, CLOB, BLOB
        if 'CHAR' in datatype_str:
            return 'STRING'
        if 'CLOB' in datatype_str:
            return 'STRING'
        if 'BLOB' in datatype_str:
            return 'BINARY'

        return 'STRING'

    def extract_columns_from_categories(self, df: pd.DataFrame) -> Dict[str, List[Tuple[str, str]]]:
        category_columns = {
            "RDMOF": [],
            "EDL": [],
            "Original SSR": []
        }
        seen_columns = set()

        category_mappings = {
            'RDMOF': {},
            'EDL': {},
            'Original SSR': {}
        }

        for col in df.columns:
            col_upper = str(col).upper()
            if 'RDMOF' in col_upper:
                if 'SCHEMA' in col_upper:
                    category_mappings['RDMOF']['schema'] = col
                elif 'TABLE' in col_upper:
                    category_mappings['RDMOF']['table'] = col
                elif 'COLUMN' in col_upper:
                    category_mappings['RDMOF']['column'] = col
                elif 'DATATYPE' in col_upper or 'DATA_TYPE' in col_upper:
                    category_mappings['RDMOF']['datatype'] = col
            elif 'EDL' in col_upper:
                if 'SCHEMA' in col_upper:
                    category_mappings['EDL']['schema'] = col
                elif 'TABLE' in col_upper:
                    category_mappings['EDL']['table'] = col
                elif 'COLUMN' in col_upper:
                    category_mappings['EDL']['column'] = col
                elif 'DATATYPE' in col_upper or 'DATA_TYPE' in col_upper:
                    category_mappings['EDL']['datatype'] = col
            elif 'SSR' in col_upper or 'ORIGINAL' in col_upper:
                if 'SCHEMA' in col_upper:
                    category_mappings['Original SSR']['schema'] = col
                elif 'TABLE' in col_upper:
                    category_mappings['Original SSR']['table'] = col
                elif 'COLUMN' in col_upper:
                    category_mappings['Original SSR']['column'] = col
                elif 'DATATYPE' in col_upper or 'DATA_TYPE' in col_upper:
                    category_mappings['Original SSR']['datatype'] = col

        for category, mapping in category_mappings.items():
            if mapping.get('column'):
                col_col = mapping['column']
                dtype_col = mapping.get('datatype')
                for _, row in df.iterrows():
                    col_name = row.get(col_col)
                    if pd.notna(col_name):
                        col_name = str(col_name).strip()
                        if (category, col_name.lower()) in seen_columns:
                            continue
                        raw_dtype = str(row.get(dtype_col)).strip() if dtype_col and pd.notna(row.get(dtype_col)) else ''
                        mapped_dtype = self.map_datatype(raw_dtype)

                        if mapped_dtype == 'STRING' and raw_dtype:
                            print(f"‚ö†Ô∏è  Defaulted to STRING for column '{col_name}' with raw datatype '{raw_dtype}' in category '{category}'")

                        category_columns[category].append((col_name, mapped_dtype))
                        seen_columns.add((category, col_name.lower()))

        return {
            "RDMOF": category_columns["RDMOF"],
            "EDL": category_columns["EDL"],
            "Original_SSR": category_columns["Original SSR"]
        }

    def generate_table_schema(self, table_name: str, columns: List[Tuple[str, str]]) -> str:
        if not columns:
            return f"-- No columns found for {table_name}"
        sql = f"-- {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.EDM_Reporting.{table_name} (\n"
        sql += ",\n".join(f"    {name} {dtype}" for name, dtype in columns)
        sql += "\n);"
        return sql

    def run(self, debug: bool = True) -> None:
        print("Starting schema generation...")

        df = self.load_excel_data()
        if df is None:
            print("Error: Could not load Excel file.")
            return

        category_columns_dict = self.extract_columns_from_categories(df)

        for category, columns in category_columns_dict.items():
            table_name = f"{category.upper()}_TABLE"
            schema_sql = self.generate_table_schema(table_name, columns)

            print(f"\nGenerated schema for {category}:\n")
            print(schema_sql)

            # Save to individual .sql files
            file_name = f"{table_name.lower()}.sql"
            with open(file_name, "w") as f:
                f.write(schema_sql)
            print(f"Schema saved to '{file_name}'")


# Example Usage
def main():
    excel_file_path = "your_excel_file.xlsx"  # üîÅ Replace with your actual Excel file path
    generator = DatabricksSchemaGenerator(excel_file_path)
    generator.run(debug=True)


if __name__ == "__main__":
    main()
