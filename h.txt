import os
import glob
import pandas as pd
import numpy as np

class InsertSQLGenerator:
    def __init__(self, config: dict, input_folder: str, output_folder: str = "generated_inserts"):
        self.input_folder = input_folder
        self.output_folder = output_folder
        self.categories = config.get("categories", {})
        self.category_keys = list(self.categories.keys())

    def get_excel_files(self):
        patterns = ["*.xls", "*.xlsx", "*.xlsm"]
        files = []
        for p in patterns:
            files.extend(glob.glob(os.path.join(self.input_folder, p)))
        return files

    def load_all_sheets(self, file_path):
        try:
            xls = pd.ExcelFile(file_path)
            sheets = []
            for sheet_name in xls.sheet_names:
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    df.columns = df.columns.str.strip()
                    
                    # Only forward-fill comment columns, not main mapping columns
                    for cfg in self.categories.values():
                        for col_key in ["table_comment_col", "column_comment_col"]:
                            col = cfg.get(col_key)
                            if col in df.columns:
                                df[col] = df[col].ffill()
                    
                    sheets.append(df)
                except Exception as e:
                    print(f"‚ö†Ô∏è Error reading sheet {sheet_name}: {e}")
            return pd.concat(sheets, ignore_index=True) if sheets else pd.DataFrame()
        except Exception as e:
            print(f"‚ùå Error loading {file_path}: {e}")
            return pd.DataFrame()

    def is_na_value(self, value):
        """Enhanced NA detection function"""
        if pd.isna(value) or value is None:
            return True
        if isinstance(value, str):
            clean_value = str(value).strip().lower()
            if clean_value in ["", "nan", "none", "null", "na", "#n/a", "#na"]:
                return True
        return False

    def create_row_mappings(self, df):
        row_mappings = []
        valid_rows = df.dropna(how="all")
        
        for idx, row in valid_rows.iterrows():
            mapping = {}
            print(f"\nüîç Processing Row {idx + 1}:")
            
            for cat in self.category_keys:
                cfg = self.categories[cat]
                
                # Get raw values
                schema_raw = row.get(cfg["schema_col"], np.nan)
                table_raw = row.get(cfg["table_col"], np.nan)
                column_raw = row.get(cfg["column_col"], np.nan)
                table_comment_raw = row.get(cfg.get("table_comment_col", ""), "")
                column_comment_raw = row.get(cfg.get("column_comment_col", ""), "")

                # NA detection
                schema_is_na = self.is_na_value(schema_raw)
                table_is_na = self.is_na_value(table_raw)
                column_is_na = self.is_na_value(column_raw)
                has_na = schema_is_na or table_is_na or column_is_na

                # Keep original NA values, don't fill forward
                schema = "NA" if schema_is_na else str(schema_raw).strip()
                table = "NA" if table_is_na else str(table_raw).strip()
                column = "NA" if column_is_na else str(column_raw).strip()
                table_comment = "" if self.is_na_value(table_comment_raw) else str(table_comment_raw).strip()
                column_comment = "" if self.is_na_value(column_comment_raw) else str(column_comment_raw).strip()

                mapping[cat] = {
                    "table": (schema, table),
                    "column": column,
                    "table_comment": table_comment,
                    "column_comment": column_comment,
                    "has_na": has_na
                }
                
                print(f"   {cat}: Schema={schema}, Table={table}, Column={column}, Has_NA={has_na}")
            
            row_mappings.append(mapping)
        return row_mappings

    def generate_sql_for_row(self, mapping):
        """Generate SQL statements per row with only one case applied per row."""
        edl = mapping.get("EDL", {})
        rdmof = mapping.get("RDMOF", {})
        original = mapping.get("Original_SSR", {})

        edl_na = edl.get("has_na", True)
        rdmof_na = rdmof.get("has_na", True)
        original_na = original.get("has_na", True)

        sql_statements = []

        print(f"   üéØ Case Analysis: Original_NA={original_na}, EDL_NA={edl_na}, RDMOF_NA={rdmof_na}")

        # CASE 1: All present ‚Üí Original‚ÜíEDL + EDL‚ÜíRDMOF
        if not original_na and not edl_na and not rdmof_na:
            print("   ‚úÖ CASE 1: All present ‚Üí Original‚ÜíEDL + EDL‚ÜíRDMOF")
            sql_statements.append(self.build_sql(original, edl, description="Original_SSR ‚Üí EDL"))
            sql_statements.append(self.build_sql(edl, rdmof, description="EDL ‚Üí RDMOF"))

        # CASE 2: Only Original‚ÜíRDMOF (EDL NA)
        elif not original_na and edl_na and not rdmof_na:
            print("   ‚úÖ CASE 2: Only Original‚ÜíRDMOF (EDL NA)")
            sql_statements.append(self.build_sql(original, rdmof, description="Original_SSR ‚Üí RDMOF (EDL is NA)"))

        # CASE 3: Only Original‚ÜíEDL (RDMOF NA)
        elif not original_na and not edl_na and rdmof_na:
            print("   ‚úÖ CASE 3: Only Original‚ÜíEDL (RDMOF NA)")
            sql_statements.append(self.build_sql(original, edl, description="Original_SSR ‚Üí EDL (RDMOF is NA)"))

        # CASE 4: Only EDL‚ÜíRDMOF (Original NA)
        elif original_na and not edl_na and not rdmof_na:
            print("   ‚úÖ CASE 4: Only EDL‚ÜíRDMOF (Original NA)")
            sql_statements.append(self.build_sql(edl, rdmof, description="EDL ‚Üí RDMOF (Original_SSR is NA)"))

        # CASE 5: Anything else ‚Üí commented placeholder
        else:
            print("   ‚ö†Ô∏è CASE 5: Multiple categories have NA values")
            sql_statements.append(self.build_commented_placeholder(mapping, "Multiple categories have NA values"))

        return sql_statements

    def build_sql(self, src, tgt, description="", comment_all=False):
        src_schema, src_table = src.get("table", ("NA", "NA"))
        tgt_schema, tgt_table = tgt.get("table", ("NA", "NA"))
        src_col = src.get("column", "NA")
        tgt_col = tgt.get("column", "NA")

        comments = [f"-- {description}"] if description else []

        for c in [src.get("table_comment", ""), src.get("column_comment", ""),
                  tgt.get("table_comment", ""), tgt.get("column_comment", "")]:
            if c:
                comments.extend([f"-- {line.strip()}" for line in str(c).splitlines() if line.strip()])

        comment_text = "\n".join(comments)
        if comment_text:
            comment_text += "\n"

        if comment_all or src.get("has_na") or tgt.get("has_na"):
            return (
                f"{comment_text}"
                f"-- Mapping skipped due to NA values\n"
                f"-- {src_schema}.{src_table}.{src_col} ‚Üí {tgt_schema}.{tgt_table}.{tgt_col}\n"
            )
        else:
            return (
                f"{comment_text}"
                f"INSERT INTO {tgt_schema}.{tgt_table} ({tgt_col})\n"
                f"SELECT DISTINCT {src_col} FROM {src_schema}.{src_table};\n"
            )

    def build_commented_placeholder(self, mapping, reason):
        return (
            f"-- {reason}\n"
            f"-- Row skipped: No valid transformation possible\n"
            f"-- Original_SSR: {'NA' if mapping.get('Original_SSR', {}).get('has_na', True) else 'Valid'}\n"
            f"-- EDL: {'NA' if mapping.get('EDL', {}).get('has_na', True) else 'Valid'}\n"
            f"-- RDMOF: {'NA' if mapping.get('RDMOF', {}).get('has_na', True) else 'Valid'}\n"
        )

    def process_file(self, file_path):
        df = self.load_all_sheets(file_path)
        if df.empty:
            return [], set()

        row_maps = self.create_row_mappings(df)
        all_sql = []
        unique_set = set()

        for i, row_map in enumerate(row_maps, 1):
            print(f"\nüìã Generating SQL for Row {i}:")
            sql_list = self.generate_sql_for_row(row_map)
            for sql in sql_list:
                sql_normalized = " ".join(sql.split())
                if sql_normalized not in unique_set:
                    all_sql.append(sql)
                    unique_set.add(sql_normalized)
                    print(f"   ‚úÖ Added SQL statement")
                else:
                    print(f"   ‚ö†Ô∏è Duplicate SQL statement skipped")

        return all_sql, unique_set

    def run(self):
        os.makedirs(self.output_folder, exist_ok=True)
        files = self.get_excel_files()
        if not files:
            print(f"‚ùå No Excel files found in {self.input_folder}")
            return

        total_unique = 0
        print(f"üîÑ Processing {len(files)} Excel file(s)...")

        for f in files:
            print(f"\nüìÑ Processing: {os.path.basename(f)}")
            sql_list, unique_sql = self.process_file(f)

            if sql_list:
                out_file = os.path.splitext(os.path.basename(f))[0] + ".sql"
                out_path = os.path.join(self.output_folder, out_file)
                with open(out_path, "w", encoding="utf-8") as fw:
                    fw.write(f"-- Generated SQL from: {os.path.basename(f)}\n")
                    fw.write(f"-- Total unique statements: {len(sql_list)}\n\n")
                    fw.write("\n".join(sql_list))
                total_unique += len(unique_sql)
                print(f"‚úÖ {out_file}: {len(unique_sql)} unique statements")
            else:
                print(f"‚ö†Ô∏è No valid SQL generated for {os.path.basename(f)}")

        print(f"\nüìä Summary: Total unique statements: {total_unique}")
