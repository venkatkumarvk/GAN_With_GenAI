import pandas as pd
import re
from typing import Dict, List, Tuple


class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str):
        self.excel_file_path = excel_file_path

    def load_excel_data(self) -> pd.DataFrame:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            available_sheets = excel_file.sheet_names
            print(f"Available sheets: {available_sheets}")

            target_sheet = None
            for sheet in available_sheets:
                if 'BROKER_GROUP_RELATION' in sheet.upper() or 'BROKER GROUP RELATION' in sheet.upper():
                    target_sheet = sheet
                    break

            if target_sheet:
                print(f"Loading sheet: {target_sheet}")
                df = pd.read_excel(self.excel_file_path, sheet_name=target_sheet)
            else:
                print("Sheet not found. Using first sheet instead.")
                df = pd.read_excel(self.excel_file_path, sheet_name=available_sheets[0])

            print(f"Loaded {len(df)} rows and {len(df.columns)} columns.")
            return df
        except Exception as e:
            print(f"Error loading Excel file: {e}")
            return None

    def map_datatype(self, original_datatype: str) -> str:
        if pd.isna(original_datatype):
            return 'STRING'

        datatype_str = str(original_datatype).upper().strip()

        # VARCHAR2(N BYTE) -> VARCHAR(N)
        match = re.match(r'VARCHAR2\((\d+)\s*BYTE\)', datatype_str)
        if match:
            return f'VARCHAR({match.group(1)})'

        match = re.match(r'VARCHAR2\((\d+)\)', datatype_str)
        if match:
            return f'VARCHAR({match.group(1)})'

        # NUMBER(P,S) -> DECIMAL(P,S)
        match = re.match(r'NUMBER\((\d+),\s*(\d+)\)', datatype_str)
        if match:
            return f'DECIMAL({match.group(1)},{match.group(2)})'

        # NUMBER(P) -> INT or BIGINT
        match = re.match(r'NUMBER\((\d+)\)', datatype_str)
        if match:
            precision = int(match.group(1))
            return 'INT' if precision <= 10 else 'BIGINT'

        if 'TIMESTAMP' in datatype_str:
            return 'TIMESTAMP'
        if 'DATE' in datatype_str:
            return 'DATE'
        if 'CHAR' in datatype_str:
            return 'STRING'
        if 'CLOB' in datatype_str:
            return 'STRING'
        if 'BLOB' in datatype_str:
            return 'BINARY'

        return 'STRING'

    def extract_columns_from_categories(self, df: pd.DataFrame) -> Dict[str, List[Tuple[str, str]]]:
        category_columns = {
            "RDMOF": [],
            "EDL": [],
            "Original SSR": []
        }
        seen_columns = set()

        # Map Excel columns to category fields
        category_mappings = {
            'RDMOF': {},
            'EDL': {},
            'Original SSR': {}
        }

        for col in df.columns:
            col_upper = col.upper()
            for category in category_mappings.keys():
                if category in col_upper:
                    if 'SCHEMA' in col_upper:
                        category_mappings[category]['schema'] = col
                    elif 'TABLE' in col_upper:
                        category_mappings[category]['table'] = col
                    elif 'COLUMN' in col_upper:
                        category_mappings[category]['column'] = col
                    elif 'DATATYPE' in col_upper or 'DATA_TYPE' in col_upper:
                        category_mappings[category]['datatype'] = col

        # Loop through each category and extract columns
        for category, mapping in category_mappings.items():
            if 'column' not in mapping:
                continue

            col_col = mapping['column']
            dtype_col = mapping.get('datatype')

            for _, row in df.iterrows():
                col_name = row.get(col_col)
                if pd.isna(col_name) or str(col_name).strip() == '':
                    continue
                col_name = str(col_name).strip()

                if (category, col_name.lower()) in seen_columns:
                    continue

                # Use RDMOF's datatype column if exists
                if dtype_col and pd.notna(row.get(dtype_col)):
                    raw_dtype = str(row[dtype_col]).strip()
                elif category != 'RDMOF' and category_mappings['RDMOF'].get('datatype'):
                    # Fallback: use RDMOF datatype column if available
                    raw_dtype = str(row.get(category_mappings['RDMOF']['datatype'], '')).strip()
                else:
                    raw_dtype = ''

                mapped_dtype = self.map_datatype(raw_dtype)

                if mapped_dtype == 'STRING' and raw_dtype:
                    print(f"⚠️ Defaulted to STRING for column '{col_name}' with raw datatype '{raw_dtype}' in {category}")

                category_columns[category].append((col_name, mapped_dtype))
                seen_columns.add((category, col_name.lower()))

        return {
            "RDMOF": category_columns["RDMOF"],
            "EDL": category_columns["EDL"],
            "Original_SSR": category_columns["Original SSR"]
        }

    def generate_table_schema(self, table_name: str, columns: List[Tuple[str, str]]) -> str:
        if not columns:
            return f"-- No columns found for {table_name}"
        sql = f"-- {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.EDM_Reporting.{table_name} (\n"
        sql += ",\n".join(f"    {name} {dtype}" for name, dtype in columns)
        sql += "\n);"
        return sql

    def run(self, debug: bool = True) -> None:
        print("Starting schema generation...")

        df = self.load_excel_data()
        if df is None:
            print("Error: Could not load Excel file.")
            return

        category_columns_dict = self.extract_columns_from_categories(df)

        for category, columns in category_columns_dict.items():
            table_name = f"{category.upper()}_TABLE"
            schema_sql = self.generate_table_schema(table_name, columns)

            print(f"\nGenerated schema for {category}:\n")
            print(schema_sql)

            # Save each schema
            file_name = f"{table_name.lower()}.sql"
            with open(file_name, "w") as f:
                f.write(schema_sql)
            print(f"✅ Schema saved to: {file_name}")


# Example Usage
def main():
    excel_file_path = "your_excel_file.xlsx"  # Replace this with your actual file
    generator = DatabricksSchemaGenerator(excel_file_path)
    generator.run(debug=True)


if __name__ == "__main__":
    main()
