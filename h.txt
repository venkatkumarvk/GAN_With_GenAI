#1. First, add token tracking to the AzureOpenAIClient class in llm.py:
class AzureOpenAIClient:
    def __init__(self, config):
        self.api_key = config["azure_openai"]["api_key"]
        self.api_version = config["azure_openai"]["api_version"]
        self.endpoint = config["azure_openai"]["azure_endpoint"]
        self.deployment_name = config["azure_openai"]["deployment_name"]
        self.batch_size = config["processing"]["batch_size"]
        self.timeout = config["processing"]["timeout_seconds"]
        
        # Add token cost parameters
        self.input_token_cost_per_million = 2.50
        self.output_token_cost_per_million = 10.0
        
        # Add token counters
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        
        self.client = AzureOpenAI(
            api_key=self.api_key,
            api_version=self.api_version,
            azure_endpoint=self.endpoint
        )
    
    def calculate_cost(self):
        """Calculate the cost based on token usage."""
        input_cost = (self.total_input_tokens / 1000000) * self.input_token_cost_per_million
        output_cost = (self.total_output_tokens / 1000000) * self.output_token_cost_per_million
        total_cost = input_cost + output_cost
        
        return {
            "input_tokens": self.total_input_tokens,
            "output_tokens": self.total_output_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost
        }
    
    def track_token_usage(self, response_data):
        """
        Track token usage from API response data.
        This works with both batch and general responses.
        """
        try:
            if isinstance(response_data, dict):
                if "usage" in response_data:
                    # Direct OpenAI API response
                    usage = response_data["usage"]
                    self.total_input_tokens += usage.get("prompt_tokens", 0)
                    self.total_output_tokens += usage.get("completion_tokens", 0)
                elif "response" in response_data and "body" in response_data["response"]:
                    # Batch API response format
                    body = response_data["response"]["body"]
                    if isinstance(body, str):
                        body = json.loads(body)
                    
                    if "usage" in body:
                        usage = body["usage"]
                        self.total_input_tokens += usage.get("prompt_tokens", 0)
                        self.total_output_tokens += usage.get("completion_tokens", 0)
        except Exception as e:
            print(f"Error tracking tokens: {str(e)}")

#2. Update the process_batch method in AzureOpenAIClient to track token usage:
def process_batch(self, image_base64_strings, prompts):
    """
    Process images in a batch using the Azure OpenAI batch API.
    Tracks token usage.
    """
    # Existing implementation...
    
    # When retrieving results:
    print(f"Batch completed. Retrieving results...")
    file_response = self.client.files.content(output_file_id)
    raw_responses = file_response.text.strip().split('\n')
    
    # Track token usage from each response
    for raw_response in raw_responses:
        try:
            response_data = json.loads(raw_response)
            self.track_token_usage(response_data)
        except Exception as e:
            print(f"Error tracking tokens: {str(e)}")
    
    return raw_responses

#3. Update the process_general method in AzureOpenAIClient to track token usage:
def process_general(self, image_base64_strings, prompts):
    """
    Process images using the general (non-batch) API.
    Tracks token usage.
    """
    results = []
    
    for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
        try:
            print(f"Processing image {i+1}/{len(image_base64_strings)}")
            
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_img}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            # Track token usage from the response
            if hasattr(response, 'usage'):
                self.total_input_tokens += response.usage.prompt_tokens
                self.total_output_tokens += response.usage.completion_tokens
            
            if hasattr(response, 'choices') and len(response.choices) > 0:
                content = response.choices[0].message.content
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "response": {
                        "body": {
                            "choices": [
                                {
                                    "message": {
                                        "content": content
                                    }
                                }
                            ],
                            "usage": {
                                "prompt_tokens": response.usage.prompt_tokens if hasattr(response, 'usage') else 0,
                                "completion_tokens": response.usage.completion_tokens if hasattr(response, 'usage') else 0
                            }
                        }
                    }
                }))
            else:
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "error": "No response content"
                }))
        
        except Exception as e:
            print(f"Error processing image {i+1}: {str(e)}")
            results.append(json.dumps({
                "custom_id": f"request-{i+1}",
                "error": str(e)
            }))
    
    return results

#pdf process
def process_azure_pdf_files(config, api_type, azure_folder):
    """Process PDF files from Azure Blob Storage."""
    # Existing implementation...
    
    # After all processing is complete, display token usage and cost
    cost_info = ai_client.calculate_cost()
    
    print("\n===== GPT-4o Token Usage and Cost =====")
    print(f"Total Input Tokens: {cost_info['input_tokens']:,}")
    print(f"Total Output Tokens: {cost_info['output_tokens']:,}")
    print(f"Input Cost: ${cost_info['input_cost']:.4f}")
    print(f"Output Cost: ${cost_info['output_cost']:.4f}")
    print(f"Total Cost: ${cost_info['total_cost']:.4f}")
    print("=======================================")
    
    print("Processing complete!")
#Do the same for process_local_pdf_files.
5. Add per-document token tracking:
If you want to track costs per document as well, you can add this functionality by modifying the processing loop:
def process_local_pdf_files(config, api_type, local_folder):
    """Process PDF files from a local folder."""
    # Existing initialization...
    
    # Add document-level tracking
    document_costs = []
    
    # Process each PDF
    for i, pdf_file in enumerate(pdf_files):
        try:
            print(f"Processing file {i+1}/{len(pdf_files)}: {pdf_file.name}")
            
            # Track tokens before processing this document
            previous_input_tokens = ai_client.total_input_tokens
            previous_output_tokens = ai_client.total_output_tokens
            
            # Read file content and process...
            # [Existing processing code]
            
            # Calculate tokens used for this document
            doc_input_tokens = ai_client.total_input_tokens - previous_input_tokens
            doc_output_tokens = ai_client.total_output_tokens - previous_output_tokens
            
            # Calculate cost for this document
            doc_input_cost = (doc_input_tokens / 1000000) * ai_client.input_token_cost_per_million
            doc_output_cost = (doc_output_tokens / 1000000) * ai_client.output_token_cost_per_million
            doc_total_cost = doc_input_cost + doc_output_cost
            
            document_costs.append({
                "filename": pdf_file.name,
                "input_tokens": doc_input_tokens,
                "output_tokens": doc_output_tokens,
                "total_cost": doc_total_cost
            })
            
            print(f"Document Cost: ${doc_total_cost:.4f} (Input: {doc_input_tokens} tokens, Output: {doc_output_tokens} tokens)")
            
        except Exception as e:
            print(f"Error processing {pdf_file.name}: {str(e)}")
    
    # Display per-document costs
    print("\n===== Per-Document Costs =====")
    for doc_cost in document_costs:
        print(f"{doc_cost['filename']}: ${doc_cost['total_cost']:.4f} (Input: {doc_cost['input_tokens']} tokens, Output: {doc_cost['output_tokens']} tokens)")
    
    # Display total costs
    cost_info = ai_client.calculate_cost()
    
    print("\n===== GPT-4o Token Usage and Cost Summary =====")
    print(f"Total Documents Processed: {len(document_costs)}")
    print(f"Total Input Tokens: {cost_info['input_tokens']:,}")
    print(f"Total Output Tokens: {cost_info['output_tokens']:,}")
    print(f"Input Cost: ${cost_info['input_cost']:.4f}")
    print(f"Output Cost: ${cost_info['output_cost']:.4f}")
    print(f"Total Cost: ${cost_info['total_cost']:.4f}")
    print("=======================================")
    
    print("Processing complete!")
