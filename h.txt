import os
import json
import hashlib
import logging
import traceback
from datetime import datetime
from typing import Dict, Any, List

from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.core.polling import LROPoller

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

class DocumentIntelligenceModelTrainer:
    def __init__(self, config_path: str = 'config.json'):
        """
        Initialize Document Intelligence Model Trainer
        """
        # Load configuration
        with open(config_path, 'r') as f:
            self.cfg = json.load(f)
        
        # Setup logging
        self.logger = logging.getLogger(__name__)
        
        # Initialize Azure client
        self.client = self._initialize_client()
        
        # Ensure model cache directory
        os.makedirs(self.cfg['paths']['model_cache_dir'], exist_ok=True)

    def _initialize_client(self) -> DocumentIntelligenceClient:
        """
        Initialize Azure Document Intelligence Client
        """
        try:
            credential = AzureKeyCredential(self.cfg["azure_document_intelligence"]["key"])
            return DocumentIntelligenceClient(
                endpoint=self.cfg["azure_document_intelligence"]["endpoint"],
                credential=credential
            )
        except Exception as e:
            self.logger.critical(f"Client initialization error: {e}")
            raise

    def _compute_reference_hash(self, ref_dir: str) -> str:
        """
        Compute hash of reference documents
        """
        hasher = hashlib.sha256()
        
        for root, _, files in sorted(os.walk(ref_dir)):
            for file in sorted(files):
                path = os.path.join(root, file)
                with open(path, 'rb') as f:
                    hasher.update(f.read())
        
        return hasher.hexdigest()

    def _prepare_training_data(self) -> Dict[str, List[str]]:
        """
        Collect and validate training documents
        """
        ref_dir = self.cfg['paths']['reference_dir']
        training_data = {}
        
        for main_category, subcategories in self.cfg['categories'].items():
            training_data[main_category] = {}
            
            for subcategory in subcategories:
                subcat_path = os.path.join(ref_dir, main_category, subcategory)
                
                # Collect documents
                documents = [
                    os.path.join(subcat_path, doc) 
                    for doc in os.listdir(subcat_path) 
                    if os.path.isfile(os.path.join(subcat_path, doc))
                ]
                
                # Validate document count
                if len(documents) < self.cfg['model_training']['min_documents_per_category']:
                    self.logger.warning(f"Insufficient documents in {main_category}/{subcategory}")
                    continue
                
                # Limit documents
                documents = documents[:self.cfg['model_training']['max_training_documents']]
                training_data[main_category][subcategory] = documents
        
        return training_data

    def train_custom_model(self) -> str:
        """
        Train custom Document Intelligence model
        """
        try:
            # Prepare training data
            training_data = self._prepare_training_data()
            
            # Validate training data
            if not training_data:
                self.logger.error("No valid training data found")
                return None
            
            # Generate model ID
            model_id = f"custom_doc_classifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # Prepare training files
            training_files = []
            for main_cat, subcategories in training_data.items():
                for subcat, docs in subcategories.items():
                    training_files.extend(docs)
            
            # Open training files
            training_file_handles = [open(file, 'rb') for file in training_files]
            
            try:
                # Start model training
                poller: LROPoller = self.client.begin_build_model(
                    model_id=model_id,
                    training_files=training_file_handles
                )
                
                # Wait for training to complete
                result = poller.result()
                
                # Prepare model metadata
                model_metadata = {
                    'model_id': result.model_id,
                    'created_at': datetime.now().isoformat(),
                    'categories': list(training_data.keys())
                }
                
                # Save model metadata
                metadata_path = os.path.join(
                    self.cfg['paths']['model_cache_dir'], 
                    f"{model_id}_metadata.json"
                )
                
                with open(metadata_path, 'w') as f:
                    json.dump(model_metadata, f, indent=2)
                
                self.logger.info(f"Custom model trained: {result.model_id}")
                return result.model_id
            
            finally:
                # Close all file handles
                for file in training_file_handles:
                    file.close()
        
        except Exception as e:
            self.logger.error(f"Model training failed: {e}")
            return None

    def get_latest_model_id(self) -> str:
        """
        Retrieve latest trained model ID with robust error handling
        """
        model_cache_dir = self.cfg['paths']['model_cache_dir']
        
        # Ensure model cache directory exists
        if not os.path.exists(model_cache_dir):
            os.makedirs(model_cache_dir)
        
        # Get all metadata files
        model_files = [
            f for f in os.listdir(model_cache_dir) 
            if f.endswith('_metadata.json')
        ]
        
        # If no models, train a new one
        if not model_files:
            self.logger.warning("No trained models found. Training a new model.")
            new_model_id = self.train_custom_model()
            
            if not new_model_id:
                # Fallback to pre-built document model
                self.logger.warning("Using pre-built document model as fallback")
                return "prebuilt-document"
            
            return new_model_id
        
        # Find the most recent model
        latest_model_file = max(
            model_files, 
            key=lambda f: os.path.getctime(os.path.join(model_cache_dir, f))
        )
        
        # Read model metadata
        with open(os.path.join(model_cache_dir, latest_model_file), 'r') as f:
            model_metadata = json.load(f)
        
        return model_metadata['model_id']

    def check_and_train(self) -> bool:
        """
        Check if model needs retraining
        """
        ref_dir = self.cfg['paths']['reference_dir']
        hash_file = os.path.join(ref_dir, ".reference_hash")
        
        try:
            # Compute current reference hash
            current_hash = self._compute_reference_hash(ref_dir)
            
            # Check existing hash
            if os.path.exists(hash_file):
                with open(hash_file, 'r') as f:
                    last_hash = f.read().strip()
            else:
                last_hash = ''
            
            # Compare hashes
            if current_hash != last_hash:
                # Train new model
                new_model_id = self.train_custom_model()
                
                if new_model_id:
                    # Update hash file
                    with open(hash_file, 'w') as f:
                        f.write(current_hash)
                
                return True
            
            return False
        
        except Exception as e:
            self.logger.error(f"Reference check error: {e}")
            return False
