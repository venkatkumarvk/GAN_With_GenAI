# helper.py
import pandas as pd
import re
import os
import json
from typing import List, Tuple, Dict, Set, Optional, Union
from collections import defaultdict

class DatabricksSchemaGenerator:
    def __init__(self, 
                 excel_file_path: str, 
                 output_base_folder: str = "generated_schemas",
                 sheet_name: Optional[str] = None,
                 categories: Optional[Dict[str, Dict[str, str]]] = None):
        self.excel_file_path = excel_file_path
        self.output_base_folder = output_base_folder
        self.sheet_name = sheet_name
        self.custom_category_config = categories or {}
        self.predefined_categories = set(self.custom_category_config.keys())
        self.tables_created = 0
        self.categories = set()

    def load_excel_data(self) -> Optional[pd.DataFrame]:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            sheets = excel_file.sheet_names
            if self.sheet_name and self.sheet_name in sheets:
                sheet = self.sheet_name
            else:
                sheet = sheets[0]
            df = pd.read_excel(excel_file, sheet_name=sheet)
            return df
        except Exception as e:
            print(f"Error loading Excel file: {e}")
            return None

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype):
            return 'VARCHAR(255)'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'VARCHAR(1)'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'VARCHAR(255)'

    def extract_tables_by_physical_name(self, df: pd.DataFrame) -> Dict[str, Dict[Tuple[str, str], List[Tuple[str, str]]]]:
        result = {}
        seen_columns = {}

        for category, config in self.custom_category_config.items():
            result[category] = defaultdict(list)
            seen_columns[category] = defaultdict(set)

        for _, row in df.iterrows():
            for category, config in self.custom_category_config.items():
                schema_col = config.get('schema_col')
                table_col = config.get('table_col')
                column_col = config.get('column_col')
                datatype_col = config.get('datatype_col')

                schema_name = row.get(schema_col, 'EDM_Reporting')
                table_name = row.get(table_col)
                column_name = row.get(column_col)
                datatype_value = row.get(datatype_col)

                if pd.notna(schema_name) and pd.notna(table_name) and pd.notna(column_name):
                    schema_name = str(schema_name).strip()
                    table_name = str(table_name).strip()
                    column_name = str(column_name).strip()

                    if column_name.lower() not in seen_columns[category][(schema_name, table_name)]:
                        dtype = self.map_datatype(datatype_value) if pd.notna(datatype_value) else 'VARCHAR(255)'
                        result[category][(schema_name, table_name)].append((column_name, dtype))
                        seen_columns[category][(schema_name, table_name)].add(column_name.lower())

        self.categories = set(result.keys())
        return result

    def create_folder_structure(self):
        folders = list(self.categories) + ["consolidated"]
        if not os.path.exists(self.output_base_folder):
            os.makedirs(self.output_base_folder)
        for folder in folders:
            folder_path = os.path.join(self.output_base_folder, folder)
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)

    def generate_schema_sql(self, schema_name: str, table_name: str, columns: List[Tuple[str, str]], category: str) -> str:
        if not columns:
            return f"-- No columns found for {table_name}"
        sql = f"-- {category} - {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.{schema_name}.{table_name} (\n"
        sql += ",\n".join([f"    [{col}] {dtype}" for col, dtype in columns])
        sql += "\n);"
        return sql

    def generate_category_consolidated_schema(self, category: str, tables: Dict[Tuple[str, str], List[Tuple[str, str]]]) -> str:
        consolidated_sql = f"-- {category} CATEGORY - CONSOLIDATED SCHEMA\n"
        for (schema_name, table_name), columns in tables.items():
            sql = self.generate_schema_sql(schema_name, table_name, columns, category)
            consolidated_sql += sql + "\n\n"
        consolidated_sql += f"-- Total tables in {category} consolidated schema: {len(tables)}\n"
        return consolidated_sql

    def generate_master_consolidated_schema(self, tables_by_category: Dict[str, Dict[Tuple[str, str], List[Tuple[str, str]]]]) -> str:
        consolidated_sql = "-- MASTER CONSOLIDATED SCHEMA\n\n"
        total_tables = 0
        for category, tables in tables_by_category.items():
            consolidated_sql += f"-- {category} CATEGORY\n\n"
            for (schema_name, table_name), columns in tables.items():
                sql = self.generate_schema_sql(schema_name, table_name, columns, category)
                consolidated_sql += sql + "\n\n"
                total_tables += 1
        consolidated_sql += f"-- Total tables in master consolidated schema: {total_tables}\n"
        return consolidated_sql

    def run(self):
        df = self.load_excel_data()
        if df is None:
            return

        tables_by_category = self.extract_tables_by_physical_name(df)
        self.create_folder_structure()

        total_tables = 0
        for category, tables in tables_by_category.items():
            folder = os.path.join(self.output_base_folder, category)
            for (schema_name, table_name), columns in tables.items():
                sql = self.generate_schema_sql(schema_name, table_name, columns, category)
                with open(os.path.join(folder, f"{table_name.lower()}.sql"), "w") as f:
                    f.write(sql)
                total_tables += 1

            table_count = len(tables)
            consolidated_filename = f"{category.lower()}_consolidated_{table_count}.sql"
            consolidated_path = os.path.join(folder, consolidated_filename)
            consolidated_sql = self.generate_category_consolidated_schema(category, tables)
            with open(consolidated_path, "w") as f:
                f.write(consolidated_sql)

        master_sql = self.generate_master_consolidated_schema(tables_by_category)
        master_path = os.path.join(self.output_base_folder, "consolidated", "all_tables_master_consolidated.sql")
        with open(master_path, "w") as f:
            f.write(master_sql)
        self.tables_created = total_tables


def load_config_from_file(config_file: str) -> Dict:
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        return config
    except Exception as e:
        print(f"Error loading config file: {e}")
        return {}```
