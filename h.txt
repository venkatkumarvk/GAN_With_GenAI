{
    "azure_document_intelligence": {
        "endpoint": "YOUR_DOCUMENT_INTELLIGENCE_ENDPOINT",
        "key": "YOUR_DOCUMENT_INTELLIGENCE_KEY",
        "model_version": "2024-02-15-preview"
    },
    "categories": {
        "cms1500": ["cadwell", "rhymlink"],
        "invoice": ["tesla", "amazon"],
        "scheduling": ["email", "iomrequest"]
    },
    "paths": {
        "reference_dir": "reference",
        "input_dir": "input_docs",
        "output_dir": "output",
        "source_dir": "output/source",
        "classified_dir": "output/classified",
        "unclassified_dir": "output/unclassified"
    },
    "model_training": {
        "min_documents_per_category": 5,
        "max_training_documents": 100,
        "confidence_threshold": 0.7
    },
    "allowed_file_types": [".pdf", ".tiff", ".jpg", ".jpeg", ".png"]
}

-----

help.py

import os
import json
import hashlib
import logging
import traceback
from datetime import datetime
from typing import Dict, Any, List, Optional

from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

class DocumentIntelligenceModelTrainer:
    def __init__(self, config_path: str = 'config.json'):
        """
        Initialize Document Intelligence Model Trainer
        """
        # Load configuration
        with open(config_path, 'r') as f:
            self.cfg = json.load(f)
        
        # Setup logging
        self.logger = logging.getLogger(__name__)
        
        # Initialize Azure client
        self.client = self._initialize_client()
        
        # Ensure directories exist
        self._prepare_directories()

    def _prepare_directories(self):
        """
        Ensure necessary directories exist
        """
        directories = [
            self.cfg['paths']['input_dir'],
            self.cfg['paths']['output_dir'],
            self.cfg['paths']['source_dir'],
            self.cfg['paths']['classified_dir'],
            self.cfg['paths']['unclassified_dir']
        ]
        
        for dir_path in directories:
            os.makedirs(dir_path, exist_ok=True)

    def _initialize_client(self) -> DocumentIntelligenceClient:
        """
        Initialize Azure Document Intelligence Client
        """
        try:
            credential = AzureKeyCredential(self.cfg["azure_document_intelligence"]["key"])
            return DocumentIntelligenceClient(
                endpoint=self.cfg["azure_document_intelligence"]["endpoint"],
                credential=credential
            )
        except Exception as e:
            self.logger.critical(f"Client initialization error: {e}")
            raise

    def _compute_reference_hash(self, ref_dir: str) -> str:
        """
        Compute hash of reference documents
        """
        hasher = hashlib.sha256()
        
        for root, _, files in sorted(os.walk(ref_dir)):
            for file in sorted(files):
                path = os.path.join(root, file)
                with open(path, 'rb') as f:
                    hasher.update(f.read())
        
        return hasher.hexdigest()

    def train_custom_model(self) -> Optional[str]:
        """
        Train custom Document Intelligence model
        """
        try:
            # Prepare reference documents
            ref_dir = self.cfg['paths']['reference_dir']
            training_files = []
            
            # Collect training documents
            for main_category in os.listdir(ref_dir):
                main_path = os.path.join(ref_dir, main_category)
                if not os.path.isdir(main_path):
                    continue
                
                for subcategory in os.listdir(main_path):
                    subcat_path = os.path.join(main_path, subcategory)
                    if not os.path.isdir(subcat_path):
                        continue
                    
                    # Collect documents
                    documents = [
                        os.path.join(subcat_path, doc) 
                        for doc in os.listdir(subcat_path)
                        if os.path.isfile(os.path.join(subcat_path, doc))
                    ]
                    
                    # Limit documents
                    documents = documents[:self.cfg['model_training']['max_training_documents']]
                    training_files.extend(documents)
            
            # Validate training files
            if not training_files:
                self.logger.error("No training documents found")
                return None
            
            # Generate unique model identifier
            model_id = f"custom-model-{datetime.now().strftime('%Y%m%d%H%M%S')}"
            
            # Prepare training files
            training_file_handles = [open(file, 'rb') for file in training_files]
            
            try:
                # Placeholder for model training
                # Note: This might need adjustment based on current SDK
                result = self.client.build_model(
                    model_id=model_id,
                    training_files=training_file_handles
                )
                
                self.logger.info(f"Custom model trained: {model_id}")
                return model_id
            
            finally:
                # Close file handles
                for file in training_file_handles:
                    file.close()
        
        except Exception as e:
            self.logger.error(f"Model training error: {e}")
            self.logger.error(traceback.format_exc())
            return None

    def get_latest_model_id(self) -> str:
        """
        Retrieve latest trained model ID
        """
        # For now, return pre-built document model
        # In a full implementation, you'd track and retrieve custom models
        return "prebuilt-document"

    def classify_document(self, file_path: str) -> Dict[str, Any]:
        """
        Classify document using Document Intelligence
        """
        try:
            # Open document
            with open(file_path, "rb") as doc_file:
                # Use pre-built document model
                poller = self.client.begin_analyze_document(
                    "prebuilt-document",
                    doc_file
                )
                result = poller.result()
            
            # Default classification
            classification = {
                'main_category': 'unknown',
                'subcategory': 'unknown',
                'confidence_score': 0.0
            }
            
            # Process results
            if result.documents and result.documents[0].fields:
                document = result.documents[0]
                
                # Implement classification logic
                for cat, subcats in self.cfg['categories'].items():
                    for subcat in subcats:
                        # Example classification logic
                        if any(subcat in str(field) for field in document.fields.values()):
                            classification['main_category'] = cat
                            classification['subcategory'] = subcat
                            classification['confidence_score'] = document.confidence
                            break
            
            return classification

        except Exception as e:
            self.logger.error(f"Document classification error: {e}")
            return {
                'main_category': 'unknown',
                'subcategory': 'unknown',
                'confidence_score': 0.0
            }


----

import os
import shutil
import logging
import fitz
from helper import DocumentIntelligenceModelTrainer

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

def process_documents(trainer):
    """
    Document processing workflow
    """
    # Configuration
    cfg = trainer.cfg
    input_dir = cfg['paths']['input_dir']
    source_dir = cfg['paths']['source_dir']
    classified_dir = cfg['paths']['classified_dir']
    unclassified_dir = cfg['paths']['unclassified_dir']
    
    # Train custom model (if possible)
    trainer.train_custom_model()
    
    # Process each document
    for filename in os.listdir(input_dir):
        filepath = os.path.join(input_dir, filename)
        
        # Skip unsupported file types
        if not any(filename.lower().endswith(ext) for ext in cfg['allowed_file_types']):
            logging.warning(f"Unsupported file type: {filename}")
            continue
        
        # Copy to source directory
        source_filepath = os.path.join(source_dir, filename)
        shutil.copy(filepath, source_filepath)
        
        # Open PDF
        doc = fitz.open(filepath)
        
        # Classify document
        classification = trainer.classify_document(filepath)
        
        # Determine confidence threshold
        confidence_threshold = cfg['model_training']['confidence_threshold']
        
        # Prepare output
        if classification['confidence_score'] >= confidence_threshold:
            # Classified document
            output_category_dir = os.path.join(
                classified_dir, 
                classification['main_category'], 
                classification['subcategory']
            )
            os.makedirs(output_category_dir, exist_ok=True)
            
            # Save classified document
            output_filepath = os.path.join(output_category_dir, filename)
            shutil.copy(filepath, output_filepath)
        else:
            # Unclassified document
            output_filepath = os.path.join(unclassified_dir, filename)
            shutil.copy(filepath, output_filepath)
        
        # Close document
        doc.close()

def main():
    # Initialize model trainer
    trainer = DocumentIntelligenceModelTrainer()
    
    # Process documents
    process_documents(trainer)

if __name__ == "__main__":
    main()
```

### Output Structure
```
output/
├── source/
│   └── original_documents
├── classified/
│   ├── cms1500/
│   │   └── cadwell/
│   │       └── classified_documents
│   └── invoice/
│       └── tesla/
│           └── classified_documents
└── unclassified/
    └── unclassified_documents
