help.py

import os
import json
import hashlib
import logging
import shutil
from openai import AzureOpenAI

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='w'
)

def verify_azure_configuration(cfg):
    """
    Comprehensive Azure OpenAI configuration validation
    """
    # Check API Key
    if not cfg['azure_openai']['api_key'] or cfg['azure_openai']['api_key'] == 'YOUR_AZURE_OPENAI_KEY':
        raise ValueError("‚ùå Invalid Azure OpenAI API Key. Replace with your actual key.")
    
    # Check Endpoint
    if not cfg['azure_openai']['endpoint'] or 'openai.azure.com' not in cfg['azure_openai']['endpoint']:
        raise ValueError("‚ùå Invalid Azure OpenAI Endpoint")
    
    # Check Deployment Name
    if not cfg['azure_openai']['deployment_name']:
        raise ValueError("‚ùå Missing Deployment Name")
    
    # Check API Version
    supported_versions = ['2024-05-01-preview', '2024-02-15-preview']
    if cfg['azure_openai']['api_version'] not in supported_versions:
        raise ValueError(f"‚ùå Unsupported API Version. Use one of {supported_versions}")

def load_config(config_path="config.json"):
    """
    Load configuration from JSON file
    """
    try:
        with open(config_path, "r") as f:
            cfg = json.load(f)
        
        # Verify configuration
        verify_azure_configuration(cfg)
        return cfg
    except FileNotFoundError:
        logging.error(f"Config file not found: {config_path}")
        raise
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON in config file: {config_path}")
        raise

def compute_reference_hash(ref_dir):
    """
    Compute a comprehensive hash of reference directory contents
    """
    hasher = hashlib.sha256()
    
    # Ensure consistent sorting and hashing
    for root, _, files in sorted(os.walk(ref_dir)):
        for f in sorted(files):
            path = os.path.join(root, f)
            
            # Include file path, modification time, and content
            hasher.update(path.encode())
            hasher.update(str(os.path.getmtime(path)).encode())
            
            try:
                with open(path, 'rb') as file:
                    hasher.update(file.read())
            except Exception as e:
                logging.warning(f"Could not read file {path} for hashing: {e}")
    
    return hasher.hexdigest()

def prepare_reference_metadata(ref_dir):
    """
    Generate a structured metadata of reference documents
    """
    reference_metadata = {}
    
    for main_category in os.listdir(ref_dir):
        main_path = os.path.join(ref_dir, main_category)
        if not os.path.isdir(main_path):
            continue
        
        reference_metadata[main_category] = {}
        
        for subcategory in os.listdir(main_path):
            subcat_path = os.path.join(main_path, subcategory)
            if not os.path.isdir(subcat_path):
                continue
            
            # Count documents in each subcategory
            doc_count = len([f for f in os.listdir(subcat_path) 
                             if os.path.isfile(os.path.join(subcat_path, f))])
            
            reference_metadata[main_category][subcategory] = {
                'document_count': doc_count,
                'documents': [f for f in os.listdir(subcat_path) 
                              if os.path.isfile(os.path.join(subcat_path, f))]
            }
    
    return reference_metadata

def fine_tune_if_new_reference(cfg):
    """
    Check if reference data has changed and log details
    """
    ref_dir = cfg["paths"]["reference_dir"]
    hash_file = os.path.join(ref_dir, ".reference_hash")
    
    try:
        # Compute current reference hash
        hash_now = compute_reference_hash(ref_dir)
        
        # Check if hash file exists
        if os.path.exists(hash_file):
            with open(hash_file, 'r') as f:
                last_hash = f.read().strip()
        else:
            last_hash = ''
        
        # Compare hashes
        if hash_now != last_hash:
            # Log detailed changes
            current_metadata = prepare_reference_metadata(ref_dir)
            
            logging.info("üöÄ New Reference Data Detected")
            logging.info("Reference Document Metadata:")
            logging.info(json.dumps(current_metadata, indent=2))
            
            # Save new hash
            with open(hash_file, 'w') as f:
                f.write(hash_now)
            
            return True
        
        return False
    
    except Exception as e:
        logging.error(f"Reference check error: {e}")
        return False

def get_azure_client(cfg):
    """
    Initialize Azure OpenAI client with error handling
    """
    try:
        client = AzureOpenAI(
            api_key=cfg["azure_openai"]["api_key"],
            api_version=cfg["azure_openai"]["api_version"],
            azure_endpoint=cfg["azure_openai"]["endpoint"]
        )
        return client, cfg["azure_openai"]["deployment_name"]
    except Exception as e:
        logging.critical(f"Azure client initialization error: {e}")
        raise

def prepare_directories(cfg):
    """
    Prepare necessary directories for processing
    """
    # Ensure input, output, and reference directories exist
    directories = [
        cfg['paths']['input_dir'],
        cfg['paths']['output_dir'],
        cfg['paths']['reference_dir'],
        os.path.join(cfg['paths']['output_dir'], 'source'),
        os.path.join(cfg['paths']['output_dir'], 'classified'),
        os.path.join(cfg['paths']['output_dir'], 'unclassified')
    ]
    
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)


----
main.py

import os
import io
import json
import base64
import shutil
import logging
import traceback
import fitz  # PyMuPDF
from PIL import Image

from helper import (
    load_config, 
    fine_tune_if_new_reference, 
    get_azure_client, 
    prepare_directories,
    prepare_reference_metadata
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='w'
)

def preprocess_document_image(document_image):
    """
    Standardize image for classification
    """
    try:
        # Open image from bytes
        img = Image.open(io.BytesIO(document_image))
        
        # Convert to RGB if needed
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # Resize to standard size
        img = img.resize((800, 600), Image.LANCZOS)
        
        # Compress and convert back to bytes
        buffer = io.BytesIO()
        img.save(buffer, format="PNG", optimize=True, quality=85)
        
        return buffer.getvalue()
    
    except Exception as e:
        logging.error(f"Image preprocessing error: {e}")
        # Return original image if preprocessing fails
        return document_image

def extract_first_page_image(file_path):
    """
    Extract first page image from various document types
    """
    try:
        # PDF handling
        if file_path.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            
            # Check if document has pages
            if len(doc) == 0:
                logging.error(f"Empty PDF: {file_path}")
                return _create_blank_image()
            
            page = doc.load_page(0)
            pix = page.get_pixmap()
            
            # Validate pixmap
            if not pix or pix.width <= 0 or pix.height <= 0:
                logging.error(f"Invalid PDF page: {file_path}")
                return _create_blank_image()
            
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            doc.close()
        
        # Image handling
        elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):
            img = Image.open(file_path)
        
        else:
            logging.error(f"Unsupported file type: {file_path}")
            return _create_blank_image()
        
        # Resize and convert to bytes
        img = img.resize((800, 600), Image.LANCZOS)
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        return buf.getvalue()
    
    except Exception as e:
        logging.error(f"Critical error extracting image from {file_path}: {e}")
        logging.error(traceback.format_exc())
        return _create_blank_image()

def _create_blank_image():
    """
    Create a blank white image for error cases
    """
    img = Image.new('RGB', (800, 600), color='white')
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()


def classify_document(document_image, client, deployment_name, cfg):
    """
    Robust document classification method compatible with Azure GPT-4o
    """
    try:
        # Validate image input
        if not isinstance(document_image, bytes):
            raise ValueError("Invalid image input type")

        # Convert image to base64
        base64_image = base64.b64encode(document_image).decode('utf-8')

        # Prepare reference categories for prompt
        categories_str = json.dumps(cfg['categories'], indent=2)

        # Detailed classification prompt
        prompt = f"""
        Carefully analyze this document image and classify it into the most appropriate category.

        Available Categories:
        {categories_str}

        Provide your classification in this JSON format:
        {{
            "main_category": "exact main category",
            "subcategory": "exact subcategory",
            "confidence_score": 0.0-1.0,
            "reasoning": "brief explanation"
        }}

        If unsure, choose the most likely category with an appropriate confidence score.
        """

        # ‚úÖ Azure-compliant chat completion with image input
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {
                    "role": "system",
                    "content": prompt
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "Please classify this document image."
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            response_format={"type": "json_object"}
        )

        # Parse response
        result = json.loads(response.choices[0].message.content)

        main_category = result.get('main_category', 'unknown')
        subcategory = result.get('subcategory', 'unknown')
        confidence_score = float(result.get('confidence_score', 0.0))
        reasoning = result.get('reasoning', 'No reasoning provided')

        # Validate categories
        if main_category not in cfg['categories']:
            main_category = list(cfg['categories'].keys())[0]

        if subcategory not in cfg['categories'].get(main_category, []):
            subcategory = cfg['categories'][main_category][0]

        logging.info("Classification Result:\n" + json.dumps(result, indent=2))

        return main_category, subcategory, confidence_score, reasoning

    except Exception as e:
        logging.error(f"Classification error: {e}")
        logging.error(traceback.format_exc())
        
        default_main = list(cfg['categories'].keys())[0]
        default_sub = cfg['categories'][default_main][0]
        
        return default_main, default_sub, 0.1, f"Error in classification: {str(e)}"


def process_documents():
    """
    Automated document classification
    """
    try:
        # Load configuration
        cfg = load_config()
        
        # Prepare directories
        prepare_directories(cfg)
        
        # Check for reference changes
        fine_tune_if_new_reference(cfg)
        
        # Initialize Azure client
        client, deployment = get_azure_client(cfg)
        
        # Set up directories
        input_dir = cfg['paths']['input_dir']
        output_dir = cfg['paths']['output_dir']
        
        # Processing statistics
        stats = {
            'total_documents': 0,
            'classified_documents': 0,
            'unclassified_documents': 0,
            'category_breakdown': {}
        }

        # Process each document
        for fname in os.listdir(input_dir):
            fpath = os.path.join(input_dir, fname)
            
            # Skip directories and hidden files
            if not os.path.isfile(fpath) or fname.startswith('.'):
                continue

            # Increment total documents
            stats['total_documents'] += 1

            try:
                # Extract first page image
                document_image = extract_first_page_image(fpath)
                
                # Preprocess image
                processed_image = preprocess_document_image(document_image)

                # Classify document
                main_cat, sub_cat, confidence, reasoning = classify_document(
                    processed_image, 
                    client, 
                    deployment,
                    cfg
                )

                # Determine destination
                confidence_threshold = cfg['classification'].get('confidence_threshold', 0.5)
                if confidence >= confidence_threshold:
                    dest_dir = os.path.join(output_dir, "classified", main_cat, sub_cat)
                    stats['classified_documents'] += 1
                    
                    # Update category breakdown
                    if main_cat not in stats['category_breakdown']:
                        stats['category_breakdown'][main_cat] = {}
                    stats['category_breakdown'][main_cat][sub_cat] = \
                        stats['category_breakdown'][main_cat].get(sub_cat, 0) + 1
                else:
                    dest_dir = os.path.join(output_dir, "unclassified")
                    stats['unclassified_documents'] += 1

                # Create destination directory
                os.makedirs(dest_dir, exist_ok=True)
                
                # Copy document
                dest_path = os.path.join(dest_dir, fname)
                shutil.copy(fpath, dest_path)
                
                # Create metadata
                metadata_path = os.path.join(dest_dir, f"{os.path.splitext(fname)[0]}_metadata.json")
                with open(metadata_path, 'w') as metadata_file:
                    json.dump({
                        "filename": fname,
                        "main_category": main_cat,
                        "subcategory": sub_cat,
                        "confidence_score": confidence,
                        "reasoning": reasoning
                    }, metadata_file, indent=2)

            except Exception as e:
                logging.error(f"Processing error for {fname}: {e}")
                stats['unclassified_documents'] += 1

        # Log processing summary
        logging.info("\n--- Processing Summary ---")
        logging.info(json.dumps(stats, indent=2))

        print("\n‚úÖ Document Processing Complete")

    except Exception as overall_error:
        logging.critical(f"Critical processing error: {overall_error}")
        logging.critical(traceback.format_exc())
        print("‚ùå Document Processing Failed. Check logs for details.")

if __name__ == "__main__":
    process_documents()

--------
config.json

{
  "azure_openai": {
    "api_key": "YOUR_AZURE_OPENAI_KEY",
    "endpoint": "https://YOUR-RESOURCE-NAME.openai.azure.com/",
    "deployment_name": "gpt-4o",
    "api_version": "2024-05-01-preview"
  },
  "categories": {
    "cms1500": ["cadwell", "rhymlink"],
    "invoice": ["tesla", "amazon"],
    "scheduling": ["email", "iomrequest"]
  },
  "paths": {
    "reference_dir": "reference",
    "input_dir": "input_docs",
    "output_dir": "output"
  },
  "classification": {
    "confidence_threshold": 0.5
  }
}

--------------
updated main.py

import os
import io
import json
import base64
import shutil
import logging
import traceback
import fitz  # PyMuPDF
from PIL import Image, ImageDraw, ImageFont
from datetime import datetime

from helper import (
    load_config, 
    fine_tune_if_new_reference, 
    get_azure_client, 
    prepare_directories,
    prepare_reference_metadata
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'  # Append mode to keep historical logs
)

def extract_first_page_image(file_path):
    """
    Extract first page image from various document types
    """
    try:
        # PDF handling
        if file_path.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            
            # Check if document has pages
            if len(doc) == 0:
                logging.error(f"Empty PDF: {file_path}")
                return _create_blank_image()
            
            page = doc.load_page(0)
            pix = page.get_pixmap()
            
            # Validate pixmap
            if not pix or pix.width <= 0 or pix.height <= 0:
                logging.error(f"Invalid PDF page: {file_path}")
                return _create_blank_image()
            
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            doc.close()
        
        # DOCX handling
        elif file_path.lower().endswith('.docx'):
            from docx import Document
            doc = Document(file_path)
            
            # Create image from first paragraph text
            text = "\n".join([p.text for p in doc.paragraphs[:3]])
            img = Image.new("RGB", (800, 600), "white")
            draw = ImageDraw.Draw(img)
            draw.text((10, 10), text, fill="black")
        
        # Image handling
        elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):
            img = Image.open(file_path)
        
        else:
            logging.error(f"Unsupported file type: {file_path}")
            return _create_blank_image()
        
        # Resize and convert to bytes
        img = img.resize((800, 600), Image.LANCZOS)
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        return buf.getvalue()
    
    except Exception as e:
        logging.error(f"Critical error extracting image from {file_path}: {e}")
        logging.error(traceback.format_exc())
        return _create_blank_image()

def _create_blank_image():
    """
    Create a blank white image for error cases
    """
    img = Image.new('RGB', (800, 600), color='white')
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()

def extract_page_image(file_path, page_number):
    """
    Extract image for a specific page from various document types
    """
    try:
        # PDF handling
        if file_path.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            
            # Validate page number
            if page_number < 0 or page_number >= len(doc):
                logging.error(f"Invalid page number {page_number} for {file_path}")
                return _create_blank_image()
            
            page = doc.load_page(page_number)
            pix = page.get_pixmap()
            
            # Validate pixmap
            if not pix or pix.width <= 0 or pix.height <= 0:
                logging.error(f"Invalid page {page_number} in {file_path}")
                return _create_blank_image()
            
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            doc.close()
        
        # Image handling
        elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):
            img = Image.open(file_path)
        
        else:
            logging.error(f"Unsupported file type: {file_path}")
            return _create_blank_image()
        
        # Resize and convert to bytes
        img = img.resize((800, 600), Image.LANCZOS)
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        return buf.getvalue()
    
    except Exception as e:
        logging.error(f"Error extracting page {page_number} from {file_path}: {e}")
        logging.error(traceback.format_exc())
        return _create_blank_image()

def classify_page(document_image, client, deployment_name, cfg):
    """
    Page-level classification method
    """
    try:
        # Validate image input
        if not isinstance(document_image, bytes):
            raise ValueError("Invalid image input type")

        # Convert image to base64
        base64_image = base64.b64encode(document_image).decode('utf-8')

        # Prepare reference categories for prompt
        categories_str = json.dumps(cfg['categories'], indent=2)

        # Detailed page classification prompt
        prompt = f"""
        Carefully analyze this single page image and classify it into the most precise category.

        Available Categories:
        {categories_str}

        Provide your classification in this JSON format:
        {{
            "main_category": "exact main category",
            "subcategory": "exact subcategory",
            "confidence_score": 0.0-1.0,
            "reasoning": "brief explanation of page classification"
        }}

        Focus on the content of this specific page. If the page doesn't clearly belong to a category, 
        choose the most probable category or return 'unknown'.
        """

        # Make API call
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": [
                    {
                        "type": "text",
                        "text": "Please classify this page."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image}"
                        }
                    }
                ]}
            ],
            response_format={"type": "json_object"}
        )

        # Parse response
        result = json.loads(response.choices[0].message.content)

        # Extract and validate classification
        main_category = result.get('main_category', 'unknown')
        subcategory = result.get('subcategory', 'unknown')
        confidence_score = float(result.get('confidence_score', 0.0))
        reasoning = result.get('reasoning', 'No reasoning provided')

        # Validate categories
        if main_category not in cfg['categories']:
            main_category = list(cfg['categories'].keys())[0]

        if subcategory not in cfg['categories'].get(main_category, []):
            subcategory = cfg['categories'][main_category][0]

        return main_category, subcategory, confidence_score, reasoning

    except Exception as e:
        logging.error(f"Page classification error: {e}")
        logging.error(traceback.format_exc())
        
        default_main = list(cfg['categories'].keys())[0]
        default_sub = cfg['categories'][default_main][0]
        
        return default_main, default_sub, 0.1, f"Error in page classification: {str(e)}"

def process_documents():
    """
    Page-level document classification
    """
    try:
        # Load configuration
        cfg = load_config()
        
        # Prepare directories
        prepare_directories(cfg)
        
        # Check for reference changes
        fine_tune_if_new_reference(cfg)
        
        # Initialize Azure client
        client, deployment = get_azure_client(cfg)
        
        # Set up directories
        input_dir = cfg['paths']['input_dir']
        output_dir = cfg['paths']['output_dir']
        source_dir = os.path.join(output_dir, 'source')
        classified_dir = os.path.join(output_dir, 'classified')
        unclassified_dir = os.path.join(output_dir, 'unclassified')
        
        # Logging for processing session
        processing_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        logging.info(f"\n\n--- Processing Session Started: {processing_timestamp} ---")
        
        # Processing statistics
        stats = {
            'total_documents': 0,
            'total_pages': 0,
            'classified_pages': 0,
            'unclassified_pages': 0,
            'page_classification_details': {}
        }

        # Process each document
        for fname in os.listdir(input_dir):
            fpath = os.path.join(input_dir, fname)
            
            # Skip directories and hidden files
            if not os.path.isfile(fpath) or fname.startswith('.'):
                continue

            # Determine file type
            file_type = os.path.splitext(fname)[1].lower()
            
            # Skip non-processable file types
            if file_type not in ['.pdf', '.jpg', '.jpeg', '.png', '.docx']:
                logging.warning(f"Unsupported file type: {fname}")
                continue

            # Count total documents
            stats['total_documents'] += 1

            try:
                # Open PDF to get page count (for PDFs)
                if file_type == '.pdf':
                    doc = fitz.open(fpath)
                    total_pages = len(doc)
                    doc.close()
                else:
                    total_pages = 1  # For non-PDF files

                stats['total_pages'] += total_pages

                # Copy original document to source directory
                shutil.copy(fpath, os.path.join(source_dir, fname))

                # Page-level classification
                classified_pages_by_category = {}
                unclassified_pages = []

                for page_num in range(total_pages):
                    # Extract page image
                    page_image = extract_page_image(fpath, page_num)
                    
                    # Classify page
                    main_cat, sub_cat, confidence, reasoning = classify_page(
                        page_image, 
                        client, 
                        deployment,
                        cfg
                    )

                    # Determine page destination
                    confidence_threshold = cfg['classification'].get('confidence_threshold', 0.5)
                    if confidence >= confidence_threshold:
                        # Organize classified pages by category
                        category_key = f"{main_cat}|{sub_cat}"
                        if category_key not in classified_pages_by_category:
                            classified_pages_by_category[category_key] = []
                        classified_pages_by_category[category_key].append(page_num)
                        
                        stats['classified_pages'] += 1
                    else:
                        unclassified_pages.append(page_num)
                        stats['unclassified_pages'] += 1

                    # Update page classification details
                    if main_cat not in stats['page_classification_details']:
                        stats['page_classification_details'][main_cat] = {}
                    stats['page_classification_details'][main_cat][sub_cat] = \
                        stats['page_classification_details'][main_cat].get(sub_cat, 0) + 1

                # Process classified pages by category
                for category, pages in classified_pages_by_category.items():
                    main_cat, sub_cat = category.split('|')
                    
                    # Create classified directory
                    cat_dest_dir = os.path.join(classified_dir, main_cat, sub_cat)
                    os.makedirs(cat_dest_dir, exist_ok=True)

                    # Create new PDF with classified pages
                    doc = fitz.open(fpath)
                    output = fitz.open()
                    
                    # Extract specified pages
                    for page_num in pages:
                        output.insert_pdf(doc, from_page=page_num, to_page=page_num)
                    
                    # Generate output filename
                    page_range_str = "_".join(map(str, [p+1 for p in pages]))
                    output_filename = f"{os.path.splitext(fname)[0]}_pages_{page_range_str}.pdf"
                    
                    # Save classified pages PDF
                    output_path = os.path.join(cat_dest_dir, output_filename)
                    output.save(output_path)
                    
                    output.close()
                    doc.close()

                # Handle unclassified pages
                if unclassified_pages:
                    # Create unclassified directory
                    os.makedirs(unclassified_dir, exist_ok=True)

                    # Create new PDF with unclassified pages
                    doc = fitz.open(fpath)
                    output = fitz.open()
                    
                    # Extract unclassified pages
                    for page_num in unclassified_pages:
                        output.insert_pdf(doc, from_page=page_num, to_page=page_num)
                    
                    # Generate unclassified filename
                    page_range_str = "_".join(map(str, [p+1 for p in unclassified_pages]))
                    unclassified_filename = f"{os.path.splitext(fname)[0]}_unclassified_pages_{page_range_str}.pdf"
                    
                    # Save unclassified pages PDF
                    unclassified_path = os.path.join(unclassified_dir, unclassified_filename)
                    output.save(unclassified_path)
                    
                    output.close()
                    doc.close()

            except Exception as e:
                logging.error(f"Processing error for {fname}: {e}")
                logging.error(traceback.format_exc())

        # Log comprehensive processing summary
        logging.info("\n--- Processing Session Summary ---")
        logging.info(json.dumps(stats, indent=2))

        print("\n‚úÖ Page-Level Document Processing Complete")

    except Exception as overall_error:
        logging.critical(f"Critical processing error: {overall_error}")
        logging.critical(traceback.format_exc())
        print("‚ùå Document Processing Failed. Check logs for details.")

if __name__ == "__main__":
    process_documents()
