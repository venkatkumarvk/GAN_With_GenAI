#config.json
{
  "azure_openai": {
    "api_key": "YOUR_AZURE_OPENAI_API_KEY",
    "api_version": "2025-03-01-preview",
    "azure_endpoint": "YOUR_AZURE_OPENAI_ENDPOINT",
    "deployment_name": "YOUR_DEPLOYMENT_NAME"
  },
  "azure_storage": {
    "connection_string": "YOUR_AZURE_STORAGE_CONNECTION_STRING",
    "input_container": "pdf-input",
    "output_container": "pdf-extraction-results",
    "high_confidence_folder": "high_confidence/",
    "low_confidence_folder": "low_confidence/"
  },
  "processing": {
    "confidence_threshold": 95.0,
    "batch_size": 10,
    "zoom_factor": 1.5,
    "timeout_seconds": 600,
    "extraction_fields": [
      "VendorName", 
      "InvoiceNumber", 
      "InvoiceDate", 
      "CustomerName", 
      "PurchaseOrder", 
      "StockCode", 
      "UnitPrice", 
      "InvoiceAmount", 
      "Freight", 
      "Salestax", 
      "Total"
    ]
  }
}

#llm.py
import os
import json
import time
import tempfile
from io import BytesIO
from datetime import datetime
from openai import AzureOpenAI

class AzureOpenAIClient:
    def __init__(self, config):
        self.api_key = config["azure_openai"]["api_key"]
        self.api_version = config["azure_openai"]["api_version"]
        self.endpoint = config["azure_openai"]["azure_endpoint"]
        self.deployment_name = config["azure_openai"]["deployment_name"]
        self.batch_size = config["processing"]["batch_size"]
        self.timeout = config["processing"]["timeout_seconds"]
        
        self.client = AzureOpenAI(
            api_key=self.api_key,
            api_version=self.api_version,
            azure_endpoint=self.endpoint
        )
    
    def prepare_batch_jsonl(self, image_base64_strings, prompts):
        """
        Prepares a JSONL file for batch processing.
        
        Parameters:
        - image_base64_strings: List of base64-encoded image strings
        - prompts: List of corresponding prompts
        
        Returns:
        - BytesIO object containing the JSONL content
        """
        jsonl_file = BytesIO()
        
        for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
            # Create the request object with proper data URL format
            request = {
                "custom_id": f"request-{i+1}",
                "method": "POST",
                "url": "/chat/completions",
                "body": {
                    "model": self.deployment_name,
                    "messages": [
                        {
                            "role": "system",
                            "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
                        },
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_img}"
                                    }
                                }
                            ]
                        }
                    ],
                    "max_tokens": 2000
                }
            }
            
            # Write the JSON line to the file
            jsonl_file.write((json.dumps(request) + "\n").encode('utf-8'))
        
        # Reset the file pointer to the beginning
        jsonl_file.seek(0)
        return jsonl_file
    
    def process_batch(self, image_base64_strings, prompts):
        """
        Process images in a batch using the Azure OpenAI batch API.
        
        Parameters:
        - image_base64_strings: List of base64-encoded image strings
        - prompts: List of corresponding prompts
        
        Returns:
        - List of raw API responses
        """
        # We need to create a temporary JSONL file as the API might have issues with direct BytesIO
        tmp_jsonl_path = None
        
        try:
            # Prepare the JSONL batch file
            jsonl_file = self.prepare_batch_jsonl(image_base64_strings, prompts)
            
            # Create a temporary file for the JSONL content
            with tempfile.NamedTemporaryFile(suffix='.jsonl', delete=False) as tmp_jsonl:
                tmp_jsonl.write(jsonl_file.getvalue())
                tmp_jsonl_path = tmp_jsonl.name
            
            # Upload with explicit file open
            print("Uploading batch file to Azure...")
            with open(tmp_jsonl_path, 'rb') as f:
                file = self.client.files.create(
                    file=f,
                    purpose="batch"
                )
            
            # We can delete the temp file immediately after upload
            if tmp_jsonl_path:
                os.unlink(tmp_jsonl_path)
                tmp_jsonl_path = None
            
            file_id = file.id
            print(f"File uploaded (ID: {file_id}). Creating batch job...")
            
            # Submit batch job
            batch_response = self.client.batches.create(
                input_file_id=file_id,
                endpoint="/chat/completions",
                completion_window="24h"
            )
            
            batch_id = batch_response.id
            print(f"Batch job created (ID: {batch_id}). Waiting for processing...")
            
            # Track batch job status
            status = "validating"
            start_time = time.time()
            
            while status not in ("completed", "failed", "canceled"):
                time.sleep(10)  # Check every 10 seconds for faster feedback
                
                # Check for timeout
                if time.time() - start_time > self.timeout:
                    print(f"Timeout after {self.timeout} seconds. Canceling batch job.")
                    try:
                        self.client.batches.cancel(batch_id)
                    except:
                        pass
                    raise TimeoutError(f"Batch processing timed out after {self.timeout} seconds")
                
                batch_response = self.client.batches.retrieve(batch_id)
                status = batch_response.status
                status_message = f"{datetime.now()} Batch Id: {batch_id}, Status: {status}"
                print(status_message)
            
            if batch_response.status == "failed":
                error_message = "Batch processing failed:"
                if hasattr(batch_response, 'errors') and hasattr(batch_response.errors, 'data'):
                    for error in batch_response.errors.data:
                        error_message += f"\nError code {error.code} Message {error.message}"
                else:
                    error_message += " Unknown error occurred"
                print(error_message)
                raise Exception(error_message)
            
            # Retrieve results
            output_file_id = batch_response.output_file_id
            
            if not output_file_id:
                output_file_id = batch_response.error_file_id
                if not output_file_id:
                    print("No output or error file was produced by the batch job.")
                    raise Exception("No output file produced")
            
            print(f"Batch completed. Retrieving results...")
            file_response = self.client.files.content(output_file_id)
            raw_responses = file_response.text.strip().split('\n')
            
            return raw_responses
        
        except Exception as e:
            print(f"Error during batch processing: {str(e)}")
            raise
        finally:
            # Ensure temporary file is deleted if it exists
            if tmp_jsonl_path and os.path.exists(tmp_jsonl_path):
                try:
                    os.unlink(tmp_jsonl_path)
                except:
                    pass
    
    def process_general(self, image_base64_strings, prompts):
        """
        Process images using the general (non-batch) API.
        
        Parameters:
        - image_base64_strings: List of base64-encoded image strings
        - prompts: List of corresponding prompts
        
        Returns:
        - List of API responses
        """
        results = []
        
        for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
            try:
                print(f"Processing image {i+1}/{len(image_base64_strings)}")
                
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
                        },
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_img}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=2000,
                    temperature=0.7
                )
                
                if hasattr(response, 'choices') and len(response.choices) > 0:
                    content = response.choices[0].message.content
                    results.append(json.dumps({
                        "custom_id": f"request-{i+1}",
                        "response": {
                            "body": {
                                "choices": [
                                    {
                                        "message": {
                                            "content": content
                                        }
                                    }
                                ]
                            }
                        }
                    }))
                else:
                    results.append(json.dumps({
                        "custom_id": f"request-{i+1}",
                        "error": "No response content"
                    }))
            
            except Exception as e:
                print(f"Error processing image {i+1}: {str(e)}")
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "error": str(e)
                }))
        
        return results
#helper.py

import os
import io
import base64
import json
import tempfile
import fitz  # PyMuPDF
import pandas as pd
from datetime import datetime
from pathlib import Path
from azure.storage.blob import BlobServiceClient, ContentSettings

class AzureStorageHelper:
    def __init__(self, connection_string, input_container, output_container):
        self.connection_string = connection_string
        self.input_container = input_container
        self.output_container = output_container
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
    
    def list_blobs_in_folder(self, folder_path=""):
        """List all PDF blobs in the specified folder."""
        pdf_blobs = []
        container_client = self.blob_service_client.get_container_client(self.input_container)
        
        for blob in container_client.list_blobs(name_starts_with=folder_path):
            if blob.name.lower().endswith('.pdf'):
                pdf_blobs.append(blob.name)
        
        return pdf_blobs
    
    def download_blob_to_memory(self, blob_name):
        """Download a blob to memory."""
        try:
            container_client = self.blob_service_client.get_container_client(self.input_container)
            blob_client = container_client.get_blob_client(blob_name)
            
            download_stream = blob_client.download_blob()
            content = download_stream.readall()
            
            print(f"Successfully downloaded: {blob_name}")
            return content
        except Exception as e:
            print(f"Error downloading blob {blob_name}: {e}")
            return None
    
    def upload_to_storage(self, blob_name, data, content_type):
        """Upload data to Azure Blob Storage."""
        try:
            container_client = self.blob_service_client.get_container_client(self.output_container)
            
            # Create the container if it doesn't exist
            if not container_client.exists():
                container_client.create_container()
            
            # Upload blob
            blob_client = container_client.get_blob_client(blob_name)
            
            # Set content settings
            content_settings = ContentSettings(content_type=content_type)
            
            # Upload the file
            blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
            
            print(f"Successfully uploaded: {blob_name}")
            return True, blob_client.url
        except Exception as e:
            print(f"Error uploading blob {blob_name}: {e}")
            return False, str(e)

class PDFProcessor:
    def __init__(self, config):
        self.config = config
        self.extraction_fields = config["processing"]["extraction_fields"]
        self.confidence_threshold = config["processing"]["confidence_threshold"]
        self.zoom_factor = config["processing"]["zoom_factor"]
    
    def image_to_base64(self, image_bytes):
        """Convert image bytes to base64 string."""
        return base64.b64encode(image_bytes).decode('utf-8')
    
    def extract_pdf_pages(self, pdf_content):
        """
        Extract pages from PDF content as base64 encoded images.
        Returns: List of tuples (page_num, base64_string)
        """
        pages = []
        
        # Create a BytesIO object from content
        pdf_io = io.BytesIO(pdf_content)
        
        # Open the PDF directly from memory
        with fitz.open(stream=pdf_io, filetype="pdf") as doc:
            page_count = len(doc)
            print(f"PDF has {page_count} pages")
            
            # Extract all pages
            for page_num in range(page_count):
                try:
                    # Load page and convert to image
                    page = doc.load_page(page_num)
                    pix = page.get_pixmap(matrix=fitz.Matrix(self.zoom_factor, self.zoom_factor))
                    
                    # Convert to base64
                    image_bytes = pix.tobytes()
                    base64_string = base64.b64encode(image_bytes).decode('utf-8')
                    
                    # Add to pages
                    pages.append((page_num, base64_string))
                    
                    # Clean memory immediately
                    del image_bytes
                    del pix
                    
                except Exception as e:
                    print(f"Error extracting page {page_num+1}: {e}")
        
        return pages
    
    def create_extraction_prompt(self):
        """Create the prompt for classification and extraction."""
        return """First, classify this document into one of these categories:
- Terms & Conditions
- General Terms and Conditions
- Sale Order
- Delivery
- Price and Payment
- Warranty
- Other

If and ONLY if the document is in the "Other" category, extract the following information:
1) Vendor name
2) Invoice number
3) Invoice date
4) Customer name
5) Purchase order number
6) Stock code
7) Unit price
8) Invoice amount
9) Freight cost
10) Sales tax
11) Total amount

Format your response as a JSON object with these fields:
{
  "category": "the category name",
  "shouldExtract": true/false,
  "extractedData": {
    // Only include if shouldExtract is true
    "VendorName": {"value": "value", "confidence": 0.95},
    "InvoiceNumber": {"value": "value", "confidence": 0.95},
    ...and so on for all fields
  }
}"""
    
    def process_batch_results(self, results, page_numbers):
        """
        Process results from batch API.
        Returns: List of (page_num, category, extracted_info) tuples
        """
        processed_results = []
        
        for raw_response in results:
            try:
                json_response = json.loads(raw_response)
                
                # Extract the request ID to identify the page
                request_id = json_response.get("custom_id", "")
                if request_id.startswith("request-"):
                    idx = int(request_id.split("-")[1]) - 1
                    if idx < len(page_numbers):
                        page_num = page_numbers[idx]
                    else:
                        page_num = -1
                else:
                    page_num = -1
                
                # Process the actual content
                if "response" in json_response and "body" in json_response["response"]:
                    content = json_response["response"]["body"]
                    if isinstance(content, str):
                        content = json.loads(content)
                    
                    if "choices" in content and len(content["choices"]) > 0:
                        message_content = content["choices"][0]["message"]["content"]
                        result = json.loads(message_content)
                        
                        category = result.get("category", "Unknown")
                        
                        if category == "Other" and result.get("shouldExtract", False):
                            extracted_info = result.get("extractedData", {})
                            processed_results.append((page_num, category, extracted_info))
                        else:
                            processed_results.append((page_num, category, None))
            except Exception as e:
                print(f"Error processing result: {str(e)}")
                processed_results.append((page_num, "Error", None))
        
        return processed_results
    
    def has_high_confidence(self, extracted_results, threshold=None):
        """
        Determine if all fields have high confidence.
        
        Parameters:
        - extracted_results: List of (page_num, category, data) tuples
        - threshold: Confidence threshold (default: use config value)
        
        Returns:
        - True if all fields have confidence above threshold
        """
        if threshold is None:
            threshold = self.confidence_threshold
        
        for _, category, data in extracted_results:
            if category != "Other" or data is None:
                continue
                
            # Check each field's confidence
            for field_name, field_data in data.items():
                if field_name not in self.extraction_fields:
                    continue
                    
                if isinstance(field_data, dict) and "confidence" in field_data:
                    confidence = field_data["confidence"] * 100
                    if confidence < threshold:
                        return False
        
        return True
    
    def create_csv_for_results(self, extracted_results, filename):
        """
        Create a CSV file from extraction results.
        
        Parameters:
        - extracted_results: List of (page_num, category, data) tuples
        - filename: Original filename for metadata
        
        Returns:
        - CSV content as string
        - Invoice number from data (or "unknown")
        - Total amount from data (or "unknown")
        """
        # Extract fields for CSV
        pdf_rows = []
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Variables to store invoice number and total
        invoice_number = "unknown"
        total_amount = "unknown"
        
        for page_num, category, data in extracted_results:
            if category != "Other" or data is None:
                continue
                
            # Initialize row data
            row_data = {
                "Filename": filename,
                "Page": page_num + 1,
                "Extraction_Timestamp": timestamp
            }
            
            # Process each field
            for field in self.extraction_fields:
                field_data = data.get(field, {})
                
                if isinstance(field_data, dict):
                    value = field_data.get("value", "N/A")
                    confidence = field_data.get("confidence", 0)
                else:
                    value = field_data if field_data else "N/A"
                    confidence = 0
                
                # Add to row data
                row_data[field] = value
                row_data[f"{field} Confidence"] = round(confidence * 100, 2)
                
                # Capture invoice number and total for filename
                if field == "InvoiceNumber" and value != "N/A":
                    invoice_number = value
                elif field == "Total" and value != "N/A":
                    total_amount = value
            
            # No manual edits in automated version
            row_data["Manually_Edited_Fields"] = ""
            row_data["Manual_Edit"] = "N"
            
            pdf_rows.append(row_data)
        
        # Create DataFrame and CSV
        if pdf_rows:
            pdf_df = pd.DataFrame(pdf_rows, dtype=str)
            pdf_csv = pdf_df.to_csv(index=False)
            
            # Clean values for filename use
            safe_invoice_number = ''.join(c for c in str(invoice_number) if c.isalnum() or c in '-_.')
            safe_total_amount = ''.join(c for c in str(total_amount) if c.isalnum() or c in '-_.')
            
            return pdf_csv, safe_invoice_number, safe_total_amount
        
        return None, "unknown", "unknown"

#main.py
import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime

from helper import AzureStorageHelper, PDFProcessor
from llm import AzureOpenAIClient

def load_config(config_path):
    """Load configuration from a JSON file."""
    with open(config_path, 'r') as f:
        return json.load(f)

def process_pdf_files(config, api_type, azure_folder):
    """
    Process PDF files from Azure Blob Storage.
    
    Parameters:
    - config: Configuration dictionary
    - api_type: 'batch' or 'general'
    - azure_folder: Folder path in Azure Blob Storage
    """
    # Initialize helpers
    storage_helper = AzureStorageHelper(
        config["azure_storage"]["connection_string"],
        config["azure_storage"]["input_container"],
        config["azure_storage"]["output_container"]
    )
    
    pdf_processor = PDFProcessor(config)
    
    ai_client = AzureOpenAIClient(config)
    
    # List PDF blobs in the specified folder
    pdf_blobs = storage_helper.list_blobs_in_folder(azure_folder)
    
    if not pdf_blobs:
        print(f"No PDF files found in folder: {azure_folder}")
        return
    
    print(f"Found {len(pdf_blobs)} PDF files to process")
    
    # Process each PDF
    for i, blob_name in enumerate(pdf_blobs):
        try:
            print(f"Processing file {i+1}/{len(pdf_blobs)}: {blob_name}")
            
            # Download blob to memory
            blob_content = storage_helper.download_blob_to_memory(blob_name)
            
            if blob_content is None:
                print(f"Could not download blob: {blob_name}")
                continue
            
            # Extract pages as base64 strings
            filename = blob_name.split('/')[-1]
            pages = pdf_processor.extract_pdf_pages(blob_content)
            
            if not pages:
                print(f"No pages extracted from {filename}")
                continue
            
            print(f"Extracted {len(pages)} pages from {filename}")
            
            # Prepare batches for processing
            batch_size = config["processing"]["batch_size"]
            
            all_results = []
            for batch_start in range(0, len(pages), batch_size):
                batch_end = min(batch_start + batch_size, len(pages))
                batch_pages = pages[batch_start:batch_end]
                
                # Split into page numbers and base64 strings
                page_nums = [p[0] for p in batch_pages]
                base64_strings = [p[1] for p in batch_pages]
                
                # Create prompts
                prompts = [pdf_processor.create_extraction_prompt() for _ in range(len(batch_pages))]
                
                print(f"Processing batch of {len(batch_pages)} pages (pages {batch_start+1}-{batch_end})")
                
                # Process batch using specified API type
                if api_type == "batch":
                    raw_results = ai_client.process_batch(base64_strings, prompts)
                else:
                    raw_results = ai_client.process_general(base64_strings, prompts)
                
                # Process the results
                processed_results = pdf_processor.process_batch_results(raw_results, page_nums)
                all_results.extend(processed_results)
                
                print(f"Processed batch {batch_start+1}-{batch_end}")
            
            # Create CSV and determine confidence level
            csv_content, invoice_number, total_amount = pdf_processor.create_csv_for_results(
                all_results, filename
            )
            
            if csv_content:
                # Determine confidence level for folder structure
                is_high_confidence = pdf_processor.has_high_confidence(all_results)
                
                # Determine folder path based on confidence
                if is_high_confidence:
                    folder_path = config["azure_storage"]["high_confidence_folder"]
                    print(f"{filename} has HIGH confidence (â‰¥{config['processing']['confidence_threshold']}%)")
                else:
                    folder_path = config["azure_storage"]["low_confidence_folder"]
                    print(f"{filename} has LOW confidence (<{config['processing']['confidence_threshold']}%)")
                
                # Prepare filenames for upload
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                base_filename = os.path.splitext(filename)[0]
                
                # Upload CSV to blob storage
                csv_blob_name = f"{folder_path}{base_filename}_{invoice_number}_{total_amount}_{timestamp}.csv"
                csv_success, csv_url = storage_helper.upload_to_storage(
                    csv_blob_name,
                    csv_content,
                    "text/csv"
                )
                
                # Upload original PDF to appropriate folder
                source_folder = "source_documents/" + folder_path
                source_blob_name = f"{source_folder}{filename}"
                source_success, source_url = storage_helper.upload_to_storage(
                    source_blob_name,
                    blob_content,
                    "application/pdf"
                )
                
                print(f"CSV upload: {'Success' if csv_success else 'Failed'}")
                print(f"Source PDF upload: {'Success' if source_success else 'Failed'}")
            else:
                print(f"No extractable content found in {filename}")
        
        except Exception as e:
            print(f"Error processing {blob_name}: {str(e)}")
    
    print("Processing complete!")

def main():
    parser = argparse.ArgumentParser(description="Process PDF files using Azure OpenAI")
    parser.add_argument("--apitype", choices=["general", "batch"], required=True, 
                        help="API type to use (general or batch)")
    parser.add_argument("--azure_folder", required=True, 
                        help="Folder path in Azure Blob Storage")
    parser.add_argument("--config", default="config.json", 
                        help="Path to configuration file")
    
    args = parser.parse_args()
    
    try:
        # Load configuration
        config = load_config(args.config)
        
        # Process PDF files
        process_pdf_files(config, args.apitype, args.azure_folder)
        
    except Exception as e:
        print(f"Error: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

I need to add new argument  in local folder, user select azure or local then provide folder in arguments

which portion i need to change

