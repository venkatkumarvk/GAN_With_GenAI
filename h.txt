üìÅ PROJECT STRUCTURE
project/
‚îÇ
‚îú‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ helper.py
‚îú‚îÄ‚îÄ process_doc.py
‚îú‚îÄ‚îÄ documentintelligence_ocr.py
‚îú‚îÄ‚îÄ Embedding.py
‚îú‚îÄ‚îÄ AIsearch.py
‚îú‚îÄ‚îÄ rag.py
‚îú‚îÄ‚îÄ prompt.py
‚îú‚îÄ‚îÄ cost_tracking.py

1Ô∏è‚É£ config.json
{
  "Azureblob": {
    "connection_string": "YOUR_BLOB_CONNECTION_STRING",
    "inputcontainer": "testinputcontainer",
    "outputcontainer": "testoutputcontainer"
  },

  "AzureOpenAI_Chat": {
    "endpoint": "https://chat-resource.openai.azure.com/",
    "api_key": "CHAT_API_KEY",
    "api_version": "2024-02-01",
    "deployment_name": "gpt5-deployment"
  },

  "AzureOpenAI_Embedding": {
    "endpoint": "https://embedding-resource.openai.azure.com/",
    "api_key": "EMBEDDING_API_KEY",
    "api_version": "2024-02-01",
    "deployment_name": "embedding-deployment",
    "vector_dimensions": 3072
  },

  "AzureAISearch": {
    "endpoint": "https://your-search.search.windows.net",
    "api_key": "YOUR_SEARCH_ADMIN_KEY"
  },

  "DocumentIntelligence": {
    "endpoint": "https://your-docint.cognitiveservices.azure.com/",
    "api_key": "YOUR_DOCINT_KEY"
  },

  "required_fields": [
    "name",
    "dob",
    "passport_number",
    "license_number",
    "expiry_date"
  ],

  "confidence_threshold": 0.90
}

2Ô∏è‚É£ helper.py
import os
import json
import re
from datetime import datetime

SUPPORTED_EXTENSIONS = {
    ".pdf", ".doc", ".docx",
    ".png", ".jpg", ".jpeg", ".tiff", ".bmp"
}

def load_config():
    with open("config.json") as f:
        return json.load(f)

def utc_timestamp():
    return datetime.utcnow().strftime("%Y-%m-%dT%H-%M-%S")

def is_supported(filename):
    return os.path.splitext(filename.lower())[1] in SUPPORTED_EXTENSIONS

def normalize_index_name(provider):
    name = provider.lower()
    name = re.sub(r"[^a-z0-9-]", "-", name)
    return f"{name}-index"

3Ô∏è‚É£ process_doc.py
from azure.storage.blob import BlobServiceClient

def list_providers(connection_string, container, inputfolder):
    service = BlobServiceClient.from_connection_string(connection_string)
    container_client = service.get_container_client(container)

    blobs = container_client.list_blobs(name_starts_with=inputfolder + "/")

    providers = {}

    for blob in blobs:
        parts = blob.name.split("/")
        if len(parts) >= 2:
            provider = parts[1]
            providers.setdefault(provider, []).append(blob.name)

    return providers

4Ô∏è‚É£ documentintelligence_ocr.py
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

class OCRClient:
    def __init__(self, endpoint, key):
        self.client = DocumentAnalysisClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(key)
        )

    def read(self, file_bytes):
        poller = self.client.begin_analyze_document(
            "prebuilt-read", file_bytes
        )
        result = poller.result()

        pages = []
        for page in result.pages:
            pages.append(" ".join([l.content for l in page.lines]))

        return pages

5Ô∏è‚É£ Embedding.py
from openai import AzureOpenAI

class EmbeddingClient:
    def __init__(self, cfg, cost_tracker):
        self.client = AzureOpenAI(
            api_key=cfg["api_key"],
            azure_endpoint=cfg["endpoint"],
            api_version=cfg["api_version"]
        )
        self.deployment = cfg["deployment_name"]
        self.cost = cost_tracker

    def embed(self, text):
        response = self.client.embeddings.create(
            model=self.deployment,
            input=text
        )

        tokens = response.usage.total_tokens
        self.cost.add_embedding(tokens)

        return response.data[0].embedding

6Ô∏è‚É£ AIsearch.py
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import *
from azure.core.credentials import AzureKeyCredential

class AISearch:
    def __init__(self, endpoint, key, vector_dimensions):
        self.endpoint = endpoint
        self.credential = AzureKeyCredential(key)
        self.index_client = SearchIndexClient(endpoint, self.credential)
        self.vector_dimensions = vector_dimensions

    def ensure_index(self, index_name):
        existing = [i.name for i in self.index_client.list_indexes()]
        if index_name in existing:
            return

        fields = [
            SimpleField("id", SearchFieldDataType.String, key=True),
            SearchableField("content"),
            SimpleField("document", SearchFieldDataType.String),
            SearchField(
                name="embedding",
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                vector_search_dimensions=self.vector_dimensions,
                vector_search_configuration="vector-config"
            )
        ]

        vector_search = VectorSearch(
            algorithms=[HnswAlgorithmConfiguration(name="vector-config")]
        )

        index = SearchIndex(
            name=index_name,
            fields=fields,
            vector_search=vector_search
        )

        self.index_client.create_index(index)

    def upload(self, index_name, docs):
        client = SearchClient(self.endpoint, index_name, self.credential)
        client.upload_documents(docs)

    def search(self, index_name, vector, k=5):
        client = SearchClient(self.endpoint, index_name, self.credential)

        results = client.search(
            search_text="",
            vector_queries=[{
                "vector": vector,
                "fields": "embedding",
                "k": k
            }]
        )

        return list(results)

7Ô∏è‚É£ prompt.py
def field_extraction_prompt(field, context):
    return f"""
Extract the field '{field}' from the text below.

Rules:
- Use only provided text
- Do not guess
- If not found return null
- Confidence must be 0 to 1

Return JSON:
{{
  "value": null,
  "confidence": 0.0,
  "source_document": ""
}}

TEXT:
{context}
"""

8Ô∏è‚É£ rag.py
import json
from openai import AzureOpenAI
from prompt import field_extraction_prompt

class RAG:
    def __init__(self, cfg, cost_tracker):
        self.client = AzureOpenAI(
            api_key=cfg["api_key"],
            azure_endpoint=cfg["endpoint"],
            api_version=cfg["api_version"]
        )
        self.deployment = cfg["deployment_name"]
        self.cost = cost_tracker

    def extract(self, field, context):
        prompt = field_extraction_prompt(field, context)

        response = self.client.chat.completions.create(
            model=self.deployment,
            messages=[
                {"role": "system", "content": "Extract structured information."},
                {"role": "user", "content": prompt}
            ],
            temperature=0
        )

        tokens = response.usage.total_tokens
        self.cost.add_llm(tokens)

        return json.loads(response.choices[0].message.content)

9Ô∏è‚É£ cost_tracking.py
class CostTracker:
    def __init__(self):
        self.embedding_tokens = 0
        self.llm_tokens = 0
        self.ocr_pages = 0

    def add_embedding(self, tokens):
        self.embedding_tokens += tokens

    def add_llm(self, tokens):
        self.llm_tokens += tokens

    def add_ocr_pages(self, pages):
        self.ocr_pages += pages

    def summary(self):
        embedding_cost = (self.embedding_tokens / 1000) * 0.00013
        llm_cost = (self.llm_tokens / 1000) * 0.01
        ocr_cost = (self.ocr_pages / 1000) * 1.50
        total = embedding_cost + llm_cost + ocr_cost

        return f"""
RAG COST SUMMARY
========================
Embedding Tokens : {self.embedding_tokens}
LLM Tokens       : {self.llm_tokens}
OCR Pages        : {self.ocr_pages}

Embedding Cost   : ${embedding_cost:.4f}
LLM Cost         : ${llm_cost:.4f}
OCR Cost         : ${ocr_cost:.4f}
------------------------
TOTAL COST       : ${total:.4f}
"""

üîü main.py
import argparse, io, csv, json
from azure.storage.blob import BlobServiceClient
from helper import load_config, utc_timestamp, is_supported, normalize_index_name
from process_doc import list_providers
from documentintelligence_ocr import OCRClient
from Embedding import EmbeddingClient
from AIsearch import AISearch
from rag import RAG
from cost_tracking import CostTracker

parser = argparse.ArgumentParser()
parser.add_argument("--inputfolder", required=True)
parser.add_argument("--outputfolder", required=True)
parser.add_argument("--ocr", default="true")
args = parser.parse_args()

OCR_ENABLED = args.ocr.lower() == "true"

cfg = load_config()
timestamp = utc_timestamp()

blob_cfg = cfg["Azureblob"]
service = BlobServiceClient.from_connection_string(
    blob_cfg["connection_string"]
)

providers = list_providers(
    blob_cfg["connection_string"],
    blob_cfg["inputcontainer"],
    args.inputfolder
)

cost = CostTracker()

embedding_cfg = cfg["AzureOpenAI_Embedding"]
chat_cfg = cfg["AzureOpenAI_Chat"]

embedder = EmbeddingClient(embedding_cfg, cost)
rag = RAG(chat_cfg, cost)

search = AISearch(
    cfg["AzureAISearch"]["endpoint"],
    cfg["AzureAISearch"]["api_key"],
    embedding_cfg["vector_dimensions"]
)

ocr = OCRClient(
    cfg["DocumentIntelligence"]["endpoint"],
    cfg["DocumentIntelligence"]["api_key"]
)

for provider, files in providers.items():

    log_lines = []
    index_name = normalize_index_name(provider)
    search.ensure_index(index_name)

    chunks = []

    for path in files:
        filename = path.split("/")[-1]

        if not is_supported(filename):
            log_lines.append(f"Unsupported skipped: {filename}")
            continue

        data = service.get_blob_client(
            blob_cfg["inputcontainer"], path
        ).download_blob().readall()

        pages = ocr.read(data) if OCR_ENABLED else [data.decode(errors="ignore")]
        cost.add_ocr_pages(len(pages))

        for i, text in enumerate(pages):
            chunks.append({
                "id": f"{filename}_{i}",
                "content": text,
                "document": filename,
                "embedding": embedder.embed(text)
            })

    if chunks:
        search.upload(index_name, chunks)

    results = {}
    low_conf = False

    for field in cfg["required_fields"]:
        vec = embedder.embed(field)
        hits = search.search(index_name, vec)
        context = "\n".join([h["content"] for h in hits])
        res = rag.extract(field, context)

        if res["confidence"] < cfg["confidence_threshold"]:
            low_conf = True

        results[field] = res

    bucket = "Lowconfidence" if low_conf else "Highconfidence"

    # CSV
    csv_buf = io.StringIO()
    writer = csv.writer(csv_buf)
    header = ["id", "extraction_datetime"]
    row = [provider, timestamp]

    for f, v in results.items():
        header += [f, f"{f}_confidence", f"{f}_sourcedocument"]
        row += [v["value"], v["confidence"], v["source_document"]]

    writer.writerow(header)
    writer.writerow(row)

    service.get_blob_client(
        blob_cfg["outputcontainer"],
        f"{args.outputfolder}/{bucket}/processedcsvresult/{provider}.csv"
    ).upload_blob(csv_buf.getvalue(), overwrite=True)

    # JSON
    service.get_blob_client(
        blob_cfg["outputcontainer"],
        f"{args.outputfolder}/{bucket}/processedjsonresult/{provider}.json"
    ).upload_blob(json.dumps(results, indent=2), overwrite=True)

    # Log
    service.get_blob_client(
        blob_cfg["outputcontainer"],
        f"{args.outputfolder}/logs/{provider}_{timestamp}.log"
    ).upload_blob("\n".join(log_lines), overwrite=True)

# COST FILE
service.get_blob_client(
    blob_cfg["outputcontainer"],
    f"{args.outputfolder}/cost/run_{timestamp}.txt"
).upload_blob(cost.summary(), overwrite=True)
