import pandas as pd
import re
from typing import Dict, List, Tuple

class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str):
        """
        Initialize the schema generator with Excel file path
        """
        self.excel_file_path = excel_file_path
        self.datatype_mapping = {
            # RDMOF datatype mappings
            'VARCHAR2(255 BYTE)': 'VARCHAR(255)',
            'VARCHAR2(100 BYTE)': 'VARCHAR(100)',
            'TIMESTAMP(6)': 'TIMESTAMP',
            'NUMBER(12,6)': 'DECIMAL(12,6)',
            # Add more mappings as needed
            'VARCHAR2': 'VARCHAR',
            'NUMBER': 'DECIMAL',
            'DATE': 'DATE',
            'CHAR': 'CHAR',
            'CLOB': 'STRING',
            'BLOB': 'BINARY'
        }
    
    def load_excel_data(self) -> pd.DataFrame:
        """
        Load data from Excel file, specifically from BROKER_GROUP_RELATION sheet
        """
        try:
            # First, check available sheets
            excel_file = pd.ExcelFile(self.excel_file_path)
            available_sheets = excel_file.sheet_names
            print(f"Available sheets: {available_sheets}")
            
            # Look for BROKER_GROUP_RELATION sheet (case insensitive)
            target_sheet = None
            for sheet in available_sheets:
                if 'BROKER_GROUP_RELATION' in sheet.upper() or 'BROKER GROUP RELATION' in sheet.upper():
                    target_sheet = sheet
                    break
            
            if target_sheet:
                print(f"Loading sheet: {target_sheet}")
                df = pd.read_excel(self.excel_file_path, sheet_name=target_sheet)
            else:
                print("BROKER_GROUP_RELATION sheet not found. Available sheets:", available_sheets)
                # Try first sheet as fallback
                print(f"Using first sheet: {available_sheets[0]}")
                df = pd.read_excel(self.excel_file_path, sheet_name=available_sheets[0])
            
            print(f"Successfully loaded Excel file with {len(df)} rows and {len(df.columns)} columns")
            return df
        except Exception as e:
            print(f"Error loading Excel file: {e}")
            return None
    
    def filter_broker_group_relation(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Since we're already loading the BROKER_GROUP_RELATION sheet, return all data
        """
        print(f"Using all data from BROKER_GROUP_RELATION sheet: {len(df)} rows")
        return df
    
    def map_datatype(self, original_datatype: str) -> str:
        """
        Map RDMOF datatypes to Databricks datatypes
        """
        if pd.isna(original_datatype):
            return 'STRING'
        
        datatype_str = str(original_datatype).upper().strip()
        
        # Direct mapping
        if datatype_str in self.datatype_mapping:
            return self.datatype_mapping[datatype_str]
        
        # Pattern-based mapping for parameterized types
        for pattern, replacement in self.datatype_mapping.items():
            if pattern in datatype_str:
                if 'VARCHAR2' in pattern and '(' in datatype_str:
                    # Extract size for VARCHAR2
                    size_match = re.search(r'\((\d+)', datatype_str)
                    if size_match:
                        size = size_match.group(1)
                        return f'VARCHAR({size})'
                elif 'NUMBER' in pattern and '(' in datatype_str:
                    # Extract precision and scale for NUMBER
                    number_match = re.search(r'\((\d+),(\d+)\)', datatype_str)
                    if number_match:
                        precision, scale = number_match.groups()
                        return f'DECIMAL({precision},{scale})'
                    else:
                        precision_match = re.search(r'\((\d+)\)', datatype_str)
                        if precision_match:
                            precision = precision_match.group(1)
                            return f'DECIMAL({precision})'
                return replacement
        
        # Default fallback
        return 'STRING'
    
    def process_categories(self, df: pd.DataFrame) -> Dict[str, List[Tuple[str, str, str, str]]]:
        """
        Process the three categories and extract schema information
        """
        categories = {
            'RDMOF': [],
            'EDL': [],
            'Original SSR': []
        }
        
        print("\nAnalyzing Excel structure...")
        print(f"Columns found: {list(df.columns)}")
        print(f"First few rows:")
        print(df.head())
        
        # More flexible column identification
        column_mapping = {}
        
        # Look for category-related columns
        category_patterns = ['category', 'cat', 'type', 'source', 'system']
        schema_patterns = ['schema', 'database', 'db', 'namespace']
        table_patterns = ['table', 'tbl', 'entity', 'object']
        column_patterns = ['column', 'col', 'field', 'attribute', 'element']
        datatype_patterns = ['datatype', 'data_type', 'type', 'format', 'dtype']
        
        for col in df.columns:
            col_lower = col.lower().replace('_', ' ').replace('-', ' ')
            
            # Check for category column
            if any(pattern in col_lower for pattern in category_patterns) and 'category' not in column_mapping:
                column_mapping['category'] = col
            
            # Check for schema column
            elif any(pattern in col_lower for pattern in schema_patterns) and 'schema' not in column_mapping:
                column_mapping['schema'] = col
            
            # Check for table name column
            elif any(pattern in col_lower for pattern in table_patterns) and 'name' in col_lower and 'column' not in col_lower:
                column_mapping['table_name'] = col
            
            # Check for column name
            elif any(pattern in col_lower for pattern in column_patterns) and 'name' in col_lower:
                column_mapping['column_name'] = col
            
            # Check for datatype
            elif any(pattern in col_lower for pattern in datatype_patterns) and 'datatype' not in column_mapping:
                column_mapping['datatype'] = col
        
        print(f"\nIdentified column mapping: {column_mapping}")
        
        # If we can't find the standard columns, let's look for category-specific columns
        if not column_mapping:
            print("\nNo standard columns found. Looking for category-specific columns...")
            
            # Look for columns that might contain category information
            for col in df.columns:
                col_str = str(col)
                if any(cat in col_str.upper() for cat in ['RDMOF', 'EDL', 'SSR']):
                    print(f"Found potential category column: {col}")
                    
                    # Try to determine what type of information this column contains
                    sample_values = df[col].dropna().head(10).astype(str)
                    print(f"Sample values from {col}: {sample_values.tolist()}")
        
        # If still no mapping found, try to use column positions or ask user
        if not column_mapping:
            print("\nCould not automatically identify columns.")
            print("Available columns:")
            for i, col in enumerate(df.columns):
                print(f"{i}: {col}")
            
            # For now, let's try to work with what we have
            # Look for any columns that might contain the three categories
            for col in df.columns:
                unique_values = df[col].dropna().astype(str).str.upper().unique()
                if any('RDMOF' in val or 'EDL' in val or 'SSR' in val for val in unique_values):
                    column_mapping['category'] = col
                    print(f"Found category column based on values: {col}")
                    break
        
        # Process the data if we have at least a category column
        if 'category' in column_mapping or any('RDMOF' in str(col).upper() or 'EDL' in str(col).upper() or 'SSR' in str(col).upper() for col in df.columns):
            
            # If we have category-specific columns (like RDMOF_Schema, EDL_Schema, etc.)
            category_specific_columns = {}
            for col in df.columns:
                col_upper = str(col).upper()
                for cat in ['RDMOF', 'EDL', 'ORIGINAL SSR', 'SSR']:
                    if cat in col_upper:
                        if cat not in category_specific_columns:
                            category_specific_columns[cat] = {}
                        
                        if 'SCHEMA' in col_upper:
                            category_specific_columns[cat]['schema'] = col
                        elif 'TABLE' in col_upper and 'NAME' in col_upper:
                            category_specific_columns[cat]['table_name'] = col
                        elif 'COLUMN' in col_upper and 'NAME' in col_upper:
                            category_specific_columns[cat]['column_name'] = col
                        elif 'DATATYPE' in col_upper or 'DATA_TYPE' in col_upper:
                            category_specific_columns[cat]['datatype'] = col
            
            print(f"Category-specific columns found: {category_specific_columns}")
            
            # Process each row
            for _, row in df.iterrows():
                # Method 1: Use category-specific columns
                if category_specific_columns:
                    for cat_key, col_info in category_specific_columns.items():
                        category = 'RDMOF' if cat_key == 'RDMOF' else 'EDL' if cat_key == 'EDL' else 'Original SSR'
                        
                        if col_info:  # If we have column info for this category
                            schema = row.get(col_info.get('schema'), 'default_schema')
                            table_name = row.get(col_info.get('table_name'), 'broker_group_relation')
                            column_name = row.get(col_info.get('column_name'), f'unknown_column_{len(categories[category])}')
                            datatype = row.get(col_info.get('datatype'), 'STRING')
                            
                            # Skip if essential data is missing
                            if pd.isna(column_name) or str(column_name).strip() == '':
                                continue
                            
                            # Map datatype for RDMOF category
                            if category == 'RDMOF':
                                mapped_datatype = self.map_datatype(datatype)
                            else:
                                mapped_datatype = str(datatype) if not pd.isna(datatype) else 'STRING'
                            
                            categories[category].append((
                                str(schema) if not pd.isna(schema) else 'default_schema',
                                str(table_name) if not pd.isna(table_name) else 'broker_group_relation',
                                str(column_name).strip(),
                                mapped_datatype
                            ))
                
                # Method 2: Use single category column
                elif 'category' in column_mapping:
                    cat_value = str(row[column_mapping['category']]).upper()
                    category = None
                    
                    if 'RDMOF' in cat_value:
                        category = 'RDMOF'
                    elif 'EDL' in cat_value:
                        category = 'EDL'
                    elif 'SSR' in cat_value or 'ORIGINAL SSR' in cat_value:
                        category = 'Original SSR'
                    
                    if category:
                        schema = row.get(column_mapping.get('schema', ''), 'default_schema')
                        table_name = row.get(column_mapping.get('table_name', ''), 'broker_group_relation')
                        column_name = row.get(column_mapping.get('column_name', ''), f'unknown_column_{len(categories[category])}')
                        datatype = row.get(column_mapping.get('datatype', ''), 'STRING')
                        
                        # Skip if essential data is missing
                        if pd.isna(column_name) or str(column_name).strip() == '':
                            continue
                        
                        # Map datatype for RDMOF category
                        if category == 'RDMOF':
                            mapped_datatype = self.map_datatype(datatype)
                        else:
                            mapped_datatype = str(datatype) if not pd.isna(datatype) else 'STRING'
                        
                        categories[category].append((
                            str(schema) if not pd.isna(schema) else 'default_schema',
                            str(table_name) if not pd.isna(table_name) else 'broker_group_relation',
                            str(column_name).strip(),
                            mapped_datatype
                        ))
        
        return categories
    
    def generate_sql_schema(self, categories: Dict[str, List[Tuple[str, str, str, str]]]) -> str:
        """
        Generate SQL CREATE TABLE statements for each category
        """
        sql_statements = []
        
        for category, columns in categories.items():
            if not columns:
                continue
            
            # Use the first row to get schema and table name
            schema_name = columns[0][0]
            table_name = columns[0][1]
            
            # Create table name with category suffix
            full_table_name = f"{schema_name}.{table_name}_{category.replace(' ', '_').lower()}"
            
            sql = f"-- {category} Schema\n"
            sql += f"CREATE TABLE IF NOT EXISTS {full_table_name} (\n"
            
            column_definitions = []
            for _, _, column_name, datatype in columns:
                column_definitions.append(f"    {column_name} {datatype}")
            
            sql += ",\n".join(column_definitions)
            sql += "\n);\n\n"
            
            sql_statements.append(sql)
        
        return "\n".join(sql_statements)
    
    def generate_unified_schema(self, categories: Dict[str, List[Tuple[str, str, str, str]]]) -> str:
        """
        Generate a unified schema with all categories in one table
        """
        all_columns = []
        
        for category, columns in categories.items():
            for _, _, column_name, datatype in columns:
                # Add category prefix to avoid column name conflicts
                prefixed_column_name = f"{category.replace(' ', '_').lower()}_{column_name}"
                all_columns.append((prefixed_column_name, datatype))
        
        if not all_columns:
            return "-- No columns found to generate schema"
        
        sql = "-- Unified BROKER GROUP RELATION Schema\n"
        sql += "CREATE TABLE IF NOT EXISTS broker_group_relation_unified (\n"
        
        column_definitions = []
        for column_name, datatype in all_columns:
            column_definitions.append(f"    {column_name} {datatype}")
        
        sql += ",\n".join(column_definitions)
        sql += "\n);\n"
        
        return sql
    
    def analyze_excel_structure(self, df: pd.DataFrame):
        """
        Analyze and print Excel structure to help with debugging
        """
        print("\n" + "="*60)
        print("EXCEL STRUCTURE ANALYSIS")
        print("="*60)
        
        print(f"Total rows: {len(df)}")
        print(f"Total columns: {len(df.columns)}")
        
        print(f"\nColumn names:")
        for i, col in enumerate(df.columns):
            print(f"  {i+1:2d}. {col}")
        
        print(f"\nSample data (first 3 rows):")
        for i, (idx, row) in enumerate(df.head(3).iterrows()):
            print(f"\nRow {i+1}:")
            for col in df.columns:
                value = row[col]
                if pd.notna(value):
                    print(f"  {col}: {value}")
        
        print(f"\nLooking for category indicators...")
        category_keywords = ['RDMOF', 'EDL', 'SSR', 'ORIGINAL SSR']
        
        for col in df.columns:
            unique_vals = df[col].dropna().astype(str).unique()[:10]  # First 10 unique values
            has_category = any(any(keyword in str(val).upper() for keyword in category_keywords) for val in unique_vals)
            
            if has_category:
                print(f"  Column '{col}' contains category keywords")
                print(f"    Sample values: {list(unique_vals)}")
        
        print("="*60)

    def run(self, generate_unified: bool = True, debug: bool = True) -> str:
        """
        Main method to run the schema generation process
        """
        print("Starting Databricks Schema Generation...")
        
        # Load Excel data
        df = self.load_excel_data()
        if df is None:
            return "Error: Could not load Excel file"
        
        print("Excel columns:", list(df.columns))
        
        if debug:
            self.analyze_excel_structure(df)
        
        # Filter for BROKER GROUP RELATION table
        filtered_df = self.filter_broker_group_relation(df)
        print(f"Filtered data: {len(filtered_df)} rows")
        
        # Process categories
        categories = self.process_categories(filtered_df)
        
        # Print category summary
        for cat, cols in categories.items():
            print(f"{cat}: {len(cols)} columns")
        
        # Generate SQL schemas
        separate_schemas = self.generate_sql_schema(categories)
        
        result = "-- DATABRICKS SCHEMA GENERATION RESULTS\n"
        result += "-- =====================================\n\n"
        result += separate_schemas
        
        if generate_unified:
            unified_schema = self.generate_unified_schema(categories)
            result += "\n" + unified_schema
        
        return result

# Usage Example
def main():
    # Replace with your Excel file path
    excel_file_path = "your_excel_file.xlsx"  # Update this path
    
    try:
        generator = DatabricksSchemaGenerator(excel_file_path)
        schema_sql = generator.run(generate_unified=True, debug=True)
        
        print("Generated SQL Schema:")
        print("=" * 50)
        print(schema_sql)
        
        # Save to file
        with open("databricks_schema.sql", "w") as f:
            f.write(schema_sql)
        
        print("\nSchema saved to 'databricks_schema.sql'")
        
    except FileNotFoundError:
        print(f"Error: Excel file '{excel_file_path}' not found.")
        print("Please update the excel_file_path variable with the correct path to your Excel file.")
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()
