import os
import json
import pandas as pd
import re
from typing import List, Dict, Tuple, Optional
from collections import defaultdict


class DatabricksSchemaGenerator:
    def __init__(self,
                 excel_file_path: str,
                 output_base_folder: str = "generated_schemas",
                 sheet_name: Optional[str] = None,
                 categories: Optional[Dict[str, Dict[str, str]]] = None,
                 generate_drop_statements: bool = False):
        self.excel_file_path = excel_file_path
        self.output_base_folder = output_base_folder
        self.sheet_name = sheet_name
        self.categories_config = categories or {}
        self.generate_drop_statements = generate_drop_statements

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype) or not datatype or str(datatype).strip() == '':
            return 'STRING'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'STRING'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'STRING'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'STRING'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'STRING'

    def load_excel_data(self) -> pd.DataFrame:
        try:
            df = pd.read_excel(self.excel_file_path, sheet_name=self.sheet_name)
            df.columns = df.columns.str.strip()

            for cfg in self.categories_config.values():
                for col_key in ['schema_col', 'table_col']:
                    col = cfg.get(col_key)
                    if col and col in df.columns:
                        df[col] = df[col].ffill()

            for cfg in self.categories_config.values():
                schema_col = cfg.get("schema_col")
                if schema_col and schema_col in df.columns:
                    df = df[
                        (df[schema_col].notna()) &
                        (df[schema_col].astype(str).str.upper() != "NA") &
                        (df[schema_col].astype(str).str.strip() != "")
                    ]

            return df

        except Exception as e:
            print(f"‚ùå Error loading Excel file: {e}")
            return pd.DataFrame()

    def extract_tables(self, df: pd.DataFrame) -> Dict[str, Dict[Tuple[str, str], List[Tuple[str, str, str]]]]:
        result = {}

        for cat, cfg in self.categories_config.items():
            schema_col = cfg.get('schema_col')
            table_col = cfg.get('table_col')
            column_col = cfg.get('column_col')
            datatype_col = cfg.get('datatype_col', '')
            column_comment_col = cfg.get('column_comment_col', '')
            table_comment_col = cfg.get('table_comment_col', '')

            if not (schema_col and table_col and column_col):
                continue

            tables = defaultdict(list)
            table_comments = {}

            filtered_df = df[
                df[schema_col].notna() &
                df[table_col].notna() &
                df[column_col].notna()
            ]

            seen_columns = defaultdict(set)

            for _, row in filtered_df.iterrows():
                schema = str(row[schema_col]).strip()
                table = str(row[table_col]).strip()
                column = str(row[column_col]).strip()
                dtype = str(row.get(datatype_col, '')).strip() if datatype_col in df.columns else ''
                comment = str(row.get(column_comment_col, '')).strip() if column_comment_col in df.columns else ''
                table_comment = str(row.get(table_comment_col, '')).strip() if table_comment_col in df.columns else ''

                key = (schema, table)

                if column.lower() not in seen_columns[key]:
                    tables[key].append((column, self.map_datatype(dtype), comment))
                    seen_columns[key].add(column.lower())

                if table_comment and key not in table_comments:
                    table_comments[key] = table_comment

            result[cat] = {
                'tables': tables,
                'table_comments': table_comments
            }

        return result

    def generate_schema_sql(self, schema: str, table: str, columns: List[Tuple[str, str, str]],
                            table_comment: str, category: str) -> str:
        sql = f"-- {category} - {table} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.{schema}.{table} (\n"
        sql += ",\n".join(
            [f"  `{col}` {dtype}" + (f" COMMENT '{comment}'" if comment else '') for col, dtype, comment in columns]
        )
        sql += "\n)"
        if table_comment:
            sql += f"\nCOMMENT '{table_comment}'"
        sql += ";\n"
        return sql

    def generate_drop_sql(self, schema: str, table: str, category: str) -> str:
        return f"-- {category} - Drop {table} Table\nDROP TABLE IF EXISTS external_catalog.{schema}.{table};\n"

    def create_folder_structure(self):
        for cat in self.categories_config.keys():
            os.makedirs(os.path.join(self.output_base_folder, cat, "create"), exist_ok=True)
            os.makedirs(os.path.join(self.output_base_folder, cat, "drop"), exist_ok=True)

        os.makedirs(os.path.join(self.output_base_folder, "all_create_consolidated"), exist_ok=True)
        os.makedirs(os.path.join(self.output_base_folder, "all_drop_consolidated"), exist_ok=True)

    def run(self):
        df = self.load_excel_data()
        if df.empty:
            print("‚ùå No data to process.")
            return

        self.create_folder_structure()

        all_create_sql = []
        all_drop_sql = []
        total_tables = 0

        all_extracted = self.extract_tables(df)

        for cat, data in all_extracted.items():
            tables = data['tables']
            table_comments = data['table_comments']

            create_sqls = []
            drop_sqls = []
            table_count = 0

            for (schema, table), columns in tables.items():
                comment = table_comments.get((schema, table), '')

                # CREATE
                create_sql = self.generate_schema_sql(schema, table, columns, comment, cat)
                create_sqls.append(create_sql)
                all_create_sql.append(create_sql)

                create_file = os.path.join(self.output_base_folder, cat, "create", f"{schema}_{table}.sql")
                with open(create_file, 'w') as f:
                    f.write(create_sql)

                # DROP
                drop_sql = self.generate_drop_sql(schema, table, cat)
                drop_sqls.append(drop_sql)
                all_drop_sql.append(drop_sql)

                drop_file = os.path.join(self.output_base_folder, cat, "drop", f"{schema}_{table}.sql")
                with open(drop_file, 'w') as f:
                    f.write(drop_sql)

                table_count += 1

            consolidated_create = os.path.join(self.output_base_folder, cat, "create", f"{cat.lower()}_consolidated_create.sql")
            with open(consolidated_create, 'w') as f:
                f.write("\n\n".join(create_sqls))

            consolidated_drop = os.path.join(self.output_base_folder, cat, "drop", f"{cat.lower()}_consolidated_drop.sql")
            with open(consolidated_drop, 'w') as f:
                f.write("\n\n".join(drop_sqls))

            print(f"‚úÖ {cat}: {table_count} tables processed.")
            total_tables += table_count

        # Final consolidated
        all_create_file = os.path.join(self.output_base_folder, "all_create_consolidated", "all_categories_create.sql")
        all_drop_file = os.path.join(self.output_base_folder, "all_drop_consolidated", "all_categories_drop.sql")

        with open(all_create_file, 'w') as f:
            f.write("\n\n".join(all_create_sql))

        with open(all_drop_file, 'w') as f:
            f.write("\n\n".join(all_drop_sql))

        print(f"\nüì¶ Total tables processed: {total_tables}")
        print(f"üìÅ Output folder: {self.output_base_folder}")

----
import json
from helper import DatabricksSchemaGenerator

def main():
    try:
        with open("schema_config.json", 'r') as f:
            config = json.load(f)

        generator = DatabricksSchemaGenerator(
            excel_file_path=config["excel_file_path"],
            output_base_folder=config.get("output_base_folder", "generated_schemas"),
            sheet_name=config.get("sheet_name"),
            categories=config["categories"],
            generate_drop_statements=config.get("generate_drop_statements", False)
        )
        generator.run()

    except FileNotFoundError:
        print("‚ùå schema_config.json not found.")
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()

---
{
  "excel_file_path": "your_excel_file.xlsx",
  "sheet_name": "Sheet1",
  "output_base_folder": "generated_schemas",
  "generate_drop_statements": true,
  "categories": {
    "RDMOF": {
      "schema_col": "RDMOF - Schema",
      "table_col": "RDMOF - Physical Table Name",
      "column_col": "RDMOF - Physical Column Name",
      "datatype_col": "RDMOF - Data Type",
      "column_comment_col": "RDMOF - Column Definition",
      "table_comment_col": "RDMOF - Table Definition"
    },
    "EDL": {
      "schema_col": "EDL- Schema",
      "table_col": "EDL - Physical Table Name",
      "column_col": "EDL - Physical Column Name",
      "datatype_col": "EDL - Data Type",
      "column_comment_col": "EDL - Column Definition",
      "table_comment_col": "EDL - Table Definition"
    },
    "Original_SSR": {
      "schema_col": "Original SSR - Schema",
      "table_col": "Original SSR - Physical Table Name",
      "column_col": "Original SSR - Physical Column Name",
      "datatype_col": "Original SSR - Data Type",
      "column_comment_col": "Original SSR - Column Definition",
      "table_comment_col": "Original SSR - Table Definition"
    }
  }
}
