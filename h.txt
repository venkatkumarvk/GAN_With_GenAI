#1. First, add token tracking to the AzureOpenAIClient class in llm.py:
class AzureOpenAIClient:
    def __init__(self, config):
        self.api_key = config["azure_openai"]["api_key"]
        self.api_version = config["azure_openai"]["api_version"]
        self.endpoint = config["azure_openai"]["azure_endpoint"]
        self.deployment_name = config["azure_openai"]["deployment_name"]
        self.batch_size = config["processing"]["batch_size"]
        self.timeout = config["processing"]["timeout_seconds"]
        
        # Add token cost parameters
        self.input_token_cost_per_million = 2.50
        self.output_token_cost_per_million = 10.0
        
        # Add token counters
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        
        self.client = AzureOpenAI(
            api_key=self.api_key,
            api_version=self.api_version,
            azure_endpoint=self.endpoint
        )
    
    def calculate_cost(self):
        """Calculate the cost based on token usage."""
        input_cost = (self.total_input_tokens / 1000000) * self.input_token_cost_per_million
        output_cost = (self.total_output_tokens / 1000000) * self.output_token_cost_per_million
        total_cost = input_cost + output_cost
        
        return {
            "input_tokens": self.total_input_tokens,
            "output_tokens": self.total_output_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost
        }
    
    def track_token_usage(self, response_data):
        """
        Track token usage from API response data.
        This works with both batch and general responses.
        """
        try:
            if isinstance(response_data, dict):
                if "usage" in response_data:
                    # Direct OpenAI API response
                    usage = response_data["usage"]
                    self.total_input_tokens += usage.get("prompt_tokens", 0)
                    self.total_output_tokens += usage.get("completion_tokens", 0)
                elif "response" in response_data and "body" in response_data["response"]:
                    # Batch API response format
                    body = response_data["response"]["body"]
                    if isinstance(body, str):
                        body = json.loads(body)
                    
                    if "usage" in body:
                        usage = body["usage"]
                        self.total_input_tokens += usage.get("prompt_tokens", 0)
                        self.total_output_tokens += usage.get("completion_tokens", 0)
        except Exception as e:
            print(f"Error tracking tokens: {str(e)}")

#2. Update the process_batch method in AzureOpenAIClient to track token usage:
def process_batch(self, image_base64_strings, prompts):
    """
    Process images in a batch using the Azure OpenAI batch API.
    Tracks token usage.
    """
    # Existing implementation...
    
    # When retrieving results:
    print(f"Batch completed. Retrieving results...")
    file_response = self.client.files.content(output_file_id)
    raw_responses = file_response.text.strip().split('\n')
    
    # Track token usage from each response
    for raw_response in raw_responses:
        try:
            response_data = json.loads(raw_response)
            self.track_token_usage(response_data)
        except Exception as e:
            print(f"Error tracking tokens: {str(e)}")
    
    return raw_responses

#3. Update the process_general method in AzureOpenAIClient to track token usage:
def process_general(self, image_base64_strings, prompts):
    """
    Process images using the general (non-batch) API.
    Tracks token usage.
    """
    results = []
    
    for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
        try:
            print(f"Processing image {i+1}/{len(image_base64_strings)}")
            
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_img}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            # Track token usage from the response
            if hasattr(response, 'usage'):
                self.total_input_tokens += response.usage.prompt_tokens
                self.total_output_tokens += response.usage.completion_tokens
            
            if hasattr(response, 'choices') and len(response.choices) > 0:
                content = response.choices[0].message.content
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "response": {
                        "body": {
                            "choices": [
                                {
                                    "message": {
                                        "content": content
                                    }
                                }
                            ],
                            "usage": {
                                "prompt_tokens": response.usage.prompt_tokens if hasattr(response, 'usage') else 0,
                                "completion_tokens": response.usage.completion_tokens if hasattr(response, 'usage') else 0
                            }
                        }
                    }
                }))
            else:
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "error": "No response content"
                }))
        
        except Exception as e:
            print(f"Error processing image {i+1}: {str(e)}")
            results.append(json.dumps({
                "custom_id": f"request-{i+1}",
                "error": str(e)
            }))
    
    return results

#pdf process
def process_azure_pdf_files(config, api_type, azure_folder):
    """Process PDF files from Azure Blob Storage."""
    # Existing implementation...
    
    # After all processing is complete, display token usage and cost
    cost_info = ai_client.calculate_cost()
    
    print("\n===== GPT-4o Token Usage and Cost =====")
    print(f"Total Input Tokens: {cost_info['input_tokens']:,}")
    print(f"Total Output Tokens: {cost_info['output_tokens']:,}")
    print(f"Input Cost: ${cost_info['input_cost']:.4f}")
    print(f"Output Cost: ${cost_info['output_cost']:.4f}")
    print(f"Total Cost: ${cost_info['total_cost']:.4f}")
    print("=======================================")
    
    print("Processing complete!")
