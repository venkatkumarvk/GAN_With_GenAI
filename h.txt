config

{
  "azure_openai": {
    "api_key": "YOUR_AZURE_OPENAI_KEY",
    "endpoint": "https://YOUR-RESOURCE-NAME.openai.azure.com/",
    "deployment_name": "gpt-4o",
    "api_version": "2024-05-01-preview"
  },
  "categories": {
    "cms1500": ["cadwell", "rhymlink"],
    "invoice": ["tesla", "amazon"],
    "scheduling": ["email", "iomrequest"]
  },
  "paths": {
    "reference_dir": "reference",
    "input_dir": "input_docs",
    "output_dir": "output"
  }
}

----
help

import os
import json
import hashlib
import logging
from openai import AzureOpenAI

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='w'
)

def load_config(config_path="config.json"):
    """
    Load configuration from JSON file with error handling
    """
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        logging.error(f"Config file not found: {config_path}")
        raise
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON in config file: {config_path}")
        raise

def compute_reference_hash(ref_dir):
    """
    Compute a hash of reference directory contents
    """
    hasher = hashlib.sha256()
    
    for root, _, files in os.walk(ref_dir):
        for f in sorted(files):
            path = os.path.join(root, f)
            
            # Update hash with file path and modification time
            hasher.update(path.encode())
            hasher.update(str(os.path.getmtime(path)).encode())
            
            # Include file contents for precise tracking
            try:
                with open(path, 'rb') as file:
                    hasher.update(file.read())
            except Exception as e:
                logging.warning(f"Could not read file {path} for hashing: {e}")
    
    return hasher.hexdigest()

def fine_tune_if_new_reference(cfg):
    """
    Check if reference data has changed
    """
    ref_dir = cfg["paths"]["reference_dir"]
    hash_file = os.path.join(ref_dir, ".reference_hash")
    
    try:
        # Compute current reference hash
        hash_now = compute_reference_hash(ref_dir)
        
        # Check if hash file exists
        if os.path.exists(hash_file):
            with open(hash_file, 'r') as f:
                last_hash = f.read().strip()
        else:
            last_hash = ''
        
        # Compare hashes
        if hash_now != last_hash:
            logging.info("ðŸš€ New reference data detected")
            
            # Save new hash
            with open(hash_file, 'w') as f:
                f.write(hash_now)
            
            return True
        
        return False
    
    except Exception as e:
        logging.error(f"Reference check error: {e}")
        return False

def get_azure_client(cfg):
    """
    Initialize Azure OpenAI client
    """
    try:
        client = AzureOpenAI(
            api_key=cfg["azure_openai"]["api_key"],
            api_version=cfg["azure_openai"]["api_version"],
            azure_endpoint=cfg["azure_openai"]["endpoint"]
        )
        return client, cfg["azure_openai"]["deployment_name"]
    except Exception as e:
        logging.error(f"Azure client initialization error: {e}")
        raise

def prepare_directories(cfg):
    """
    Prepare necessary directories for processing
    """
    # Ensure input and output directories exist
    directories = [
        cfg['paths']['input_dir'],
        cfg['paths']['output_dir'],
        os.path.join(cfg['paths']['output_dir'], 'source'),
        os.path.join(cfg['paths']['output_dir'], 'classified'),
        os.path.join(cfg['paths']['output_dir'], 'unclassified')
    ]
    
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)

---Â¨
import os
import io
import json
import base64
import shutil
import logging
import fitz  # PyMuPDF
from PIL import Image

from helper import (
    load_config, 
    fine_tune_if_new_reference, 
    get_azure_client, 
    prepare_directories
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='w'
)

def extract_first_page_image(file_path):
    """
    Extract first page image from various document types
    """
    try:
        # PDF handling
        if file_path.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            page = doc.load_page(0)
            pix = page.get_pixmap()
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            doc.close()
        
        # Image handling
        elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):
            img = Image.open(file_path)
        
        else:
            raise ValueError(f"Unsupported file type: {file_path}")
        
        # Resize image to standard size
        img = img.resize((800, 600), Image.LANCZOS)
        
        # Convert to bytes
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        return buf.getvalue()
    
    except Exception as e:
        logging.error(f"Error extracting image from {file_path}: {e}")
        # Fallback blank image
        img = Image.new('RGB', (800, 600), color='white')
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        return buf.getvalue()

def classify_document(document_image, client, deployment_name, cfg):
    """
    GPT-4o based document classification
    """
    # Convert image to base64
    base64_image = base64.b64encode(document_image).decode('utf-8')
    
    # Prepare reference categories for prompt
    categories_str = ", ".join([
        f"{main_cat}: {', '.join(subcats)}" 
        for main_cat, subcats in cfg['categories'].items()
    ])

    # Detailed classification prompt
    prompt = f"""
    You are an advanced document classifier. 
    Carefully analyze the document and classify it into the most appropriate category.

    Available Categories:
    {categories_str}

    Provide your classification in the following JSON format:
    {{
        "main_category": "<main_category>",
        "subcategory": "<subcategory>",
        "confidence_score": <confidence_score_between_0_and_1>,
        "reasoning": "<brief explanation of classification>"
    }}

    If you are unsure or cannot confidently classify the document, 
    return "unknown" for both category and subcategory with a low confidence score.
    """

    try:
        response = client.chat.completions.create(
            model=deployment_name,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": [
                    {
                        "type": "image",
                        "image_base64": base64_image
                    }
                ]}
            ]
        )
        
        # Parse the response
        result = json.loads(response.choices[0].message.content)
        
        # Extract classification details
        main_category = result.get('main_category', 'unknown')
        subcategory = result.get('subcategory', 'unknown')
        confidence_score = result.get('confidence_score', 0)
        reasoning = result.get('reasoning', '')
        
        # Validate categories
        if main_category not in cfg['categories']:
            main_category = 'unknown'
        elif subcategory not in cfg['categories'].get(main_category, []):
            subcategory = 'unknown'
        
        # Log detailed classification
        logging.info(f"Classification Result: {json.dumps({
            'main_category': main_category,
            'subcategory': subcategory,
            'confidence_score': confidence_score,
            'reasoning': reasoning
        }, indent=2)}")
        
        return main_category, subcategory, confidence_score, reasoning
    
    except Exception as e:
        logging.error(f"Classification error: {e}")
        return "unknown", "unknown", 0, "Error in classification"

def process_documents():
    """
    Automated document classification
    """
    # Load configuration
    cfg = load_config()
    
    # Prepare directories
    prepare_directories(cfg)
    
    # Fine-tuning trigger
    fine_tune_if_new_reference(cfg)
    
    # Initialize Azure client
    client, deployment = get_azure_client(cfg)
    
    # Set up directories
    input_dir = cfg['paths']['input_dir']
    output_dir = cfg['paths']['output_dir']
    
    # Process each document
    for fname in os.listdir(input_dir):
        fpath = os.path.join(input_dir, fname)
        
        # Skip directories
        if not os.path.isfile(fpath):
            continue
        
        # Extract first page image
        document_image = extract_first_page_image(fpath)
        
        # Classify document
        main_cat, sub_cat, confidence_score, reasoning = classify_document(
            document_image, 
            client, 
            deployment,
            cfg
        )
        
        # Determine destination
        if main_cat == "unknown":
            dest_dir = os.path.join(output_dir, "unclassified")
        else:
            dest_dir = os.path.join(output_dir, "classified", main_cat, sub_cat)
        
        # Create destination directory
        os.makedirs(dest_dir, exist_ok=True)
        
        # Copy document
        dest_path = os.path.join(dest_dir, fname)
        shutil.copy(fpath, dest_path)
        
        # Create metadata file
        metadata_path = os.path.join(dest_dir, f"{os.path.splitext(fname)[0]}_metadata.json")
        with open(metadata_path, 'w') as metadata_file:
            json.dump({
                "filename": fname,
                "main_category": main_cat,
                "subcategory": sub_cat,
                "confidence_score": confidence_score,
                "reasoning": reasoning
            }, metadata_file, indent=2)
        
        # Log classification
        logging.info(f"Classified {fname}: {main_cat} / {sub_cat}")

    print("\nâœ… Document Processing Complete")

if __name__ == "__main__":
    process_documents()
