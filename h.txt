# insert_helper.py
import os
import glob
import pandas as pd
from collections import defaultdict

class InsertSQLGenerator:
    def __init__(self, config: dict, input_folder: str, output_folder: str = "generated_inserts"):
        self.input_folder = input_folder
        self.output_folder = output_folder
        self.categories = config.get("categories", {})
        self.category_keys = list(self.categories.keys())  # preserve order

    def get_excel_files(self):
        patterns = ["*.xls", "*.xlsx", "*.xlsm"]
        files = []
        for p in patterns:
            files.extend(glob.glob(os.path.join(self.input_folder, p)))
        return files

    def load_all_sheets(self, file_path):
        try:
            xls = pd.ExcelFile(file_path)
            sheets = []
            for sheet_name in xls.sheet_names:
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    df.columns = df.columns.str.strip()
                    # forward fill schema/table/column for merged cells
                    for cfg in self.categories.values():
                        for col_key in ["schema_col", "table_col", "column_col"]:
                            col = cfg.get(col_key)
                            if col in df.columns:
                                df[col] = df[col].ffill()
                    sheets.append(df)
                except Exception as e:
                    print(f"⚠️ Error reading sheet {sheet_name}: {e}")
            return pd.concat(sheets, ignore_index=True) if sheets else pd.DataFrame()
        except Exception as e:
            print(f"❌ Error loading {file_path}: {e}")
            return pd.DataFrame()

    def create_row_mappings(self, df):
        row_mappings = []
        valid_rows = df.dropna(how="all")
        for _, row in valid_rows.iterrows():
            mapping = {}
            for cat in self.category_keys:
                cfg = self.categories[cat]
                schema = str(row.get(cfg["schema_col"], "NA")).strip()
                table = str(row.get(cfg["table_col"], "NA")).strip()
                column = str(row.get(cfg["column_col"], "NA")).strip()
                table_comment = str(row.get(cfg.get("table_comment_col", ""), "")).strip()
                column_comment = str(row.get(cfg.get("column_comment_col", ""), "")).strip()

                if schema in ["nan", "None", ""]: schema = "NA"
                if table in ["nan", "None", ""]: table = "NA"
                if column in ["nan", "None", ""]: column = "NA"
                if table_comment in ["nan", "None", ""]: table_comment = ""
                if column_comment in ["nan", "None", ""]: column_comment = ""

                mapping[cat] = {
                    "table": (schema, table),
                    "column": column,
                    "table_comment": table_comment,
                    "column_comment": column_comment,
                    "has_na": schema == "NA" or table == "NA" or column == "NA"
                }
            row_mappings.append(mapping)
        return row_mappings

    def generate_sql_for_row(self, mapping):
        """Generate SQL statements per row based on improved fallback logic"""
        edl = mapping.get("EDL", {})
        rdmof = mapping.get("RDMOF", {})
        original = mapping.get("Original_SSR", {})

        edl_na = edl.get("has_na", True)
        rdmof_na = rdmof.get("has_na", True)
        original_na = original.get("has_na", True)

        sql_statements = []

        # Case 1: All values present - Generate BOTH Original→EDL AND EDL→RDMOF
        if not original_na and not edl_na and not rdmof_na:
            sql1 = self.build_sql(original, edl, description="Original_SSR → EDL (Complete flow - Step 1)")
            sql2 = self.build_sql(edl, rdmof, description="EDL → RDMOF (Complete flow - Step 2)")
            sql_statements.extend([sql1, sql2])
        
        # Case 2: EDL is NA → Direct Original to RDMOF
        elif edl_na and not rdmof_na and not original_na:
            sql = self.build_sql(original, rdmof, description="Original_SSR → RDMOF (EDL is NA - Direct mapping)")
            sql_statements.append(sql)
        
        # Case 3: RDMOF is NA → Original to EDL only
        elif rdmof_na and not edl_na and not original_na:
            sql = self.build_sql(original, edl, description="Original_SSR → EDL (RDMOF is NA - Partial mapping)")
            sql_statements.append(sql)
        
        # Case 4: Original is NA → EDL to RDMOF only
        elif original_na and not edl_na and not rdmof_na:
            sql = self.build_sql(edl, rdmof, description="EDL → RDMOF (Original_SSR is NA - Partial mapping)")
            sql_statements.append(sql)
        
        # Case 5: Only Original and EDL (RDMOF NA)
        elif not original_na and not edl_na and rdmof_na:
            sql = self.build_sql(original, edl, description="Original_SSR → EDL (RDMOF is NA)")
            sql_statements.append(sql)
        
        # Case 6: Only EDL and RDMOF (Original NA)
        elif original_na and not edl_na and not rdmof_na:
            sql = self.build_sql(edl, rdmof, description="EDL → RDMOF (Original_SSR is NA)")
            sql_statements.append(sql)
        
        # Case 7: Only Original and RDMOF (EDL NA) - Direct mapping
        elif not original_na and edl_na and not rdmof_na:
            sql = self.build_sql(original, rdmof, description="Original_SSR → RDMOF (EDL is NA - Direct mapping)")
            sql_statements.append(sql)
        
        # Case 8: Multiple NAs or other cases
        else:
            sql = self.build_commented_placeholder(mapping, "Multiple categories have NA values")
            sql_statements.append(sql)

        return sql_statements

    def build_sql(self, src, tgt, description="", comment_all=False):
        """Build SQL statement from source to target"""
        src_schema, src_table = src.get("table", ("NA", "NA"))
        tgt_schema, tgt_table = tgt.get("table", ("NA", "NA"))
        src_col = src.get("column", "NA")
        tgt_col = tgt.get("column", "NA")
        src_table_comment = src.get("table_comment", "")
        src_col_comment = src.get("column_comment", "")
        tgt_table_comment = tgt.get("table_comment", "")
        tgt_col_comment = tgt.get("column_comment", "")

        # Build comments
        comments = []
        if description:
            comments.append(f"-- {description}")
        
        for c in [src_table_comment, src_col_comment, tgt_table_comment, tgt_col_comment]:
            if c and c != "nan":
                for line in str(c).splitlines():
                    if line.strip():
                        comments.append(f"-- {line.strip()}")

        comment_text = "\n".join(comments)
        if comment_text: 
            comment_text += "\n"

        if comment_all or src.get("has_na") or tgt.get("has_na"):
            return (
                f"{comment_text}"
                f"-- Mapping skipped due to NA values\n"
                f"-- {src_schema}.{src_table}.{src_col} → {tgt_schema}.{tgt_table}.{tgt_col}\n"
                f"-- INSERT INTO {tgt_schema}.{tgt_table} ({tgt_col})\n"
                f"-- SELECT DISTINCT {src_col} FROM {src_schema}.{src_table};\n"
            )
        else:
            return (
                f"{comment_text}"
                f"-- Insert: {src_schema}.{src_table}.{src_col} → {tgt_schema}.{tgt_table}.{tgt_col}\n"
                f"INSERT INTO {tgt_schema}.{tgt_table} ({tgt_col})\n"
                f"SELECT DISTINCT {src_col} FROM {src_schema}.{src_table};\n"
            )

    def build_commented_placeholder(self, mapping, reason):
        """Build a commented placeholder when no valid transformation is possible"""
        return (
            f"-- {reason}\n"
            f"-- Row skipped: No valid transformation possible\n"
            f"-- Original_SSR: {'NA' if mapping.get('Original_SSR', {}).get('has_na', True) else 'Valid'}\n"
            f"-- EDL: {'NA' if mapping.get('EDL', {}).get('has_na', True) else 'Valid'}\n"
            f"-- RDMOF: {'NA' if mapping.get('RDMOF', {}).get('has_na', True) else 'Valid'}\n"
        )

    def process_file(self, file_path):
        df = self.load_all_sheets(file_path)
        if df.empty: 
            return [], set()
        
        row_maps = self.create_row_mappings(df)
        all_sql = []
        unique_set = set()
        skipped_count = 0
        total_statements = 0
        
        for i, row_map in enumerate(row_maps, 1):
            # Generate SQL statements for this row (can be multiple)
            sql_list = self.generate_sql_for_row(row_map)
            
            for sql in sql_list:
                if sql:
                    total_statements += 1
                    # Normalize for duplicate detection
                    sql_normalized = " ".join(sql.split())
                    
                    if sql_normalized not in unique_set:
                        all_sql.append(sql)
                        unique_set.add(sql_normalized)
                    else:
                        skipped_count += 1
        
        print(f"   Processed {len(row_maps)} rows, generated {total_statements} statements ({len(all_sql)} unique)")
        if skipped_count > 0:
            print(f"   Skipped {skipped_count} duplicate statements")
            
        return all_sql, unique_set

    def run(self):
        os.makedirs(self.output_folder, exist_ok=True)
        files = self.get_excel_files()
        
        if not files:
            print(f"❌ No Excel files found in {self.input_folder}")
            return
            
        total_unique = 0

        print(f"🔄 Processing {len(files)} Excel file(s)...")
        
        for f in files:
            print(f"\n📄 Processing: {os.path.basename(f)}")
            sql_list, unique_sql = self.process_file(f)
            
            if sql_list:
                out_file = os.path.splitext(os.path.basename(f))[0] + ".sql"
                out_path = os.path.join(self.output_folder, out_file)
                
                with open(out_path, "w", encoding="utf-8") as fw:
                    fw.write(f"-- Generated SQL from: {os.path.basename(f)}\n")
                    fw.write(f"-- Total unique statements: {len(sql_list)}\n")
                    fw.write(f"-- Enhanced fallback logic applied\n\n")
                    fw.write("\n".join(sql_list))

                total_unique += len(unique_sql)
                print(f"✅ {out_file}: {len(unique_sql)} unique statements")
            else:
                print(f"⚠️ No valid SQL generated for {os.path.basename(f)}")

        print(f"\n📊 Summary:")
        print(f"   Files processed: {len(files)}")
        print(f"   Total unique statements: {total_unique}")
        print(f"   Categories: {' → '.join(self.category_keys)}")
        print(f"   Output folder: {self.output_folder}")

# main.py
import json
from insert_helper import InsertSQLGenerator

CONFIG_PATH = "config.json"

def main():
    try:
        with open(CONFIG_PATH) as f:
            config = json.load(f)

        input_folder = config.get("input_folder", "input_excel_files")
        output_folder = config.get("output_folder", "generated_inserts")

        print(f"🚀 Starting SQL Generator...")
        print(f"📂 Input folder: {input_folder}")
        print(f"📁 Output folder: {output_folder}")

        generator = InsertSQLGenerator(config, input_folder, output_folder)
        generator.run()
        
    except FileNotFoundError:
        print(f"❌ Config file not found: {CONFIG_PATH}")
    except Exception as e:
        print(f"❌ Error: {str(e)}")

if __name__ == "__main__":
    main()

# config.json
{
  "input_folder": "input_excel_files",
  "output_folder": "generated_inserts",
  "categories": {
    "Original_SSR": {
      "schema_col": "Original SSR - Schema",
      "table_col": "Original SSR - Physical Table Name",
      "column_col": "Original SSR - Physical Column Name",
      "table_comment_col": "Original SSR - Table Comment",
      "column_comment_col": "Original SSR - Column Comment"
    },
    "EDL": {
      "schema_col": "EDL - Schema",
      "table_col": "EDL - Physical Table Name",
      "column_col": "EDL - Physical Column Name",
      "table_comment_col": "EDL - Table Comment",
      "column_comment_col": "EDL - Column Comment"
    },
    "RDMOF": {
      "schema_col": "RDMOF - Schema",
      "table_col": "RDMOF - Physical Table Name",
      "column_col": "RDMOF - Physical Column Name",
      "table_comment_col": "RDMOF - Table Comment",
      "column_comment_col": "RDMOF - Column Comment"
    }
  }
}
