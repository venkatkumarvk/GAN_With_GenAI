#Multiple schema

import pandas as pd
import re
from typing import Dict, List, Tuple


class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str):
        self.excel_file_path = excel_file_path

    def load_excel_data(self) -> pd.DataFrame:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            sheets = excel_file.sheet_names
            print(f"Available sheets: {sheets}")
            sheet = next((s for s in sheets if 'BROKER_GROUP_RELATION' in s.upper()), sheets[0])
            df = pd.read_excel(excel_file, sheet_name=sheet)
            print(f"‚úÖ Loaded sheet: {sheet} with {df.shape[0]} rows and {df.shape[1]} columns.")
            return df
        except Exception as e:
            print(f"‚ùå Error loading Excel file: {e}")
            return None

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype):
            return 'STRING'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'STRING'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'STRING'

    def extract_category_columns(self, df: pd.DataFrame) -> Dict[str, List[Tuple[str, str]]]:
        """
        Extract and map columns per category (RDMOF, EDL, SSR)
        - RDMOF: uses RDMOF - Data Type
        - EDL/SSR: default to STRING
        """
        output = {
            "RDMOF": [],
            "EDL": [],
            "Original_SSR": []
        }
        seen = set()

        # Identify column names
        column_map = {}
        for col in df.columns:
            col_upper = col.upper()
            if 'RDMOF' in col_upper and 'COLUMN' in col_upper:
                column_map['RDMOF_column'] = col
            elif 'EDL' in col_upper and 'COLUMN' in col_upper:
                column_map['EDL_column'] = col
            elif ('SSR' in col_upper or 'ORIGINAL' in col_upper) and 'COLUMN' in col_upper:
                column_map['SSR_column'] = col
            elif 'RDMOF' in col_upper and 'DATA' in col_upper:
                column_map['datatype'] = col

        if 'datatype' not in column_map:
            print("‚ùó 'RDMOF - Data Type' column not found.")
            return output

        for _, row in df.iterrows():
            rd_dtype = row.get(column_map['datatype'], None)

            # RDMOF
            if 'RDMOF_column' in column_map:
                col = row.get(column_map['RDMOF_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if ('RDMOF', name.lower()) not in seen:
                        dtype = self.map_datatype(rd_dtype)
                        output['RDMOF'].append((name, dtype))
                        seen.add(('RDMOF', name.lower()))

            # EDL
            if 'EDL_column' in column_map:
                col = row.get(column_map['EDL_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if ('EDL', name.lower()) not in seen:
                        output['EDL'].append((name, 'STRING'))
                        seen.add(('EDL', name.lower()))

            # SSR
            if 'SSR_column' in column_map:
                col = row.get(column_map['SSR_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if ('SSR', name.lower()) not in seen:
                        output['Original_SSR'].append((name, 'STRING'))
                        seen.add(('SSR', name.lower()))

        return output

    def generate_schema_sql(self, table_name: str, columns: List[Tuple[str, str]]) -> str:
        if not columns:
            return f"-- No valid columns found for {table_name}."
        sql = f"-- {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.EDM_Reporting.{table_name} (\n"
        sql += ",\n".join([f"    {col} {dtype}" for col, dtype in columns])
        sql += "\n);"
        return sql

    def run(self):
        print("üöÄ Starting schema generation...")
        df = self.load_excel_data()
        if df is None:
            return

        category_columns = self.extract_category_columns(df)

        for category, columns in category_columns.items():
            table_name = f"{category.upper()}_TABLE"
            sql = self.generate_schema_sql(table_name, columns)

            print(f"\nüìÑ Generated schema for {category}:\n")
            print(sql)

            filename = f"{category.lower()}_table.sql"
            with open(filename, "w") as f:
                f.write(sql)
            print(f"‚úÖ Saved to file: {filename}")


# Usage
def main():
    excel_path = "your_excel_file.xlsx"  # üîÅ Replace with your Excel path
    generator = DatabricksSchemaGenerator(excel_path)
    generator.run()

if __name__ == "__main__":
    main()

######################################################################################################################3

#single schema stored

import pandas as pd
import re
from typing import List, Tuple


class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str):
        self.excel_file_path = excel_file_path

    def load_excel_data(self) -> pd.DataFrame:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            sheets = excel_file.sheet_names
            print(f"Available sheets: {sheets}")
            sheet = next((s for s in sheets if 'BROKER_GROUP_RELATION' in s.upper()), sheets[0])
            df = pd.read_excel(excel_file, sheet_name=sheet)
            print(f"Loaded sheet: {sheet} with {df.shape[0]} rows and {df.shape[1]} columns.")
            return df
        except Exception as e:
            print(f"Error loading Excel file: {e}")
            return None

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype):
            return 'STRING'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'STRING'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'STRING'

    def extract_columns(self, df: pd.DataFrame) -> List[Tuple[str, str]]:
        """
        Extract unique columns from RDMOF, EDL, and SSR.
        Only RDMOF columns get datatype from 'RDMOF - Data Type'. Others default to STRING.
        """
        columns = []
        seen = set()

        # Detect relevant columns
        column_map = {}
        for col in df.columns:
            col_upper = col.upper()
            if 'RDMOF' in col_upper and 'COLUMN' in col_upper:
                column_map['RDMOF_column'] = col
            elif 'EDL' in col_upper and 'COLUMN' in col_upper:
                column_map['EDL_column'] = col
            elif 'SSR' in col_upper and 'COLUMN' in col_upper:
                column_map['SSR_column'] = col
            elif 'RDMOF' in col_upper and 'DATA' in col_upper:
                column_map['datatype'] = col

        if 'datatype' not in column_map:
            print("‚ö†Ô∏è 'RDMOF - Data Type' column not found.")
            return []

        for _, row in df.iterrows():
            rd_dtype = row.get(column_map['datatype'], None)

            # RDMOF
            if 'RDMOF_column' in column_map:
                col = row.get(column_map['RDMOF_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen:
                        dtype = self.map_datatype(rd_dtype)
                        columns.append((name, dtype))
                        seen.add(name.lower())

            # EDL
            if 'EDL_column' in column_map:
                col = row.get(column_map['EDL_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen:
                        columns.append((name, 'STRING'))
                        seen.add(name.lower())

            # SSR
            if 'SSR_column' in column_map:
                col = row.get(column_map['SSR_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen:
                        columns.append((name, 'STRING'))
                        seen.add(name.lower())

        print(f"‚úÖ Extracted {len(columns)} unique columns.")
        return columns

    def generate_schema_sql(self, table_name: str, columns: List[Tuple[str, str]]) -> str:
        if not columns:
            return "-- No valid columns to generate schema."
        sql = f"-- {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.EDM_Reporting.{table_name} (\n"
        sql += ",\n".join([f"    {col} {dtype}" for col, dtype in columns])
        sql += "\n);"
        return sql

    def run(self):
        print("üöÄ Generating Databricks schema for BROKER_GROUP_RELATION...")
        df = self.load_excel_data()
        if df is None:
            return

        columns = self.extract_columns(df)
        sql = self.generate_schema_sql("BROKER_GROUP_RELATION", columns)

        print("\n" + "="*60)
        print("üìÑ GENERATED SQL SCHEMA")
        print("="*60)
        print(sql)

        with open("broker_group_relation_schema.sql", "w") as f:
            f.write(sql)
        print("\n‚úÖ Schema saved to 'broker_group_relation_schema.sql'")


# Example usage
def main():
    excel_path = "your_excel_file.xlsx"  # üîÅ Replace with actual file path
    generator = DatabricksSchemaGenerator(excel_path)
    generator.run()

if __name__ == "__main__":
    main()
