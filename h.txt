"""
FIELD CONFIGURATIONS FOR DOCUMENT EXTRACTION
============================================

USERS: Edit this file to add/modify field configurations
DEVELOPERS: Don't edit this file - edit prompt_builder.py instead

To add a new field:
1. Add field name to config.json
2. Add field configuration below in FIELD_LIBRARY
3. Run main.py - that's it!
"""

# ============================================================================
# FIELD LIBRARY - EDIT THIS TO ADD/MODIFY FIELDS
# ============================================================================

FIELD_LIBRARY = {
    "name": {
        "description": "Full name as it appears on the document",
        "examples": ["JOHN MICHAEL DOE", "Jane Smith", "æŽæ˜Ž"],
        "format": "First Middle Last or Last First format",
        "extraction_hints": [
            "May be split across multiple lines",
            "Check for 'Surname' and 'Given Names' labels",
            "Look for 'Full Name' or 'Name of Holder' sections"
        ],
        "confidence_factors": [
            "Clear text with proper spacing",
            "Consistent formatting across document",
            "Matches expected name pattern"
        ]
    },
    
    "passport_number": {
        "description": "Official passport identification number",
        "examples": ["AB123456", "P12345678", "N1234567890"],
        "format": "Alphanumeric, typically 7-10 characters",
        "extraction_hints": [
            "Usually labeled 'Passport No.', 'No. du Passeport'",
            "May appear multiple times (bio page and MRZ)",
            "Verify against MRZ if present"
        ],
        "confidence_factors": [
            "Clearly labeled as passport number",
            "Matches expected format for issuing country",
            "No OCR errors visible"
        ]
    },
    
    "date_of_birth": {
        "description": "Person's date of birth",
        "examples": ["15 JAN 1985", "1985-01-15", "01/15/1985"],
        "format": "Various date formats (DD MMM YYYY, YYYY-MM-DD, MM/DD/YYYY)",
        "extraction_hints": [
            "Look for 'Date of Birth', 'DOB', 'Born'",
            "May be in different formats based on country",
            "Cross-check with MRZ if available"
        ],
        "confidence_factors": [
            "Date is clearly labeled",
            "Consistent with other documents",
            "Reasonable age range"
        ]
    },
    
    "nationality": {
        "description": "Country of citizenship",
        "examples": ["USA", "United States of America", "GBR", "United Kingdom"],
        "format": "Full country name or ISO 3166-1 alpha-3 code",
        "extraction_hints": [
            "Look for 'Nationality', 'Country', 'PaÃ­s'",
            "May match issuing country",
            "Can be in native language or English"
        ],
        "confidence_factors": [
            "Clearly stated on document",
            "Matches document issuing country",
            "Valid country name/code"
        ]
    },
    
    "issue_date": {
        "description": "Date when document was issued",
        "examples": ["01 MAR 2020", "2020-03-01", "March 1, 2020"],
        "format": "Various date formats accepted",
        "extraction_hints": [
            "Look for 'Date of Issue', 'Issued', 'Date dÃ©livrance'",
            "Should be in the past",
            "Usually 10 years before expiry for passports"
        ],
        "confidence_factors": [
            "Clearly labeled as issue date",
            "Before expiry date",
            "Reasonable time frame"
        ]
    },
    
    "expiry_date": {
        "description": "Date when document expires",
        "examples": ["01 MAR 2030", "2030-03-01", "March 1, 2030"],
        "format": "Various date formats accepted",
        "extraction_hints": [
            "Look for 'Date of Expiry', 'Valid Until', 'Expiry'",
            "Usually 10 years after issue for passports",
            "May be emphasized or highlighted"
        ],
        "confidence_factors": [
            "Clearly labeled as expiry date",
            "After issue date",
            "Standard validity period"
        ]
    },
    
    "id_number": {
        "description": "National ID card number",
        "examples": ["ID123456789", "987654321", "AB-1234567"],
        "format": "Varies by country, may be alphanumeric",
        "extraction_hints": [
            "Look for 'ID Number', 'Identity Number', 'National ID'",
            "Format varies significantly by country",
            "Usually prominently displayed"
        ],
        "confidence_factors": [
            "Clearly labeled",
            "Matches expected format for country",
            "No OCR errors visible"
        ]
    },
    
    "license_number": {
        "description": "Driver's license number",
        "examples": ["DL123456", "A1234567890123", "AB-123-456"],
        "format": "Varies by state/country, alphanumeric",
        "extraction_hints": [
            "Look for 'License Number', 'DL No.', 'Permit Number'",
            "Format varies by state/country",
            "Often has specific pattern based on issuing region"
        ],
        "confidence_factors": [
            "Clearly labeled",
            "Matches known format for jurisdiction",
            "Consistent length and pattern"
        ]
    },
    
    "address": {
        "description": "Full residential address",
        "examples": [
            "123 Main Street, New York, NY 10001",
            "Flat 5, 10 Downing Street, London",
            "åŒ—äº¬å¸‚æœé˜³åŒºå»ºå›½è·¯1å·"
        ],
        "format": "Street, City, State/Province, Postal Code",
        "extraction_hints": [
            "May span multiple lines",
            "Look for 'Address', 'Residence', 'Domicile'",
            "Can be in various formats based on country"
        ],
        "confidence_factors": [
            "Clearly labeled as address",
            "Complete address components",
            "Proper formatting"
        ]
    },
    
    "place_of_birth": {
        "description": "Location where person was born",
        "examples": ["New York, USA", "London, United Kingdom", "Paris, France"],
        "format": "City, Country or City, State, Country",
        "extraction_hints": [
            "Look for 'Place of Birth', 'Born in', 'Lieu de naissance'",
            "Usually city and country",
            "Can be in native language"
        ],
        "confidence_factors": [
            "Clearly labeled",
            "Valid location name",
            "Consistent format"
        ]
    },
    
    "sex": {
        "description": "Biological sex or gender",
        "examples": ["M", "F", "Male", "Female"],
        "format": "Usually M/F or Male/Female",
        "extraction_hints": [
            "Look for 'Sex', 'Gender', 'Sexe'",
            "Usually single letter (M/F)",
            "Check MRZ for verification"
        ],
        "confidence_factors": [
            "Clearly labeled",
            "Standard format",
            "Unambiguous"
        ]
    },
    
    # ========================================================================
    # ADD YOUR CUSTOM FIELDS BELOW
    # ========================================================================
    # 
    # Template:
    # "your_field_name": {
    #     "description": "Brief description of what this field is",
    #     "examples": ["Example 1", "Example 2", "Example 3"],
    #     "format": "Expected format description",
    #     "extraction_hints": [
    #         "Hint 1: What to look for",
    #         "Hint 2: Common labels",
    #         "Hint 3: Special considerations"
    #     ],
    #     "confidence_factors": [
    #         "Factor 1: When to be confident",
    #         "Factor 2: What increases confidence",
    #         "Factor 3: What confirms correctness"
    #     ]
    # },
}


# ============================================================================
# SYSTEM PROMPT CONFIGURATION
# ============================================================================

SYSTEM_PROMPT_CONFIG = {
    "role": "You are an expert document extraction system specialized in identity documents, passports, licenses, and official records.",
    
    "extraction_rules": [
        "Extract EXACT values as they appear in the document",
        "For each field, provide: value (extracted text or null) and confidence (0.0 to 1.0)",
        "Use similar document examples provided to improve accuracy",
        "If a field is not present, return null for value and 0.0 for confidence",
        "Be conservative with confidence scores - only use >0.9 when certain"
    ],
    
    "confidence_guide": {
        "0.95-1.0": "Clear, unambiguous text that exactly matches expected format",
        "0.80-0.94": "Text is clear but format slightly unusual",
        "0.60-0.79": "Text is readable but context suggests some uncertainty",
        "0.40-0.59": "Partial match or low quality OCR",
        "0.20-0.39": "Very uncertain, possibly misread",
        "0.0-0.19": "Field not found or illegible"
    },
    
    "output_format": "Return ONLY a JSON object: {field_name: {value: '...', confidence: 0.95}, ...}",
    
    "restrictions": "Do NOT include any explanations, markdown formatting, or additional text. Just the pure JSON object."
}


# ============================================================================
# DOCUMENT TYPE SPECIFIC HINTS
# ============================================================================

DOCUMENT_TYPE_HINTS = {
    "passport": {
        "title": "PASSPORT DOCUMENT EXTRACTION",
        "special_considerations": [
            "Passport numbers may include letters and numbers",
            "Dates are often in DD MMM YYYY format (e.g., 15 JAN 1990)",
            "Look for MRZ (Machine Readable Zone) at bottom",
            "Names may be split (Surname / Given Names)"
        ]
    },
    
    "license": {
        "title": "DRIVER'S LICENSE EXTRACTION",
        "special_considerations": [
            "License numbers vary by state/country",
            "May contain multiple date fields",
            "Address may span multiple lines",
            "License class indicates vehicle types"
        ]
    },
    
    "id_card": {
        "title": "ID CARD EXTRACTION",
        "special_considerations": [
            "ID formats vary significantly by country",
            "May have both front and back text",
            "Look for official stamps or holograms",
            "National ID numbers follow country-specific patterns"
        ]
    }
}


# ============================================================================
# RAG PROMPT TEMPLATES
# ============================================================================

RAG_PROMPT_TEMPLATE = {
    "intro": "You have access to {num_docs} similar documents that have already been processed. Use these as reference examples to improve extraction accuracy.",
    "example_header": "SIMILAR DOCUMENT EXAMPLES:",
    "transition": "---\n\nNOW EXTRACT FROM THIS NEW DOCUMENT:",
    "instructions": [
        "Use the similar documents above as examples of:",
        "- How fields typically appear in this type of document",
        "- Expected formats and patterns",
        "- Typical confidence levels based on text quality"
    ],
    "reminder": "Remember to return ONLY the JSON object with extracted fields and confidence scores."
}


STANDARD_PROMPT_TEMPLATE = {
    "header": "Extract the required fields from this document:",
    "reminder": "Remember to return ONLY the JSON object with extracted fields and confidence scores."
}


-----


"""
PROMPT BUILDER - STABLE CODE (DON'T EDIT!)
============================================

This file contains the prompt building logic.
Users should NOT edit this file.

To add/modify fields: Edit prompt_config.py instead
"""

import json
from typing import List, Dict, Any

# Import configurations from prompt_config.py
from prompt_config import (
    FIELD_LIBRARY,
    SYSTEM_PROMPT_CONFIG,
    DOCUMENT_TYPE_HINTS,
    RAG_PROMPT_TEMPLATE,
    STANDARD_PROMPT_TEMPLATE
)


class ExtractionPromptBuilder:
    """
    Builds prompts dynamically from prompt_config.py
    
    STABLE CLASS - Users should not modify this
    All configurations come from prompt_config.py
    """
    
    def __init__(self, fields: List[str]):
        """
        Initialize with list of fields to extract
        
        Args:
            fields: List of field names (from config.json)
        """
        self.fields = fields
        self._validate_fields()
    
    def _validate_fields(self):
        """Validate that all fields have configurations"""
        missing = [f for f in self.fields if f not in FIELD_LIBRARY]
        if missing:
            print(f"âš ï¸  Warning: Missing configurations for fields: {', '.join(missing)}")
            print(f"   Add them to FIELD_LIBRARY in prompt_config.py")
    
    def build_system_prompt(
        self, 
        document_type: str = None,
        ocr_quality: str = "high"
    ) -> str:
        """
        Build system prompt dynamically
        
        Args:
            document_type: Type of document (passport, license, id_card)
            ocr_quality: OCR quality level (low, medium, high)
        
        Returns:
            Complete system prompt string
        """
        parts = []
        
        # Role
        parts.append(SYSTEM_PROMPT_CONFIG['role'])
        parts.append("")
        
        # Task
        field_list = ", ".join(self.fields)
        parts.append(f"Your task is to extract the following fields from documents:")
        parts.append(field_list)
        parts.append("")
        
        # Extraction rules
        parts.append("EXTRACTION RULES:")
        for idx, rule in enumerate(SYSTEM_PROMPT_CONFIG['extraction_rules'], 1):
            parts.append(f"{idx}. {rule}")
        parts.append("")
        
        # Confidence guide
        parts.append("CONFIDENCE SCORING GUIDE:")
        for range_str, description in SYSTEM_PROMPT_CONFIG['confidence_guide'].items():
            parts.append(f"- {range_str}: {description}")
        parts.append("")
        
        # Document type hints
        if document_type and document_type in DOCUMENT_TYPE_HINTS:
            hints = DOCUMENT_TYPE_HINTS[document_type]
            parts.append(hints['title'])
            parts.append("Special considerations:")
            for consideration in hints['special_considerations']:
                parts.append(f"- {consideration}")
            parts.append("")
        
        # OCR quality adjustments
        if ocr_quality == "low":
            parts.append("OCR QUALITY NOTE: This text may contain OCR errors.")
            parts.append("- Be more conservative with confidence scores")
            parts.append("- Look for context clues to verify values")
            parts.append("- Consider common OCR mistakes (0/O, 1/I, 5/S, 8/B)")
            parts.append("")
        
        # Field details
        parts.append("DETAILED FIELD INFORMATION:")
        parts.append("")
        for field in self.fields:
            if field in FIELD_LIBRARY:
                info = FIELD_LIBRARY[field]
                parts.append(f"**{field}**:")
                parts.append(f"  Description: {info['description']}")
                parts.append(f"  Examples: {', '.join(info['examples'])}")
                parts.append(f"  Format: {info['format']}")
                
                if info.get('extraction_hints'):
                    parts.append("  Extraction Hints:")
                    for hint in info['extraction_hints']:
                        parts.append(f"    - {hint}")
                
                parts.append("")
        
        # Output format
        parts.append("OUTPUT FORMAT:")
        parts.append(SYSTEM_PROMPT_CONFIG['output_format'])
        parts.append("")
        parts.append(SYSTEM_PROMPT_CONFIG['restrictions'])
        
        return "\n".join(parts)
    
    def build_extraction_prompt_without_rag(self, document_text: str) -> str:
        """
        Build extraction prompt WITHOUT RAG (fallback)
        
        Args:
            document_text: The text to extract from
        
        Returns:
            User prompt string
        """
        parts = []
        
        parts.append(STANDARD_PROMPT_TEMPLATE['header'])
        parts.append("")
        parts.append(f"Fields to extract: {', '.join(self.fields)}")
        parts.append("")
        parts.append("DOCUMENT TEXT:")
        parts.append(document_text[:8000])
        parts.append("")
        parts.append(STANDARD_PROMPT_TEMPLATE['reminder'])
        
        return "\n".join(parts)
    
    def build_extraction_prompt_with_rag(
        self,
        document_text: str,
        similar_documents: List[Dict[str, Any]],
        top_k: int = 3
    ) -> str:
        """
        Build extraction prompt WITH RAG context
        
        Args:
            document_text: The text to extract from
            similar_documents: List of similar documents from vector search
            top_k: Number of similar documents to include
        
        Returns:
            User prompt string
        """
        parts = []
        
        # Limit to top K
        similar_docs = similar_documents[:top_k]
        num_docs = len(similar_docs)
        
        # Intro
        intro = RAG_PROMPT_TEMPLATE['intro'].format(num_docs=num_docs)
        parts.append(intro)
        parts.append("")
        
        # Examples header
        parts.append(RAG_PROMPT_TEMPLATE['example_header'])
        parts.append("")
        
        # Format each similar document
        for idx, doc in enumerate(similar_docs, 1):
            example = self._format_similar_document(doc, idx)
            parts.append(example)
        
        # Transition
        parts.append(RAG_PROMPT_TEMPLATE['transition'])
        parts.append("")
        
        # Fields
        parts.append(f"Fields to extract: {', '.join(self.fields)}")
        parts.append("")
        
        # Document text
        parts.append("DOCUMENT TEXT:")
        parts.append(document_text[:8000])
        parts.append("")
        
        # Instructions
        for instruction in RAG_PROMPT_TEMPLATE['instructions']:
            parts.append(instruction)
        parts.append("")
        
        # Reminder
        parts.append(RAG_PROMPT_TEMPLATE['reminder'])
        
        return "\n".join(parts)
    
    def _format_similar_document(self, doc: Dict[str, Any], index: int) -> str:
        """Format a single similar document as an example"""
        
        # Get extracted fields
        extracted_fields_str = doc.get('extracted_fields', '{}')
        if isinstance(extracted_fields_str, str):
            try:
                extracted_fields = json.loads(extracted_fields_str)
            except:
                extracted_fields = {}
        else:
            extracted_fields = extracted_fields_str
        
        # Get metadata
        doc_name = doc.get('document_name', f'Document {index}')
        similarity = doc.get('@search.score', 0.0)
        content_preview = doc.get('content', '')[:200]
        
        # Format example
        return f"""
EXAMPLE {index} (Similarity: {similarity:.2f})
Document: {doc_name}
Content Preview: {content_preview}...

Extracted Fields:
{json.dumps(extracted_fields, indent=2)}
"""
    
    def get_field_info(self, field_name: str) -> Dict[str, Any]:
        """Get configuration for a specific field"""
        return FIELD_LIBRARY.get(field_name, {})
    
    @staticmethod
    def get_available_fields() -> List[str]:
        """Get list of all available fields"""
        return list(FIELD_LIBRARY.keys())
    
    @staticmethod
    def validate_fields(fields: List[str]) -> Dict[str, Any]:
        """Validate that all fields have configurations"""
        missing = [f for f in fields if f not in FIELD_LIBRARY]
        configured = [f for f in fields if f in FIELD_LIBRARY]
        
        return {
            'valid': len(missing) == 0,
            'configured': configured,
            'missing': missing,
            'message': f"Missing: {', '.join(missing)}" if missing else "All fields configured âœ“"
        }


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def print_available_fields():
    """Print all available fields"""
    print("\n" + "="*70)
    print("AVAILABLE FIELDS IN LIBRARY")
    print("="*70)
    for idx, field in enumerate(FIELD_LIBRARY.keys(), 1):
        info = FIELD_LIBRARY[field]
        print(f"\n{idx}. {field}")
        print(f"   Description: {info['description']}")
        print(f"   Examples: {', '.join(info['examples'][:2])}")
    print("\n" + "="*70)


def validate_configuration(fields: List[str]) -> None:
    """Validate field configuration and print results"""
    validation = ExtractionPromptBuilder.validate_fields(fields)
    
    print("\n" + "="*70)
    print("FIELD VALIDATION")
    print("="*70)
    print(f"Status: {'âœ“ Valid' if validation['valid'] else 'âœ— Invalid'}")
    print(f"Configured: {', '.join(validation['configured'])}")
    if validation['missing']:
        print(f"Missing: {', '.join(validation['missing'])}")
        print("\nTo fix: Add missing fields to FIELD_LIBRARY in prompt_config.py")
    print("="*70)


# ============================================================================
# EXAMPLE USAGE (for testing)
# ============================================================================

if __name__ == "__main__":
    print("="*70)
    print("PROMPT BUILDER TEST")
    print("="*70)
    
    # Show available fields
    print_available_fields()
    
    # Test fields
    fields = ["name", "passport_number", "date_of_birth"]
    
    # Validate
    validate_configuration(fields)
    
    # Create builder
    builder = ExtractionPromptBuilder(fields)
    
    # Build system prompt
    system_prompt = builder.build_system_prompt(document_type="passport")
    
    print("\n" + "="*70)
    print("SYSTEM PROMPT SAMPLE:")
    print("="*70)
    print(system_prompt[:500] + "...")
    print("\nâœ“ Prompt builder working correctly!")



------------

"""
TEXT RAG - Text-Only Document Extraction
=========================================

OCR text is PRIMARY source.
Uses Azure AI Search vector search to find similar past documents.
Passes them as few-shot examples to GPT-4o for extraction.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import json
import logging
from typing import List, Dict, Any, Optional

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class TextRAGExtractor:
    """
    Text-Only RAG Extractor

    Flow:
        Document â†’ OCR (primary) â†’ Embed â†’ Search Similar â†’ GPT-4o Extract

    Mode: Fast, cost-effective, 92-95% accuracy
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(f"TextRAGExtractor ready | top_k={top_k} | threshold={similarity_threshold}")
        print(f"    âœ“ RAG Mode : TEXT-ONLY")
        print(f"    âœ“ Vision   : Disabled")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PUBLIC: Extract with RAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def extract_with_rag(
        self,
        document_text: str,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Text RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, embeddings
            }
        """
        logger.info(f"Text RAG extraction | doc={source_document} | provider={provider}")

        # Step 1 â€” Generate text embedding (OCR is primary)
        embedding = self.openai_manager.generate_embeddings(document_text)
        logger.info("  âœ“ Text embedding generated")

        # Step 2 â€” Search similar documents
        similar_docs = self._search_similar(embedding, provider, index_name)

        # Step 3 â€” Build prompt
        if similar_docs:
            print(f"    âœ“ RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  âœ“ {len(similar_docs)} similar docs retrieved")
            # Use YOUR working prompt method
            user_prompt = self.prompt_builder.build_extraction_prompt_with_rag(
                document_text=document_text,
                similar_documents=similar_docs,
                top_k=min(len(similar_docs), self.top_k)
            )
        else:
            print(f"    âš  RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  âš  No similar docs â€” standard extraction")
            # Use YOUR working prompt method
            user_prompt = self.prompt_builder.build_extraction_prompt_without_rag(
                document_text=document_text
            )

        # Step 4 â€” GPT-4o extraction
        extracted_fields, system_prompt = self._call_gpt(user_prompt, document_type)
        
        # Step 5 â€” Store in Azure AI Search for future RAG
        confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        self._store_in_index(
            index_name=index_name,
            document_name=source_document,
            provider=provider,
            document_text=document_text,
            text_embedding=embedding,  # Fixed: was text_emb
            extracted_fields=extracted_fields,
            avg_confidence=avg_confidence
        )

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':        False,
            'visual_description': '',
            'system_prompt':     system_prompt,  # Include system prompt
            'user_prompt':       user_prompt,     # Include user prompt
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding   # combined = text in text-only mode
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        document_type: Optional[str] = None,
        source_document: str = '',
        provider: str = '',
        index_name: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG â€” used for first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            document_type:   Document type hint
            source_document: Source filename
            provider:        Provider name (for storage)
            index_name:      Index name (for storage)

        Returns:
            Same shape as extract_with_rag
        """
        logger.info(f"Text extraction (no RAG) | doc={source_document}")

        embedding   = self.openai_manager.generate_embeddings(document_text)
        # Use YOUR working prompt method
        user_prompt = self.prompt_builder.build_extraction_prompt_without_rag(
            document_text=document_text
        )
        extracted_fields, system_prompt = self._call_gpt(user_prompt, document_type)
        
        # Store in index for future RAG (if provider/index provided)
        if provider and index_name:
            confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            self._store_in_index(
                index_name=index_name,
                document_name=source_document,
                provider=provider,
                document_text=document_text,
                text_embedding=embedding,
                extracted_fields=extracted_fields,
                avg_confidence=avg_confidence
            )

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          False,
            'similar_docs_count': 0,
            'has_vision':        False,
            'visual_description': '',
            'system_prompt':     system_prompt,  # Include system prompt
            'user_prompt':       user_prompt,     # Include user prompt
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding
            }
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PRIVATE HELPERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _store_in_index(
        self,
        index_name: str,
        document_name: str,
        provider: str,
        document_text: str,
        text_embedding: List[float],
        extracted_fields: Dict[str, Any],
        avg_confidence: float
    ):
        """
        Store document in Azure AI Search index for future RAG
        """
        try:
            from azure.search.documents.indexes import SearchIndexClient
            from azure.search.documents.indexes.models import (
                SearchIndex, SearchField, SearchFieldDataType,
                VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile
            )
            
            # Create index if not exists
            index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=self.search_credential
            )
            
            try:
                index_client.get_index(index_name)
                logger.info(f"  Index '{index_name}' exists")
            except:
                logger.info(f"  Creating index '{index_name}'...")
                
                fields = [
                    SearchField(name="id", type=SearchFieldDataType.String, key=True, filterable=True),
                    SearchField(name="document_name", type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SearchField(name="provider", type=SearchFieldDataType.String, filterable=True),
                    SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="extracted_fields", type=SearchFieldDataType.String),
                    SearchField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
                    SearchField(
                        name="content_vector",
                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        vector_search_dimensions=3072,
                        vector_search_profile_name="text-profile"
                    )
                ]
                
                vector_search = VectorSearch(
                    algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
                    profiles=[VectorSearchProfile(name="text-profile", algorithm_configuration_name="hnsw-config")]
                )
                
                index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
                index_client.create_index(index)
                logger.info(f"  âœ“ Index '{index_name}' created")
            
            # Upload document
            doc_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            # Generate document ID for Azure AI Search
            # LONG-TERM SOLUTION: Use hash of original filename
            # This way:
            # 1. Original filename preserved in metadata
            # 2. Document ID is always valid (hash is alphanumeric)
            # 3. No future errors from new special characters
            # 4. Consistent and deterministic
            
            import hashlib
            
            # Create hash from provider + document name (deterministic)
            unique_string = f"{provider}_{document_name}"
            hash_value = hashlib.sha256(unique_string.encode()).hexdigest()[:32]  # 32 chars
            
            # Document ID: hash only (always valid!)
            doc_id = f"{provider}_{hash_value}"
            
            # Store original filename separately in document metadata
            # This is returned in search results and stored in JSON
            
            document = {
                "id": doc_id,
                "document_name": document_name,
                "provider": provider,
                "content": document_text[:50000],
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": avg_confidence,
                "content_vector": text_embedding
            }
            
            doc_client.upload_documents([document])
            logger.info(f"  âœ“ Stored in index: {doc_id}")
            
        except Exception as e:
            logger.error(f"Failed to store in index: {e}", exc_info=True)

    def _search_similar(
        self,
        embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search for similar documents"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=["document_name", "content", "extracted_fields", "avg_confidence"]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _build_rag_prompt(
        self,
        document_text: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build user prompt with RAG few-shot examples"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass
            parts.append("")

        parts.append("â”€" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")
        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:8000])
        parts.append("")
        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(self, document_text: str) -> str:
        """Build standard prompt without RAG context"""
        return (
            f"Extract the following fields from this document:\n\n"
            f"OCR TEXT (Primary Source):\n{document_text[:8000]}\n\n"
            f"Fields to extract: {', '.join(self.fields)}\n\n"
            "Return ONLY a JSON object with field values and confidence scores."
        )

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> tuple[Dict[str, Any], str]:
        """Call GPT-4o and return parsed extraction result + system prompt"""
        try:
            # Use prompt_builder to build system prompt from prompt.py
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            # Track tokens
            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content), system_prompt

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}, ""

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
    


---------------


"""
MULTIMODAL RAG - Vision + Text Document Extraction
====================================================

OCR text is PRIMARY source.
Vision (GPT-4 Vision) is SUPPLEMENTARY enhancement.
Combined embedding (text + visual) improves RAG similarity search.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import base64
import json
import logging
from typing import Dict, Any, List, Optional, Tuple

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class MultimodalRAGExtractor:
    """
    Multimodal RAG Extractor â€” Text + Vision

    Flow:
        Document â†’ OCR (primary)       â†’ Text Embed   â”€â”
                 â†’ Vision (supplement) â†’ Visual Embed â”€â”¤ â†’ Combined â†’ Search â†’ GPT-4o
                                                       â”€â”˜

    Mode: Higher accuracy (95-97%), 100% coverage (handles image-only docs)
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        text_weight: float = 0.7,
        visual_weight: float = 0.3
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
            text_weight:          Weight for text embedding in combined vector (default 0.7)
            visual_weight:        Weight for visual embedding in combined vector (default 0.3)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold
        self.text_weight          = text_weight
        self.visual_weight        = visual_weight

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(
            f"MultimodalRAGExtractor ready | "
            f"top_k={top_k} | text_w={text_weight} | visual_w={visual_weight}"
        )
        print(f"    âœ“ RAG Mode : MULTIMODAL (Text + Vision)")
        print(f"    âœ“ Vision   : Enabled")
        print(f"    âœ“ Weights  : text={text_weight} | visual={visual_weight}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PUBLIC: Extract with RAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def extract_with_rag(
        self,
        document_text: str,
        image_bytes: bytes,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Multimodal RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            image_bytes:     Raw image bytes for vision analysis
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, has_vision,
              visual_description, embeddings
            }
        """
        logger.info(f"Multimodal RAG | doc={source_document} | provider={provider}")

        # Step 1 â€” Vision analysis (supplementary)
        visual_description = ''
        if image_bytes:
            print(f"    ðŸ” Vision analysis (supplementary)...")
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']
                print(f"    âœ“ Vision       : {len(visual_description)} chars")
                logger.info(f"  âœ“ Vision complete ({len(visual_description)} chars)")
            else:
                print(f"    âš  Vision failed â€” OCR only (primary)")
                logger.warning(f"  âš  Vision failed: {result.get('error')}")
        else:
            print(f"    âš  No image bytes â€” OCR only (primary)")

        # Step 2 â€” Generate embeddings
        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )
        emb_mode = "text + visual" if visual_description else "text only (vision unavailable)"
        print(f"    âœ“ Embeddings   : {emb_mode}")
        logger.info(f"  âœ“ Embeddings generated: {emb_mode}")

        # Step 3 â€” Search similar documents
        similar_docs = self._search_similar(combined_emb, provider, index_name)

        # Step 4 â€” Build prompt
        if similar_docs:
            print(f"    âœ“ RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  âœ“ {len(similar_docs)} similar docs retrieved")
            user_prompt = self.prompt_builder.build_extraction_prompt_with_rag(document_text=document_text, similar_documents=similar_docs, top_k=min(len(similar_docs), self.top_k))
        else:
            print(f"    âš  RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  âš  No similar docs â€” standard extraction")
            user_prompt = self.prompt_builder.build_extraction_prompt_without_rag(document_text=document_text)

        # Step 5 â€” GPT-4o extraction
        extracted_fields, system_prompt = self._call_gpt(user_prompt, document_type)
        
        # Step 6 â€” Store in Azure AI Search for future RAG
        confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        self._store_in_index(
            index_name=index_name,
            document_name=source_document,
            provider=provider,
            document_text=document_text,
            visual_description=visual_description,
            combined_embedding=combined_emb,
            extracted_fields=extracted_fields,
            avg_confidence=avg_confidence
        )

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'system_prompt':     system_prompt,  # Include system prompt
            'user_prompt':       user_prompt,     # Include user prompt
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        image_bytes: Optional[bytes] = None,
        document_type: Optional[str] = None,
        source_document: str = '',
        provider: str = '',
        index_name: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG â€” first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            image_bytes:     Image bytes for vision (optional)
            document_type:   Document type hint
            source_document: Source filename
            provider:        Provider name (for storage)
            index_name:      Index name (for storage)
        """
        logger.info(f"Multimodal extraction (no RAG) | doc={source_document}")

        visual_description = ''
        if image_bytes:
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']

        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )

        user_prompt = self.prompt_builder.build_extraction_prompt_without_rag(document_text=document_text)
        extracted_fields, system_prompt = self._call_gpt(user_prompt, document_type)
        
        # Store in index for future RAG (if provider/index provided)
        if provider and index_name:
            confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            self._store_in_index(
                index_name=index_name,
                document_name=source_document,
                provider=provider,
                document_text=document_text,
                visual_description=visual_description,
                combined_embedding=combined_emb,
                extracted_fields=extracted_fields,
                avg_confidence=avg_confidence
            )

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           False,
            'similar_docs_count': 0,
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'system_prompt':     system_prompt,  # Include system prompt
            'user_prompt':       user_prompt,     # Include user prompt
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PRIVATE HELPERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _analyze_vision(
        self,
        image_bytes: bytes,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """Analyze document image using GPT-4 Vision"""
        try:
            b64 = base64.b64encode(image_bytes).decode('utf-8')

            type_hints = {
                'passport': "\n\nFocus on: MRZ zone, country emblem, biographical page layout.",
                'license':  "\n\nFocus on: License number position, photo, endorsements.",
                'id_card':  "\n\nFocus on: ID number placement, official seals, front/back layout."
            }
            doc_hint = type_hints.get(document_type, '') if document_type else ''

            prompt = (
                "Analyze this identity document image and describe:\n"
                "1. Document type and layout\n"
                "2. Security features (holograms, watermarks, seals)\n"
                "3. Document condition and image quality\n"
                "4. Photo position and quality\n"
                "5. Any notable visual elements or anomalies\n\n"
                "Keep description concise (max 400 chars) â€” it supplements OCR text."
                + doc_hint
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url":    f"data:image/jpeg;base64,{b64}",
                                "detail": "high"
                            }
                        }
                    ]
                }],
                max_tokens=500,
                temperature=0.0
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return {
                'success':            True,
                'visual_description': response.choices[0].message.content
            }

        except Exception as e:
            logger.error(f"Vision analysis failed: {e}", exc_info=True)
            return {'success': False, 'visual_description': '', 'error': str(e)}

    def _generate_embeddings(
        self,
        document_text: str,
        visual_description: str
    ) -> Tuple[List[float], Optional[List[float]], List[float]]:
        """
        Generate text, visual, and combined embeddings

        Returns:
            (text_embedding, visual_embedding_or_None, combined_embedding)
        """
        text_emb = self.openai_manager.generate_embeddings(document_text)

        if visual_description:
            visual_emb   = self.openai_manager.generate_embeddings(visual_description)
            combined_emb = [
                self.text_weight * text_emb[i] + self.visual_weight * visual_emb[i]
                for i in range(len(text_emb))
            ]
            return text_emb, visual_emb, combined_emb
        else:
            return text_emb, None, text_emb   # fallback: combined = text

    def _search_similar(
        self,
        combined_embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search using combined embedding"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": combined_embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=[
                    "document_name", "content",
                    "extracted_fields", "visual_description", "avg_confidence"
                ]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _store_in_index(
        self,
        index_name: str,
        document_name: str,
        provider: str,
        document_text: str,
        visual_description: str,
        combined_embedding: List[float],
        extracted_fields: Dict[str, Any],
        avg_confidence: float
    ):
        """
        Store document in Azure AI Search index for future RAG
        
        Creates index if doesn't exist, then uploads document
        """
        try:
            from azure.search.documents.indexes import SearchIndexClient
            from azure.search.documents.indexes.models import (
                SearchIndex, SearchField, SearchFieldDataType,
                VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile
            )
            
            # Create index if not exists
            index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=self.search_credential
            )
            
            try:
                index_client.get_index(index_name)
                logger.info(f"  Index '{index_name}' exists")
            except:
                logger.info(f"  Creating index '{index_name}'...")
                
                fields = [
                    SearchField(name="id", type=SearchFieldDataType.String, key=True, filterable=True),
                    SearchField(name="document_name", type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SearchField(name="provider", type=SearchFieldDataType.String, filterable=True),
                    SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="visual_description", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="extracted_fields", type=SearchFieldDataType.String),
                    SearchField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
                    SearchField(
                        name="content_vector",
                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        vector_search_dimensions=3072,
                        vector_search_profile_name="multimodal-profile"
                    )
                ]
                
                vector_search = VectorSearch(
                    algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
                    profiles=[VectorSearchProfile(name="multimodal-profile", algorithm_configuration_name="hnsw-config")]
                )
                
                index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
                index_client.create_index(index)
                logger.info(f"  âœ“ Index '{index_name}' created")
            
            # Upload document
            doc_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            # Generate document ID for Azure AI Search
            # LONG-TERM SOLUTION: Use hash of original filename
            # This way:
            # 1. Original filename preserved in metadata
            # 2. Document ID is always valid (hash is alphanumeric)
            # 3. No future errors from new special characters
            # 4. Consistent and deterministic
            
            import hashlib
            
            # Create hash from provider + document name (deterministic)
            unique_string = f"{provider}_{document_name}"
            hash_value = hashlib.sha256(unique_string.encode()).hexdigest()[:32]  # 32 chars
            
            # Document ID: hash only (always valid!)
            doc_id = f"{provider}_{hash_value}"
            
            # Store original filename separately in document metadata
            # This is returned in search results and stored in JSON
            
            document = {
                "id": doc_id,
                "document_name": document_name,
                "provider": provider,
                "content": document_text[:50000],  # Limit size
                "visual_description": visual_description,
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": avg_confidence,
                "content_vector": combined_embedding
            }
            
            doc_client.upload_documents([document])
            logger.info(f"  âœ“ Stored in index: {doc_id}")
            
        except Exception as e:
            logger.error(f"Failed to store in index: {e}", exc_info=True)

    def _build_rag_prompt(
        self,
        document_text: str,
        visual_description: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build prompt with RAG few-shot examples + visual context"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass

            visual_ctx = doc.get('visual_description', '')
            if visual_ctx:
                parts.append(f"Visual   : {visual_ctx[:150]}")

            parts.append("")

        parts.append("â”€" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")

        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:7000])
        parts.append("")

        if visual_description:
            parts.append("VISUAL ANALYSIS (Supplementary):")
            parts.append(visual_description)
            parts.append("")

        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(
        self,
        document_text: str,
        visual_description: str
    ) -> str:
        """Standard prompt (no RAG) with optional visual context"""
        parts = [
            "Extract the following fields from this document:",
            "",
            "OCR TEXT (Primary Source):",
            document_text[:7000],
            ""
        ]
        if visual_description:
            parts += ["VISUAL ANALYSIS (Supplementary):", visual_description, ""]

        parts += [
            f"Fields to extract: {', '.join(self.fields)}",
            "Return ONLY a JSON object with field values and confidence scores."
        ]
        return "\n".join(parts)

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> tuple[Dict[str, Any], str]:
        """Call GPT-4o and return parsed extraction + system prompt"""
        try:
            # Use prompt_builder to build system prompt from prompt.py
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )
            
            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content), system_prompt

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}, ""

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}

------------


{
  "AzureBlob": {
    "connection_string": "YOUR_BLOB_CONNECTION_STRING",
    "inputcontainer": "inputcontainer",
    "outputcontainer": "outputcontainer"
  },

  "AzureOpenAI": {
    "endpoint": "https://YOUR-GPT.openai.azure.com/",
    "api_key": "YOUR_GPT_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "gpt-4o"
  },

  "AzureEmbedding": {
    "endpoint": "https://YOUR-EMBEDDING.openai.azure.com/",
    "api_key": "YOUR_EMBEDDING_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "text-embedding-3-large",
    "dimension": 3072
  },

  "DocumentIntelligence": {
    "endpoint": "https://YOUR_DOC_INTEL.cognitiveservices.azure.com/",
    "key": "YOUR_DOC_INTEL_KEY"
  },

  "AzureAISearch": {
    "endpoint": "https://YOUR_SEARCH_SERVICE.search.windows.net",
    "api_key": "YOUR_SEARCH_KEY"
  },

  "fields": [
    "name",
    "nationality"
  ],

  "confidence_threshold": 0.50,

  "rag": {
    "mode": "text",
    "top_k": 5,
    "similarity_threshold": 0.70,
    "text_weight": 0.70,
    "visual_weight": 0.30
  },

  "costs": {
    "gpt4o_input_per_1k": 0.0025,
    "gpt4o_output_per_1k": 0.01,
    "embedding_per_1k": 0.00013,
    "doc_intel_per_page": 0.01
  },

  "logging": {
    "log_dir": "logs",
    "log_level": "INFO"
  }
}
