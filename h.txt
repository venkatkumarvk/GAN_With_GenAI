"""
Complete RAG-Enhanced Document Processing with Consolidation
- ONE row per provider with unique ID
- Proper Azure AI Search storage
- Consistent IDs across all outputs
"""

import os
import uuid
import json
import pandas as pd
from datetime import datetime
import logging
from typing import List, Dict, Any
import hashlib

from helper import (ConfigManager, AzureBlobManager, DocumentIntelligenceManager, 
                   AzureOpenAIManager, AzureAISearchManager)
from rag_extraction import RAGExtractor

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def generate_provider_id(provider_name: str, run_timestamp: str) -> str:
    """
    Generate unique, consistent ID for provider
    Format: providername_YYYYMMDD_HHMMSS
    Example: anand_20240211_143022
    """
    # Clean provider name for ID
    clean_name = provider_name.lower().replace(' ', '_').replace('-', '_')
    clean_name = ''.join(c for c in clean_name if c.isalnum() or c == '_')
    
    # Format: providername_YYYYMMDD_HHMMSS
    provider_id = f"{clean_name}_{run_timestamp}"
    
    return provider_id


class DocumentConsolidator:
    """Consolidates multiple documents into one final row per provider"""
    
    @staticmethod
    def consolidate_documents(
        documents: List[Dict[str, Any]], 
        fields: List[str],
        strategy: str = "voting"
    ) -> Dict[str, Any]:
        """
        Consolidate multiple documents into ONE final row
        
        Args:
            documents: List of processed documents
            fields: List of field names to consolidate
            strategy: Consolidation strategy ("voting" or "highest_confidence")
            
        Returns:
            Single consolidated row with best values
        """
        
        if not documents:
            return {}
        
        consolidated = {
            'total_documents_processed': len(documents),
            'document_names': [doc['document_name'] for doc in documents],
            'consolidation_strategy': strategy,
            'document_ids': [doc['id'] for doc in documents]  # Track all doc IDs
        }
        
        # Consolidate each field
        for field in fields:
            field_values = []
            
            # Collect all values for this field across documents
            for doc in documents:
                field_data = doc.get('extracted_fields', {}).get(field, {})
                if isinstance(field_data, dict) and field_data.get('value'):
                    field_values.append({
                        'value': field_data.get('value', ''),
                        'confidence': field_data.get('confidence', 0.0),
                        'source': doc.get('document_name', 'unknown')
                    })
            
            if not field_values:
                # Field not found in any document
                consolidated[field] = ''
                consolidated[f'{field}_confidence'] = 0.0
                consolidated[f'{field}_source_document'] = 'Not found'
                continue
            
            # Apply consolidation strategy
            if strategy == "voting":
                # Group by value and boost confidence if multiple sources agree
                value_groups = {}
                for fv in field_values:
                    val = fv['value']
                    if val not in value_groups:
                        value_groups[val] = []
                    value_groups[val].append(fv)
                
                # Pick value with best score (count Ã— avg_confidence)
                best_value = None
                best_confidence = 0
                best_sources = []
                
                for value, instances in value_groups.items():
                    avg_conf = sum(i['confidence'] for i in instances) / len(instances)
                    # Boost confidence if multiple sources agree
                    boost = (len(instances) - 1) * 0.03  # +3% per additional source
                    boosted_conf = min(0.99, avg_conf + boost)
                    
                    if boosted_conf > best_confidence:
                        best_confidence = boosted_conf
                        best_value = value
                        best_sources = [i['source'] for i in instances]
                
                consolidated[field] = best_value
                consolidated[f'{field}_confidence'] = round(best_confidence, 3)
                consolidated[f'{field}_source_document'] = '|'.join(best_sources)
                
            else:  # highest_confidence
                # Pick the value with highest confidence
                best = max(field_values, key=lambda x: x['confidence'])
                consolidated[field] = best['value']
                consolidated[f'{field}_confidence'] = round(best['confidence'], 3)
                consolidated[f'{field}_source_document'] = best['source']
        
        # Calculate overall statistics
        all_confidences = [
            consolidated.get(f'{field}_confidence', 0.0) 
            for field in fields
            if consolidated.get(f'{field}_confidence', 0.0) > 0
        ]
        
        consolidated['avg_confidence_overall'] = (
            round(sum(all_confidences) / len(all_confidences), 3) 
            if all_confidences else 0.0
        )
        
        # Count extraction methods
        rag_count = sum(1 for doc in documents if 'RAG' in doc.get('extraction_method', ''))
        consolidated['rag_enhanced_count'] = rag_count
        consolidated['standard_extraction_count'] = len(documents) - rag_count
        
        # Total pages
        consolidated['total_pages'] = sum(doc.get('page_count', 0) for doc in documents)
        
        return consolidated


def main(use_rag: bool = True):
    """
    Main processing pipeline with document consolidation and proper Azure AI Search storage
    """
    
    # Load configuration
    cfg = ConfigManager('config.json')
    blob_cfg = cfg.get("AzureBlob")
    docint_cfg = cfg.get("DocumentIntelligence")
    openai_cfg = cfg.get("AzureOpenAI")
    embedding_cfg = cfg.get("AzureEmbedding")
    search_cfg = cfg.get("AzureAISearch")
    fields = cfg.get("fields")
    confidence_threshold = cfg.config.get("confidence_threshold", 0.90)
    costs_cfg = cfg.get("costs")
    
    # RAG configuration
    rag_config = cfg.config.get("RAG", {})
    use_rag_extraction = rag_config.get("enabled", use_rag)
    rag_top_k = rag_config.get("top_k", 3)
    rag_similarity_threshold = rag_config.get("similarity_threshold", 0.70)
    consolidation_strategy = rag_config.get("consolidation_strategy", "voting")
    
    print(f"\n{'='*70}")
    print("RAG-ENHANCED DOCUMENT PROCESSING WITH CONSOLIDATION")
    print(f"{'='*70}")
    print(f"RAG Extraction: {'ENABLED' if use_rag_extraction else 'DISABLED'}")
    print(f"Consolidation Strategy: {consolidation_strategy}")
    print(f"Confidence Threshold: {confidence_threshold}")
    print(f"Fields: {', '.join(fields)}")
    print(f"{'='*70}\n")

    # Initialize managers
    blob_manager = AzureBlobManager(
        blob_cfg['connection_string'],
        blob_cfg['inputcontainer'],
        blob_cfg['outputcontainer']
    )
    
    doc_intel_manager = DocumentIntelligenceManager(
        docint_cfg['endpoint'],
        docint_cfg['key']
    )
    
    openai_manager = AzureOpenAIManager(
        gpt_endpoint=openai_cfg['endpoint'],
        gpt_api_key=openai_cfg['api_key'],
        gpt_api_version=openai_cfg['api_version'],
        gpt_deployment=openai_cfg['deployment_name'],
        embedding_endpoint=embedding_cfg['endpoint'],
        embedding_api_key=embedding_cfg['api_key'],
        embedding_api_version=embedding_cfg['api_version'],
        embedding_deployment=embedding_cfg['deployment_name'],
        embedding_dimension=embedding_cfg['dimension']
    )
    
    search_manager = AzureAISearchManager(
        search_cfg['endpoint'],
        search_cfg['api_key']
    )
    
    # Initialize RAG extractor
    rag_extractor = RAGExtractor(
        search_endpoint=search_cfg['endpoint'],
        search_api_key=search_cfg['api_key'],
        openai_manager=openai_manager,
        fields=fields,
        top_k=rag_top_k,
        similarity_threshold=rag_similarity_threshold,
        use_rag=use_rag_extraction
    ) if use_rag_extraction else None

    # Global summary
    global_summary = {
        'total_providers': 0,
        'total_documents': 0,
        'high_confidence_providers': 0,
        'low_confidence_providers': 0,
        'total_cost': 0.0,
        'providers': {}
    }

    # Generate timestamp
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Storage for consolidated rows
    high_confidence_rows = []
    low_confidence_rows = []

    providers = blob_manager.get_providers()
    print(f"Found {len(providers)} provider(s): {providers}\n")

    for provider in providers:
        print(f"\n{'='*70}")
        print(f"PROCESSING PROVIDER: {provider.upper()}")
        print(f"{'='*70}")
        
        # Generate unique provider ID
        provider_id = generate_provider_id(provider, run_timestamp)
        print(f"Provider ID: {provider_id}")
        
        # Process all documents for this provider
        provider_results = process_provider(
            provider=provider,
            provider_id=provider_id,
            blob_manager=blob_manager,
            doc_intel_manager=doc_intel_manager,
            openai_manager=openai_manager,
            search_manager=search_manager,
            rag_extractor=rag_extractor,
            fields=fields,
            embedding_cfg=embedding_cfg,
            use_rag=use_rag_extraction,
            run_timestamp=run_timestamp  # Pass timestamp for unique index
        )
        
        if not provider_results['documents']:
            print(f"  âŠ˜ No documents successfully processed for {provider}")
            continue
        
        # CONSOLIDATE all documents into ONE row
        consolidated_row = DocumentConsolidator.consolidate_documents(
            documents=provider_results['documents'],
            fields=fields,
            strategy=consolidation_strategy
        )
        
        # Add provider info with unique ID
        consolidated_row['provider_id'] = provider_id  # Unique ID
        consolidated_row['provider'] = provider
        consolidated_row['extraction_datetime'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Determine if high or low confidence
        avg_conf = consolidated_row.get('avg_confidence_overall', 0.0)
        
        if avg_conf >= confidence_threshold:
            high_confidence_rows.append(consolidated_row)
            category = "HIGH CONFIDENCE"
            global_summary['high_confidence_providers'] += 1
        else:
            low_confidence_rows.append(consolidated_row)
            category = "LOW CONFIDENCE"
            global_summary['low_confidence_providers'] += 1
        
        # Update summary
        global_summary['total_providers'] += 1
        global_summary['total_documents'] += consolidated_row['total_documents_processed']
        global_summary['providers'][provider] = {
            'provider_id': provider_id,
            'documents': consolidated_row['total_documents_processed'],
            'avg_confidence': avg_conf,
            'category': category
        }
        
        print(f"\nâœ“ Provider '{provider}' consolidated:")
        print(f"  - Provider ID: {provider_id}")
        print(f"  - Documents Processed: {consolidated_row['total_documents_processed']}")
        print(f"  - Average Confidence: {avg_conf:.3f}")
        print(f"  - Category: {category}")
        print(f"  - RAG Enhanced: {consolidated_row['rag_enhanced_count']}")
        print(f"  - Standard: {consolidated_row['standard_extraction_count']}")
        
        # STORE CONSOLIDATED ROW IN AZURE AI SEARCH
        store_consolidated_in_search(
            provider=provider,
            provider_id=provider_id,
            consolidated_row=consolidated_row,
            documents=provider_results['documents'],
            search_manager=search_manager,
            fields=fields,
            run_timestamp=run_timestamp  # Pass timestamp
        )
        
        # SAVE PROVIDER-SPECIFIC COST TRACKING
        save_provider_costs(
            provider=provider,
            provider_id=provider_id,
            total_documents=consolidated_row['total_documents_processed'],
            openai_manager=openai_manager,
            costs_cfg=costs_cfg,
            blob_manager=blob_manager
        )
    
    # Save consolidated results
    save_consolidated_results(
        high_confidence_rows=high_confidence_rows,
        low_confidence_rows=low_confidence_rows,
        fields=fields,
        blob_manager=blob_manager,
        run_timestamp=run_timestamp
    )
    
    # Calculate costs
    total_cost_info = openai_manager.calculate_cost(costs_cfg)
    global_summary['total_cost'] = total_cost_info['total_cost']
    global_summary['cost_breakdown'] = total_cost_info
    
    # Save global summary
    save_global_summary(blob_manager, global_summary, run_timestamp)
    
    # Print final summary
    print_final_summary(global_summary, high_confidence_rows, low_confidence_rows)


def process_provider(
    provider, provider_id, blob_manager, doc_intel_manager, openai_manager,
    search_manager, rag_extractor, fields, embedding_cfg, use_rag, run_timestamp
):
    """Process all documents for a provider"""
    
    files = blob_manager.get_provider_files(provider)
    print(f"Found {len(files)} files for provider '{provider}'")
    
    # Create vector index with timestamp for unique index per run
    try:
        index_name = search_manager.create_index(provider, embedding_cfg['dimension'], run_timestamp)
        print(f"âœ“ Vector index ready: '{index_name}'")
        vector_index_success = True
    except Exception as e:
        print(f"âœ— Vector index failed: {e}")
        logger.error(f"Index creation error: {e}", exc_info=True)
        vector_index_success = False
    
    # Process each document
    processed_documents = []
    
    for idx, file in enumerate(files, 1):
        print(f"\n  [{idx}/{len(files)}] Processing: {file['filename']}")
        
        result = process_single_document(
            file=file,
            provider=provider,
            provider_id=provider_id,
            blob_manager=blob_manager,
            doc_intel_manager=doc_intel_manager,
            openai_manager=openai_manager,
            search_manager=search_manager,
            rag_extractor=rag_extractor,
            fields=fields,
            embedding_cfg=embedding_cfg,
            vector_index_success=vector_index_success,
            use_rag=use_rag,
            run_timestamp=run_timestamp  # Pass timestamp
        )
        
        if result:
            processed_documents.append(result)
    
    return {'documents': processed_documents}


def process_single_document(
    file, provider, provider_id, blob_manager, doc_intel_manager, openai_manager,
    search_manager, rag_extractor, fields, embedding_cfg,
    vector_index_success, use_rag, run_timestamp
):
    """Process a single document"""
    
    doc_id = str(uuid.uuid4())
    blob_name = file['name']
    filename = file['filename']
    
    # Step 1: OCR
    try:
        base64_data = blob_manager.download_blob_as_base64(blob_name)
        ocr_result = doc_intel_manager.analyze_document(base64_data, file['extension'])
        
        if not ocr_result['success']:
            print(f"    âœ— OCR failed")
            return None
        
        text_content = ocr_result['text']
        print(f"    âœ“ OCR: {ocr_result['page_count']} pages, {len(text_content)} chars")
    except Exception as e:
        print(f"    âœ— OCR error: {e}")
        logger.error(f"OCR error for {filename}: {e}", exc_info=True)
        return None
    
    if not text_content or len(text_content.strip()) < 50:
        print(f"    âŠ˜ Insufficient text")
        return None
    
    # Step 2: Field Extraction
    try:
        if use_rag and rag_extractor:
            extraction_result = rag_extractor.extract_with_rag(
                document_text=text_content,
                provider=provider,
                source_document=filename,
                document_type=None
            )
            
            method = extraction_result.get('extraction_method', 'Unknown')
            similar_count = extraction_result.get('similar_docs_count', 0)
            
            if 'RAG' in method:
                print(f"    âœ“ RAG Extraction: {similar_count} similar docs")
            else:
                print(f"    âœ“ Standard Extraction")
        else:
            extraction_result = openai_manager.extract_fields(text_content, fields, filename)
            extraction_result['extraction_method'] = 'Standard (no RAG)'
            extraction_result['similar_docs_count'] = 0
            print(f"    âœ“ Standard Extraction")
        
        extracted_fields = extraction_result.get('extracted_fields', {})
        
    except Exception as e:
        print(f"    âœ— Extraction error: {e}")
        logger.error(f"Extraction error for {filename}: {e}", exc_info=True)
        return None
    
    # Step 3: Generate Embeddings
    try:
        embedding_vector = openai_manager.generate_embeddings(text_content)
        if embedding_vector and len(embedding_vector) == embedding_cfg['dimension']:
            print(f"    âœ“ Embeddings: {len(embedding_vector)} dims")
        else:
            print(f"    âš  Embeddings: unexpected dimension")
            embedding_vector = [0.0] * embedding_cfg['dimension']
    except Exception as e:
        print(f"    âœ— Embedding error: {e}")
        logger.error(f"Embedding error for {filename}: {e}", exc_info=True)
        embedding_vector = [0.0] * embedding_cfg['dimension']
    
    # Step 4: Upload INDIVIDUAL DOCUMENT to Vector Database
    if vector_index_success and embedding_vector and any(v != 0.0 for v in embedding_vector):
        try:
            search_doc = {
                "id": doc_id,
                "provider_id": provider_id,
                "provider": provider,
                "document_name": filename,
                "document_type": None,  # Individual document (not consolidated)
                "file_extension": file['extension'],
                "content": text_content[:50000],
                "page_count": ocr_result.get('page_count', 0),
                "total_documents": None,  # Only for consolidated docs
                "extraction_datetime": datetime.utcnow().isoformat(),
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": None,  # Only for consolidated docs
                "content_vector": embedding_vector
            }
            
            # Upload to search
            search_manager.upload_documents(provider, [search_doc], run_timestamp)
            print(f"    âœ“ Uploaded to search index")
            
        except Exception as e:
            print(f"    âœ— Search upload failed: {e}")
            logger.error(f"Search upload error for {filename}: {e}", exc_info=True)
    else:
        if not vector_index_success:
            print(f"    âŠ˜ Skipped search upload (index not available)")
        else:
            print(f"    âŠ˜ Skipped search upload (no valid embeddings)")
    
    return {
        'id': doc_id,
        'provider': provider,
        'provider_id': provider_id,
        'document_name': filename,
        'file_extension': file['extension'],
        'page_count': ocr_result.get('page_count', 0),
        'content': text_content,
        'extracted_fields': extracted_fields,
        'extraction_method': extraction_result.get('extraction_method', 'Unknown'),
        'similar_docs_used': extraction_result.get('similar_docs_count', 0),
        'embeddings': embedding_vector
    }


def store_consolidated_in_search(
    provider, provider_id, consolidated_row, documents,
    search_manager, fields, run_timestamp
):
    """
    Store the CONSOLIDATED row in Azure AI Search as a summary document
    This is in addition to individual documents already uploaded
    """
    
    try:
        # Create consolidated content (summary of all documents)
        all_content = "\n\n--- DOCUMENT BREAK ---\n\n".join([
            f"Document: {doc['document_name']}\n{doc['content'][:5000]}"
            for doc in documents
        ])
        
        # Create average embedding from all document embeddings
        if documents and all('embeddings' in doc for doc in documents):
            all_embeddings = [doc['embeddings'] for doc in documents]
            # Average the embeddings
            avg_embedding = [
                sum(emb[i] for emb in all_embeddings) / len(all_embeddings)
                for i in range(len(all_embeddings[0]))
            ]
        else:
            avg_embedding = [0.0] * 3072  # Default
        
        # Create consolidated search document
        consolidated_search_doc = {
            "id": f"{provider_id}-consolidated",  # Use provider_id format
            "provider_id": provider_id,
            "provider": provider,
            "document_name": f"{provider}-consolidated",
            "document_type": "consolidated",  # Mark as consolidated
            "file_extension": None,  # Not applicable for consolidated
            "content": all_content[:50000],
            "page_count": sum(doc.get('page_count', 0) for doc in documents),
            "total_documents": consolidated_row['total_documents_processed'],
            "extraction_datetime": datetime.utcnow().isoformat(),
            "extracted_fields": json.dumps({
                field: {
                    'value': consolidated_row.get(field, ''),
                    'confidence': consolidated_row.get(f'{field}_confidence', 0.0),
                    'source': consolidated_row.get(f'{field}_source_document', '')
                }
                for field in fields
            }),
            "avg_confidence": consolidated_row.get('avg_confidence_overall', 0.0),
            "content_vector": avg_embedding
        }
        
        # Upload consolidated document with timestamp
        search_manager.upload_documents(provider, [consolidated_search_doc], run_timestamp)
        print(f"    âœ“ Stored consolidated row in search index")
        
    except Exception as e:
        print(f"    âœ— Failed to store consolidated row in search: {e}")
        logger.error(f"Consolidated search storage error: {e}", exc_info=True)


def save_provider_costs(
    provider, provider_id, total_documents,
    openai_manager, costs_cfg, blob_manager
):
    """
    Save provider-specific cost tracking
    Filename: providername_datetime_costs.json
    """
    
    try:
        token_usage = openai_manager.get_token_usage()
        cost_info = openai_manager.calculate_cost(costs_cfg)
        
        # Estimate costs per provider (proportional to token usage)
        cost_data = {
            'provider_id': provider_id,
            'provider': provider,
            'total_documents': total_documents,
            'costs': {
                'gpt4o_cost': cost_info['total_cost'],
                'ocr_cost': total_documents * costs_cfg.get('doc_intel_per_page', 0.01) * 2,  # Estimate 2 pages/doc
                'embedding_cost': token_usage.get('total_tokens', 0) * costs_cfg.get('embedding_per_1k', 0.00013) / 1000,
                'total_estimated': cost_info['total_cost'] + (total_documents * 0.02)
            },
            'usage': {
                'total_tokens': token_usage.get('total_tokens', 0),
                'prompt_tokens': token_usage.get('prompt_tokens', 0),
                'completion_tokens': token_usage.get('completion_tokens', 0)
            },
            'cost_breakdown': cost_info
        }
        
        # Save with provider_id as filename: providername_datetime_costs.json
        cost_path = f"CostTracking/{provider_id}_costs.json"
        blob_manager.upload_to_blob(json.dumps(cost_data, indent=2), cost_path, 'application/json')
        print(f"    âœ“ Cost tracking: {cost_path}")
        
    except Exception as e:
        print(f"    âœ— Cost tracking failed: {e}")
        logger.error(f"Cost tracking error for {provider}: {e}", exc_info=True)


def save_consolidated_results(
    high_confidence_rows, low_confidence_rows, fields,
    blob_manager, run_timestamp
):
    """Save consolidated results with provider_id based filenames"""
    
    print(f"\n{'='*70}")
    print("SAVING CONSOLIDATED RESULTS")
    print(f"{'='*70}")
    
    # Define column order - provider_id FIRST
    base_columns = ['provider_id', 'provider', 'extraction_datetime', 'total_documents_processed']
    
    # Field columns (field, field_confidence, field_source_document)
    field_columns = []
    for field in fields:
        field_columns.extend([field, f'{field}_confidence', f'{field}_source_document'])
    
    # Metadata columns
    meta_columns = [
        'avg_confidence_overall', 'rag_enhanced_count', 'standard_extraction_count',
        'total_pages', 'consolidation_strategy'
    ]
    
    all_columns = base_columns + field_columns + meta_columns
    
    # Save High Confidence - ONE FILE PER PROVIDER
    if high_confidence_rows:
        for row in high_confidence_rows:
            provider_id = row['provider_id']
            
            # CSV - filename: providername_datetime.csv
            high_df = pd.DataFrame([row])
            high_df = high_df[[col for col in all_columns if col in high_df.columns]]
            
            csv_path = f"HighConfidence/processedcsvresult/{provider_id}.csv"
            blob_manager.upload_dataframe_as_csv(high_df, csv_path)
            print(f"âœ“ High Confidence CSV: {csv_path}")
            
            # JSON - filename: providername_datetime.json
            json_data = {
                'provider_id': provider_id,
                'timestamp': run_timestamp,
                'confidence_level': 'high',
                'provider_data': row
            }
            json_path = f"HighConfidence/processedjsonresult/{provider_id}.json"
            blob_manager.upload_to_blob(json.dumps(json_data, indent=2), json_path, 'application/json')
            print(f"âœ“ High Confidence JSON: {json_path}")
    
    # Save Low Confidence - ONE FILE PER PROVIDER
    if low_confidence_rows:
        for row in low_confidence_rows:
            provider_id = row['provider_id']
            
            # CSV
            low_df = pd.DataFrame([row])
            low_df = low_df[[col for col in all_columns if col in low_df.columns]]
            
            csv_path = f"LowConfidence/processedcsvresult/{provider_id}.csv"
            blob_manager.upload_dataframe_as_csv(low_df, csv_path)
            print(f"âœ“ Low Confidence CSV: {csv_path}")
            
            # JSON
            json_data = {
                'provider_id': provider_id,
                'timestamp': run_timestamp,
                'confidence_level': 'low',
                'provider_data': row
            }
            json_path = f"LowConfidence/processedjsonresult/{provider_id}.json"
            blob_manager.upload_to_blob(json.dumps(json_data, indent=2), json_path, 'application/json')
            print(f"âœ“ Low Confidence JSON: {json_path}")


def save_global_summary(blob_manager, global_summary, run_timestamp):
    """Save global summary"""
    
    summary_path = f"CostTracking/global_summary_{run_timestamp}.json"
    blob_manager.upload_to_blob(json.dumps(global_summary, indent=2), summary_path, 'application/json')
    print(f"âœ“ Global summary: {summary_path}")


def print_final_summary(global_summary, high_conf_rows, low_conf_rows):
    """Print final summary"""
    
    print(f"\n{'='*70}")
    print("FINAL SUMMARY")
    print(f"{'='*70}")
    print(f"Total Providers:        {global_summary['total_providers']}")
    print(f"Total Documents:        {global_summary['total_documents']}")
    print(f"")
    print(f"High Confidence:        {len(high_conf_rows)} provider(s)")
    print(f"Low Confidence:         {len(low_conf_rows)} provider(s)")
    print(f"")
    print(f"Total Cost:             ${global_summary['total_cost']:.4f}")
    print(f"{'='*70}")
    print("\nâœ“ Processing Complete!")
    print(f"\nðŸ“Š Azure AI Search Status:")
    print(f"  - Each provider has its own index")
    print(f"  - Individual documents + consolidated summary stored")
    print(f"  - Check Azure Portal for document counts")


if __name__ == "__main__":
    import sys
    
    use_rag = True
    if len(sys.argv) > 1:
        use_rag = sys.argv[1].lower() in ['true', '1', 'yes', 'rag']
    
    print(f"Starting pipeline with RAG={'ENABLED' if use_rag else 'DISABLED'}")
    main(use_rag=use_rag)
