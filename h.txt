from openai import AzureOpenAI
import csv
import json
import logging
import re
from typing import List, Dict, OrderedDict, Optional, Generator
from collections import OrderedDict
from pathlib import Path

# Logging Configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ImprovedJSONConsolidator:
    """
    Advanced JSON to CSV consolidator with LLM-based intelligent merging.
    
    Features:
    - Handles nested JSON structures
    - Preserves all _Confidence values (no deduplication)
    - Removes duplicate values for regular fields
    - Uses Azure OpenAI for intelligent consolidation
    - Robust error handling and validation
    """
    
    def __init__(
        self,
        azure_endpoint: str,
        api_key: str,
        deployment_name: str,
        api_version: str = "2023-05-15"
    ):
        """Initialize the consolidator with Azure OpenAI credentials."""
        self.client = AzureOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=api_key,
            api_version=api_version,
        )
        self.deployment_name = deployment_name
        logger.info("JSON Consolidator initialized successfully")

    def read_json(self, path: str) -> List[OrderedDict]:
        """
        Reads input JSON file, extracts and flattens the data.
        
        Args:
            path: Path to JSON file
            
        Returns:
            List of flattened row dictionaries
            
        Raises:
            ValueError: If JSON is not properly formatted
        """
        logger.info(f"Reading JSON from: {path}")
        
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f, object_pairs_hook=OrderedDict)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON format: {e}")
        except FileNotFoundError:
            raise FileNotFoundError(f"File not found: {path}")
        
        # Check if the main JSON is a dictionary and contains the 'Pages' key
        if isinstance(data, dict):
            flattened_rows = []
            
            if "Pages" in data:
                pages = data["Pages"]
                if not pages or not isinstance(pages, list):
                    raise ValueError("The 'Pages' key is empty or does not contain a list.")
                
                # Flatten the root-level fields (e.g., Filename, Extraction_Timestamp)
                root_fields = OrderedDict({k: v for k, v in data.items() if k != "Pages"})
                
                # Flatten each page and combine with root-level fields
                for page in pages:
                    flattened_page = self.flatten_extracted_data(page)
                    flattened_page.update(root_fields)
                    flattened_rows.append(flattened_page)
            else:
                # If no 'Pages' key, treat the root fields as the only row
                flattened_rows.append(data)
            
            logger.info(f"Read and flattened {len(flattened_rows)} rows from JSON")
            return flattened_rows
        else:
            raise ValueError("Input JSON is not properly formatted as a dictionary.")

    def flatten_extracted_data(self, page: Dict) -> OrderedDict:
        """
        Flattens the 'ExtractedData' field in a single page dictionary.
        
        Args:
            page: Dictionary representing a single page
            
        Returns:
            Flattened OrderedDict with all fields at root level
        """
        flattened = OrderedDict({
            "Page": page.get("Page"),
            "Category": page.get("Category"),
        })
        
        extracted_data = page.get("ExtractedData", {})
        
        for key, value in extracted_data.items():
            if isinstance(value, dict):
                # Extract the 'value' field
                flattened[key] = value.get("value", "NA")
                
                # Preserve '_Confidence' values as a comma-separated list
                confidence_value = value.get("confidence", None)
                if confidence_value is not None:
                    flattened[f"{key}_Confidence"] = confidence_value
            else:
                # If the field is not a dictionary, just copy it
                flattened[key] = value
        
        return flattened

    def build_prompt(self, rows: List[Dict]) -> str:
        """
        Builds a comprehensive prompt for the LLM to consolidate JSON data.
        
        Args:
            rows: List of row dictionaries to consolidate
            
        Returns:
            Formatted prompt string
        """
        rows_json = json.dumps(rows, indent=2, ensure_ascii=False)
        
        return f"""You are an expert data consolidation AI.

TASK: Input is a JSON array where each object represents a row of data. Consolidate all rows into ONE single row.

CRITICAL RULES:
1. Keep identical values only once (remove duplicates).
2. For fields ending with "_Confidence", KEEP ALL VALUES including duplicates - do NOT remove any confidence values.
3. Merge differing values intelligently:
   - Use "|" separator for multiple distinct values
   - Prioritize values with higher confidence scores when available
4. Merge line items so each unique line appears once.
5. Do not lose ANY information from the input.
6. Do not duplicate information unnecessarily (EXCEPT for _Confidence fields).
7. Use EXACT SAME column names as input.
8. Preserve the order of fields exactly as they appear in the input JSON.
9. Output must be VALID JSON ONLY - no markdown, no explanation, no preamble.
10. Output must represent ONE consolidated row as a single JSON object.
11. For "_Confidence" fields: preserve ALL values exactly, even if duplicates exist (e.g., "99,1.0,99,98" stays as is).
12. For regular fields: merge unique values with "|" separator (e.g., "value1|value2").

EXAMPLE:
Input: [{{"Name": "John", "Age": "30", "Age_Confidence": "99,98"}}, {{"Name": "John", "Age": "30", "Age_Confidence": "99,98"}}]
Output: {{"Name": "John", "Age": "30", "Age_Confidence": "99,98,99,98"}}

INPUT DATA:
{rows_json}

OUTPUT: Return only a single JSON object (no markdown, no explanation):"""

    def consolidate(self, rows: List[Dict]) -> OrderedDict:
        """
        Sends data to the Azure OpenAI API for consolidation.
        
        Args:
            rows: List of dictionaries to consolidate
            
        Returns:
            Single consolidated OrderedDict
            
        Raises:
            Exception: If API call fails
        """
        if not rows:
            logger.warning("No rows to consolidate")
            return OrderedDict()
        
        if len(rows) == 1:
            logger.info("Only one row - returning as-is")
            return OrderedDict(rows[0])
        
        prompt = self.build_prompt(rows)
        
        try:
            logger.info(f"Sending data to LLM for consolidation ({len(rows)} rows)...")
            
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {
                        "role": "system",
                        "content": "You only return valid JSON. No explanations."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    },
                ],
                temperature=0.0,  # Deterministic for data processing
                max_tokens=4000,
            )
            
            text = response.choices[0].message.content.strip()
            logger.info("Received response from LLM")
            
            # Parse and validate the response
            consolidated = self._parse_json_response(text)
            
            # Validate the consolidation
            if self._validate_consolidation(rows, consolidated):
                return consolidated
            else:
                logger.warning("Validation failed, using fallback consolidation")
                return self._fallback_consolidate(rows)
                
        except Exception as e:
            logger.error(f"Error during API call: {e}")
            logger.info("Using fallback consolidation method")
            return self._fallback_consolidate(rows)

    def _parse_json_response(self, text: str) -> OrderedDict:
        """
        Parse JSON from LLM response with robust error handling.
        
        Args:
            text: Raw text response from LLM
            
        Returns:
            Parsed OrderedDict
            
        Raises:
            ValueError: If JSON cannot be parsed
        """
        # Remove markdown code blocks if present
        if text.startswith("```"):
            lines = text.split("\n")
            # Remove first and last lines (markdown fences)
            if len(lines) > 2:
                text = "\n".join(lines[1:-1] if lines[-1].strip() == "```" else lines[1:])
        
        # Remove "json" language identifier
        text = text.replace("```json", "").replace("```", "").strip()
        
        # Try to extract JSON object using regex
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            text = json_match.group()
        
        try:
            return json.loads(text, object_pairs_hook=OrderedDict)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            logger.debug(f"Problematic text: {text[:500]}...")
            raise ValueError(f"Invalid JSON in response: {e}")

    def _validate_consolidation(self, original_rows: List[Dict], consolidated: OrderedDict) -> bool:
        """
        Validate that consolidation didn't lose critical fields.
        
        Args:
            original_rows: Original input rows
            consolidated: Consolidated output
            
        Returns:
            True if validation passes, False otherwise
        """
        # Collect all unique keys from original rows
        original_keys = set()
        for row in original_rows:
            original_keys.update(row.keys())
        
        consolidated_keys = set(consolidated.keys())
        
        # Check for missing fields
        missing_keys = original_keys - consolidated_keys
        if missing_keys:
            logger.warning(f"Missing fields after consolidation: {missing_keys}")
            return False
        
        # Check that consolidated result is not empty
        if not consolidated:
            logger.error("Consolidated result is empty")
            return False
        
        logger.info("Validation passed - all fields preserved")
        return True

    def _fallback_consolidate(self, rows: List[Dict]) -> OrderedDict:
        """
        Fallback consolidation method if LLM fails.
        Uses simple rule-based merging.
        
        Args:
            rows: List of dictionaries to consolidate
            
        Returns:
            Consolidated OrderedDict
        """
        logger.info("Using fallback consolidation method")
        
        if not rows:
            return OrderedDict()
        
        # Start with the first row
        consolidated = OrderedDict(rows[0])
        
        # Merge subsequent rows
        for row in rows[1:]:
            for key, value in row.items():
                if key not in consolidated:
                    consolidated[key] = value
                else:
                    # For _Confidence fields, ALWAYS concatenate (including duplicates)
                    if key.endswith("_Confidence"):
                        consolidated[key] = f"{consolidated[key]},{value}"
                    elif consolidated[key] != value:
                        # For regular fields, use "|" separator for unique values only
                        existing_values = str(consolidated[key]).split("|")
                        if str(value) not in existing_values:
                            consolidated[key] = f"{consolidated[key]}|{value}"
        
        return consolidated

    def write_csv(self, path: str, row: OrderedDict):
        """
        Writes a single row of data (dictionary) to a CSV file.
        
        Args:
            path: Output CSV file path
            row: Dictionary to write
        """
        logger.info(f"Writing output to: {path}")
        
        try:
            with open(path, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=row.keys())
                writer.writeheader()
                writer.writerow(row)
            
            logger.info(f"Output written successfully to {path}")
        except IOError as e:
            logger.error(f"Failed to write CSV: {e}")
            raise

    def process(self, input_json: str, output_csv: str) -> OrderedDict:
        """
        Main function to read JSON, consolidate rows, and write to CSV.
        
        Args:
            input_json: Path to input JSON file
            output_csv: Path to output CSV file
            
        Returns:
            Consolidated row dictionary
        """
        logger.info("=" * 60)
        logger.info("Starting JSON to CSV consolidation")
        logger.info("=" * 60)
        
        try:
            # Read and flatten JSON
            rows = self.read_json(input_json)
            
            # Consolidate rows using LLM
            consolidated = self.consolidate(rows)
            
            # Write to CSV
            self.write_csv(output_csv, consolidated)
            
            logger.info("=" * 60)
            logger.info("Consolidation complete!")
            logger.info("=" * 60)
            
            return consolidated
            
        except Exception as e:
            logger.error(f"Processing failed: {e}")
            raise


def main():
    """
    Example usage of the ImprovedJSONConsolidator.
    """
    # Configuration
    AZURE_ENDPOINT = "your-azure-endpoint"
    API_KEY = "your-api-key"
    DEPLOYMENT_NAME = "your-deployment-name"
    API_VERSION = "2023-05-15"
    
    # File paths
    INPUT_JSON = "in_4.json"
    OUTPUT_CSV = "output_consolidated.csv"
    
    try:
        # Initialize consolidator
        consolidator = ImprovedJSONConsolidator(
            azure_endpoint=AZURE_ENDPOINT,
            api_key=API_KEY,
            deployment_name=DEPLOYMENT_NAME,
            api_version=API_VERSION
        )
        
        # Process JSON to CSV
        result = consolidator.process(INPUT_JSON, OUTPUT_CSV)
        
        # Print summary
        print("\n" + "=" * 60)
        print("CONSOLIDATION SUMMARY")
        print("=" * 60)
        print(f"Total fields: {len(result)}")
        print(f"Output file: {OUTPUT_CSV}")
        print("\nSample of consolidated data:")
        for i, (key, value) in enumerate(list(result.items())[:10]):
            print(f"  {key}: {value}")
        if len(result) > 10:
            print(f"  ... and {len(result) - 10} more fields")
        
    except Exception as e:
        logger.error(f"Application error: {e}")
        raise


if __name__ == "__main__":
    main()
