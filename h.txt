"""
MULTIMODAL RAG EXTRACTION  
==========================
Vision + Text RAG extraction
Uses GPT-4o Vision to analyze document images + RAG examples
"""

import json
import logging
import base64
from typing import List, Dict, Any, Optional
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class MultimodalRAGExtractor:
    """
    Multimodal RAG extractor
    Uses GPT-4o Vision + OCR text + RAG examples
    Better for poor quality scans
    """
    
    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        blob_manager,
        fields: List[str],
        top_k: int = 5,
        similarity_threshold: float = 0.70
    ):
        self.search_endpoint = search_endpoint
        self.search_credential = AzureKeyCredential(search_api_key)
        self.openai_manager = openai_manager
        self.blob_manager = blob_manager
        self.fields = fields
        self.top_k = top_k
        self.similarity_threshold = similarity_threshold
        
        # Initialize prompt builder
        self.prompt_builder = ExtractionPromptBuilder(fields)
        
        logger.info(f"MultimodalRAGExtractor initialized | top_k={top_k}")
    
    def extract_with_rag(
        self,
        document_text: str,
        provider: str,
        source_document: str,
        index_name: str = None,
        blob_path: str = None,
        document_type: Optional[str] = "passport"
    ) -> Dict[str, Any]:
        """
        Extract fields using multimodal RAG
        
        Args:
            document_text: OCR text
            provider: Provider name
            source_document: Document filename
            index_name: Specific index name to use
            blob_path: Path to document in blob storage
            document_type: Document type hint
            
        Returns:
            Extraction result with metadata
        """
        logger.info(f"Multimodal RAG extraction | doc={source_document}")
        
        # Step 1: Get document image
        try:
            image_data = self._get_document_image(blob_path)
            if not image_data:
                logger.warning("Failed to get image, falling back to text-only")
                return self._extract_text_only(
                    document_text, provider, source_document, document_type
                )
        except Exception as e:
            logger.error(f"Image retrieval failed: {e}")
            return self._extract_text_only(
                document_text, provider, source_document, document_type
            )
        
        # Step 2: Generate embedding
        try:
            query_vector = self.openai_manager.generate_embeddings(document_text)
            logger.info(f"Generated embedding | dim={len(query_vector)}")
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
            return self._extract_text_only(
                document_text, provider, source_document, document_type
            )
        
        # Step 3: Search similar documents
        similar_docs = []
        try:
            similar_docs = self._search_similar_documents(
                provider=provider,
                query_vector=query_vector,
                top_k=self.top_k,
                index_name=index_name
            )
            
            # Filter by threshold
            similar_docs = [
                doc for doc in similar_docs
                if doc.get('@search.score', 0) >= self.similarity_threshold
            ]
            
            logger.info(f"Found {len(similar_docs)} similar docs")
            
        except Exception as e:
            logger.warning(f"Search failed: {e}")
            similar_docs = []
        
        # Step 4: Build prompts
        system_prompt = self.prompt_builder.build_system_prompt(
            document_type=document_type
        )
        
        if similar_docs:
            text_prompt = self.prompt_builder.build_extraction_prompt_with_rag(
                document_text=document_text,
                similar_documents=similar_docs,
                top_k=min(len(similar_docs), self.top_k)
            )
            method = "Multimodal RAG"
        else:
            text_prompt = self.prompt_builder.build_extraction_prompt_without_rag(
                document_text=document_text
            )
            method = "Multimodal Standard"
        
        logger.info(f"Using method: {method}")
        
        # Step 5: Extract with GPT-4o Vision
        try:
            result = self._call_vision_api(
                system_prompt=system_prompt,
                text_prompt=text_prompt,
                image_data=image_data,
                source_document=source_document
            )
            
            result['extraction_method'] = method
            result['similar_docs_count'] = len(similar_docs)
            result['used_rag'] = len(similar_docs) > 0
            result['has_vision'] = True
            
            return result
            
        except Exception as e:
            logger.error(f"Vision extraction failed: {e}")
            return {
                'success': False,
                'extracted_fields': {},
                'error': str(e),
                'extraction_method': method
            }
    
    def _get_document_image(self, blob_path: str) -> Optional[str]:
        """Get base64-encoded image from blob storage with document conversion"""
        try:
            # Download from blob
            blob_data = self.blob_manager.download_blob_as_base64(blob_path)
            file_ext = blob_path.lower()
            
            # Check if document needs conversion (PDF, DOC, DOCX)
            if file_ext.endswith('.pdf'):
                logger.info(f"Converting PDF to PNG: {blob_path}")
                base64_image = self._convert_pdf_to_image(blob_data)
                
            elif file_ext.endswith(('.doc', '.docx')):
                logger.info(f"Converting Word document to PNG: {blob_path}")
                base64_image = self._convert_word_to_image(blob_data, file_ext)
                
            elif file_ext.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                # Already an image - use directly!
                logger.info(f"Using image directly: {blob_path}")
                base64_image = blob_data
                
            else:
                logger.warning(f"Unsupported file type: {blob_path}")
                return None
            
            # Validate base64 string
            if not base64_image or len(base64_image) < 100:
                logger.error(f"Invalid base64: too short ({len(base64_image)} chars)")
                return None
            
            logger.info(f"Image ready | size={len(base64_image)} chars")
            return base64_image
            
        except Exception as e:
            logger.error(f"Failed to get/convert image: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return None
    
    def _convert_pdf_to_image(self, pdf_base64: str) -> str:
        """Convert PDF to PNG image"""
        import base64
        import io
        from PIL import Image
        import fitz  # PyMuPDF
        
        # Decode base64 to bytes
        pdf_bytes = base64.b64decode(pdf_base64)
        
        # Open PDF
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        
        # Convert first page to image with high resolution
        page = pdf_document[0]
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom
        
        # Convert to PIL Image
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Convert to PNG base64 (lossless, better quality than JPEG)
        buffer = io.BytesIO()
        img.save(buffer, format="PNG", optimize=True)
        base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        pdf_document.close()
        logger.info(f"PDF converted to PNG | size={len(base64_image)} chars")
        return base64_image
    
    def _convert_word_to_image(self, doc_base64: str, file_ext: str) -> str:
        """Convert Word document (DOC/DOCX) to PNG image"""
        import base64
        import io
        from PIL import Image
        import docx2pdf
        import fitz  # PyMuPDF
        import tempfile
        import os
        
        # Decode base64 to bytes
        doc_bytes = base64.b64decode(doc_base64)
        
        # Create temp files
        with tempfile.TemporaryDirectory() as temp_dir:
            # Save Word document
            if file_ext.endswith('.docx'):
                doc_path = os.path.join(temp_dir, "document.docx")
            else:
                doc_path = os.path.join(temp_dir, "document.doc")
            
            with open(doc_path, 'wb') as f:
                f.write(doc_bytes)
            
            # Convert Word to PDF first (docx2pdf works well)
            pdf_path = os.path.join(temp_dir, "document.pdf")
            
            try:
                # Try using docx2pdf (Windows/Mac)
                import docx2pdf
                docx2pdf.convert(doc_path, pdf_path)
            except:
                # Fallback: Use python-docx to extract text and create simple PDF
                logger.warning("docx2pdf not available, using fallback method")
                from docx import Document
                import reportlab
                from reportlab.lib.pagesizes import letter
                from reportlab.pdfgen import canvas
                
                # Read Word document
                doc = Document(doc_path)
                
                # Create simple PDF with text
                c = canvas.Canvas(pdf_path, pagesize=letter)
                y = 750
                for para in doc.paragraphs:
                    if para.text.strip():
                        c.drawString(50, y, para.text[:80])  # Truncate long lines
                        y -= 20
                        if y < 50:  # New page if needed
                            c.showPage()
                            y = 750
                c.save()
            
            # Now convert PDF to PNG
            with open(pdf_path, 'rb') as f:
                pdf_bytes = f.read()
            
            # Open PDF
            pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
            
            # Convert first page to image
            page = pdf_document[0]
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
            
            # Convert to PIL Image
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # Convert to PNG base64
            buffer = io.BytesIO()
            img.save(buffer, format="PNG", optimize=True)
            base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            pdf_document.close()
            logger.info(f"Word document converted to PNG | size={len(base64_image)} chars")
            return base64_image
    
    def _search_similar_documents(
        self,
        provider: str,
        query_vector: List[float],
        top_k: int,
        index_name: str = None
    ) -> List[Dict[str, Any]]:
        """Search for similar documents"""
        
        # Use provided index_name OR build from provider
        if not index_name:
            index_name = provider.lower().replace(' ', '_').replace('-', '_')
            index_name = ''.join(c for c in index_name if c.isalnum() or c == '_')
            index_name = f"{index_name}_index"
        
        logger.info(f"Searching index: {index_name}")
        
        try:
            search_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            results = search_client.search(
                search_text=None,
                vector_queries=[{
                    "kind": "vector",
                    "vector": query_vector,
                    "fields": "content_vector",
                    "k": top_k
                }],
                select=["id", "content", "document_name", "extracted_fields"],
                top=top_k
            )
            
            similar_documents = [dict(result) for result in results]
            logger.info(f"Retrieved {len(similar_documents)} similar documents")
            
            return similar_documents
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []
    
    def _call_vision_api(
        self,
        system_prompt: str,
        text_prompt: str,
        image_data: str,
        source_document: str
    ) -> Dict[str, Any]:
        """Call GPT-4o Vision API"""
        
        try:
            # Detect image type
            # PDFs and Word docs are converted to PNG
            # Direct images use their original format
            if source_document.lower().endswith(('.pdf', '.doc', '.docx')):
                image_type = "image/png"  # Documents converted to PNG
            elif source_document.lower().endswith('.png'):
                image_type = "image/png"  # Direct PNG
            elif source_document.lower().endswith(('.jpg', '.jpeg')):
                image_type = "image/jpeg"  # Direct JPEG
            elif source_document.lower().endswith('.gif'):
                image_type = "image/gif"  # Direct GIF
            elif source_document.lower().endswith('.webp'):
                image_type = "image/webp"  # Direct WEBP
            else:
                image_type = "image/png"  # default PNG
            
            # Build vision message (simplified, matching reference code)
            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": text_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{image_type};base64,{image_data}"
                            }
                        }
                    ]
                }
            ]
            
            # Call Azure OpenAI (matching reference pattern)
            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=messages,
                temperature=0.0,
                max_tokens=2000
            )
            
            # Track tokens
            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens += response.usage.total_tokens
            
            content = response.choices[0].message.content.strip()
            
            # Clean markdown
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()
            
            # Parse JSON
            data = json.loads(content)
            
            # Normalize format
            normalized_data = {}
            for field_name in data:
                field_value = data[field_name]
                
                if isinstance(field_value, dict) and 'value' in field_value:
                    normalized_data[field_name] = {
                        'value': field_value.get('value', ''),
                        'confidence': float(field_value.get('confidence', 0.0)),
                        'source_document': source_document
                    }
                else:
                    normalized_data[field_name] = {
                        'value': str(field_value) if field_value else '',
                        'confidence': 0.5,
                        'source_document': source_document
                    }
            
            logger.info(f"Extracted {len(normalized_data)} fields with vision")
            
            return {
                'success': True,
                'extracted_fields': normalized_data,
                'raw_response': content,
                'system_prompt': system_prompt,
                'user_prompt': text_prompt
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e}")
            return {
                'success': False,
                'extracted_fields': {},
                'error': f"JSON parse error: {e}",
                'raw_response': content[:500] if 'content' in locals() else ''
            }
        except Exception as e:
            # Log detailed error information
            error_str = str(e)
            logger.error(f"Vision API call failed: {error_str}")
            
            # Log additional debug info
            if hasattr(e, 'response'):
                logger.error(f"Response status: {getattr(e.response, 'status_code', 'N/A')}")
                logger.error(f"Response text: {getattr(e.response, 'text', 'N/A')[:500]}")
            
            # Check if it's a base64 issue
            if 'Invalid image' in error_str or 'base64' in error_str.lower():
                logger.error(f"Base64 data length: {len(image_data) if 'image_data' in locals() else 'N/A'}")
                logger.error(f"Image type detected: {image_type if 'image_type' in locals() else 'N/A'}")
                logger.error(f"Document: {source_document}")
            
            return {
                'success': False,
                'extracted_fields': {},
                'error': error_str,
                'raw_response': ''
            }
    
    def _extract_text_only(
        self,
        document_text: str,
        provider: str,
        source_document: str,
        document_type: Optional[str]
    ) -> Dict[str, Any]:
        """Fallback to text-only if vision fails"""
        
        logger.info("Falling back to text-only extraction")
        
        # Use text_rag approach
        from text_rag import TextRAGExtractor
        
        text_extractor = TextRAGExtractor(
            search_endpoint=self.search_endpoint,
            search_api_key=self.search_credential.key,
            openai_manager=self.openai_manager,
            fields=self.fields,
            top_k=self.top_k,
            similarity_threshold=self.similarity_threshold
        )
        
        return text_extractor.extract_with_rag(
            document_text=document_text,
            provider=provider,
            source_document=source_document,
            document_type=document_type
        )
