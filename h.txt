config.json
{
  "azure_document_intelligence": {
    "endpoint": "YOUR_DOCUMENT_INTELLIGENCE_ENDPOINT",
    "key": "YOUR_DOCUMENT_INTELLIGENCE_KEY",
    "model_version": "2023-07-31",
    "custom_model_id": null
  },
  "categories": {
    "cms1500": ["cadwell", "rhymlink"],
    "invoice": ["tesla", "amazon"],
    "scheduling": ["email", "iomrequest"]
  },
  "paths": {
    "reference_dir": "reference",
    "input_dir": "input_docs",
    "output_dir": "output"
  },
  "classification": {
    "confidence_threshold": 0.5,
    "min_training_documents": 5,
    "max_training_documents": 100
  }
}

----
helper.py

import os
import re
import json
import hashlib
import logging
import traceback
from datetime import datetime
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

def load_config(config_path="config.json"):
    """
    Load configuration from JSON file
    """
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"Config loading error: {e}")
        raise

def compute_reference_hash(ref_dir):
    """
    Compute a comprehensive hash of reference directory contents
    """
    hasher = hashlib.sha256()
    
    for root, _, files in sorted(os.walk(ref_dir)):
        for f in sorted(files):
            path = os.path.join(root, f)
            
            try:
                with open(path, 'rb') as file:
                    hasher.update(file.read())
            except Exception as e:
                logging.warning(f"Could not read file {path}: {e}")
    
    return hasher.hexdigest()

def get_document_intelligence_client(cfg):
    """
    Initialize Azure Document Intelligence client
    """
    try:
        credential = AzureKeyCredential(cfg["azure_document_intelligence"]["key"])
        client = DocumentIntelligenceClient(
            endpoint=cfg["azure_document_intelligence"]["endpoint"],
            credential=credential
        )
        return client
    except Exception as e:
        logging.critical(f"Client initialization error: {e}")
        raise

def prepare_reference_metadata(ref_dir):
    """
    Generate a structured metadata of reference documents
    """
    reference_metadata = {}
    
    for main_category in os.listdir(ref_dir):
        main_path = os.path.join(ref_dir, main_category)
        if not os.path.isdir(main_path):
            continue
        
        reference_metadata[main_category] = {}
        
        for subcategory in os.listdir(main_path):
            subcat_path = os.path.join(main_path, subcategory)
            if not os.path.isdir(subcat_path):
                continue
            
            # Count documents in each subcategory
            doc_count = len([f for f in os.listdir(subcat_path) 
                             if os.path.isfile(os.path.join(subcat_path, f))])
            
            reference_metadata[main_category][subcategory] = {
                'document_count': doc_count,
                'documents': [f for f in os.listdir(subcat_path) 
                              if os.path.isfile(os.path.join(subcat_path, f))]
            }
    
    return reference_metadata

def train_custom_model(ref_dir, client, cfg):
    """
    Train a custom model based on reference documents
    """
    try:
        logging.info("üöÄ Preparing custom model training")
        
        # Collect training documents
        training_documents = []
        for main_category in os.listdir(ref_dir):
            main_path = os.path.join(ref_dir, main_category)
            
            for subcategory in os.listdir(main_path):
                subcat_path = os.path.join(main_path, subcategory)
                
                for doc_file in os.listdir(subcat_path):
                    doc_path = os.path.join(subcat_path, doc_file)
                    training_documents.append(doc_path)
        
        # Limit training documents
        min_docs = cfg['classification'].get('min_training_documents', 5)
        max_docs = cfg['classification'].get('max_training_documents', 100)
        
        if len(training_documents) < min_docs:
            logging.warning(f"Insufficient documents for training. Minimum required: {min_docs}")
            return None
        
        training_documents = training_documents[:max_docs]
        
        # Prepare training files
        training_files = [open(doc, 'rb') for doc in training_documents]
        
        # Generate unique model identifier
        model_id = f"custom-doc-classifier-{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        try:
            # Start model training
            poller = client.begin_build_model(
                model_id=model_id,
                training_files=training_files
            )
            
            # Wait for training to complete
            result = poller.result()
            
            # Log training details
            logging.info(f"Custom Model Trained: {result.model_id}")
            logging.info(f"Training Status: {result.status}")
            
            return result.model_id
        
        finally:
            # Ensure files are closed
            for file in training_files:
                file.close()
    
    except Exception as e:
        logging.error(f"Custom model training error: {e}")
        logging.error(traceback.format_exc())
        return None

def check_reference_changes(cfg, client):
    """
    Check if reference documents have changed
    """
    ref_dir = cfg['paths']['reference_dir']
    hash_file = os.path.join(ref_dir, ".reference_hash")
    
    try:
        # Compute current reference hash
        hash_now = compute_reference_hash(ref_dir)
        
        # Check if hash file exists
        if os.path.exists(hash_file):
            with open(hash_file, 'r') as f:
                last_hash = f.read().strip()
        else:
            last_hash = ''
        
        # Compare hashes
        if hash_now != last_hash:
            # Log detailed changes
            current_metadata = prepare_reference_metadata(ref_dir)
            
            logging.info("üöÄ New Reference Data Detected")
            logging.info("Reference Document Metadata:")
            logging.info(json.dumps(current_metadata, indent=2))
            
            # Train custom model
            custom_model_id = train_custom_model(ref_dir, client, cfg)
            
            # Update configuration if model trained
            if custom_model_id:
                cfg['azure_document_intelligence']['custom_model_id'] = custom_model_id
            
            # Save new hash
            with open(hash_file, 'w') as f:
                f.write(hash_now)
            
            return True
        
        return False
    
    except Exception as e:
        logging.error(f"Reference check error: {e}")
        return False

def prepare_directories(cfg):
    """
    Prepare necessary directories for processing
    """
    directories = [
        cfg['paths']['input_dir'],
        cfg['paths']['output_dir'],
        os.path.join(cfg['paths']['output_dir'], 'source'),
        os.path.join(cfg['paths']['output_dir'], 'classified'),
        os.path.join(cfg['paths']['output_dir'], 'unclassified')
    ]
    
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)

-----
main.py

import os
import io
import json
import shutil
import logging
import traceback

from helper import (
    load_config, 
    get_document_intelligence_client,
    prepare_directories,
    check_reference_changes
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

def classify_document(client, file_path, cfg):
    """
    Classify document using Document Intelligence
    """
    try:
        # Determine model to use
        model_id = cfg['azure_document_intelligence'].get('custom_model_id', 'prebuilt-document')
        
        # Analyze document
        with open(file_path, 'rb') as doc_file:
            poller = client.begin_analyze_document(
                model_id=model_id,
                analyze_request=doc_file
            )
            result = poller.result()
        
        # Default classification
        main_category = 'unknown'
        subcategory = 'unknown'
        confidence_score = 0.0
        reasoning = "No specific reasoning"
        
        # Process analysis results
        if result.documents and len(result.documents) > 0:
            document = result.documents[0]
            
            # Extract document features
            if hasattr(document, 'fields'):
                for field_name, field in document.fields.items():
                    # Attempt to map to categories based on field information
                    for cat, subcats in cfg['categories'].items():
                        if any(subcat.lower() in str(field_name).lower() for subcat in subcats):
                            main_category = cat
                            subcategory = [
                                subcat for subcat in subcats 
                                if subcat.lower() in str(field_name).lower()
                            ][0]
                            confidence_score = getattr(field, 'confidence', 0.0)
                            reasoning = f"Matched by field: {field_name}"
                            break
        
        return main_category, subcategory, confidence_score, reasoning

    except Exception as e:
        logging.error(f"Document classification error: {e}")
        logging.error(traceback.format_exc())
        return 'unknown', 'unknown', 0.0, f"Classification error: {str(e)}"

def process_documents():
    """
    Document processing and classification
    """
    try:
        # Load configuration
        cfg = load_config()
        
        # Prepare directories
        prepare_directories(cfg)
        
        # Initialize Document Intelligence client
        client = get_document_intelligence_client(cfg)
        
        # Check for reference changes and potentially train model
        check_reference_changes(cfg, client)
        
        # Set up directories
        input_dir = cfg['paths']['input_dir']
        output_dir = cfg['paths']['output_dir']
        source_dir = os.path.join(output_dir, 'source')
        classified_dir = os.path.join(output_dir, 'classified')
        unclassified_dir = os.path.join(output_dir, 'unclassified')
        
        # Processing statistics
        stats = {
            'total_documents': 0,
            'classified_documents': 0,
            'unclassified_documents': 0
        }

        # Process each document
        for fname in os.listdir(input_dir):
            fpath = os.path.join(input_dir, fname)
            
            # Skip directories and hidden files
            if not os.path.isfile(fpath) or fname.startswith('.'):
                continue

            # Increment total documents
            stats['total_documents'] += 1

            try:
                # Copy original document to source directory
                shutil.copy(fpath, os.path.join(source_dir, fname))

                # Classify document
                main_cat, sub_cat, confidence, reasoning = classify_document(
                    client, 
                    fpath, 
                    cfg
                )

                # Determine destination based on classification
                confidence_threshold = cfg['classification'].get('confidence_threshold', 0.5)
                if confidence >= confidence_threshold:
                    # Classified document
                    dest_dir = os.path.join(classified_dir, main_cat, sub_cat)
                    stats['classified_documents'] += 1
                else:
                    # Unclassified document
                    dest_dir = unclassified_dir
                    stats['unclassified_documents'] += 1

                # Create destination directory
                os.makedirs(dest_dir, exist_ok=True)
                
                # Copy document
                dest_path = os.path.join(dest_dir, fname)
                shutil.copy(fpath, dest_path)
                
                # Create metadata
                metadata_path = os.path.join(dest_dir, f"{os.path.splitext(fname)[0]}_metadata.json")
                with open(metadata_path, 'w') as metadata_file:
                    json.dump({
                        "filename": fname,
                        "main_category": main_cat,
                        "subcategory": sub_cat,
                        "confidence_score": confidence,
                        "reasoning": reasoning
                    }, metadata_file, indent=2)

            except Exception as e:
                logging.error(f"Processing error for {fname}: {e}")
                logging.error(traceback.format_exc())

        # Log processing summary
        logging.info("\n--- Processing Summary ---")
        logging.info(json.dumps(stats, indent=2))

        print("\n‚úÖ Document Processing Complete")

    except Exception as overall_error:
        logging.critical(f"Critical processing error: {overall_error}")
        logging.critical(traceback.format_exc())
        print("‚ùå Document Processing Failed. Check logs for details.")

if __name__ == "__main__":
    process_documents()
