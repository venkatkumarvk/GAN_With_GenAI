#log

"""
LOGGING CONFIGURATION MODULE
=============================

Sets up comprehensive logging for document extraction pipeline
Creates timestamped log files for each run
"""

import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional


def setup_logging(
    log_dir: str = "logs",
    log_level: str = "INFO",
    run_timestamp: Optional[str] = None,
    console_output: bool = True
) -> str:
    """
    Setup logging configuration for the pipeline
    
    Args:
        log_dir: Directory to store log files
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR)
        run_timestamp: Timestamp for log filename (auto-generated if None)
        console_output: Whether to also output to console
    
    Returns:
        Path to the log file
    """
    
    # Create logs directory if it doesn't exist
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    # Generate timestamp if not provided
    if run_timestamp is None:
        run_timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    
    # Create log filename
    log_filename = f"rag_processing_{run_timestamp}.log"
    log_filepath = log_path / log_filename
    
    # Configure logging format
    log_format = logging.Formatter(
        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Get root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))
    
    # Remove existing handlers
    root_logger.handlers = []
    
    # File handler (always created)
    file_handler = logging.FileHandler(log_filepath, mode='w', encoding='utf-8')
    file_handler.setLevel(getattr(logging, log_level.upper()))
    file_handler.setFormatter(log_format)
    root_logger.addHandler(file_handler)
    
    # Console handler (optional)
    if console_output:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)  # Console shows INFO and above
        console_handler.setFormatter(log_format)
        root_logger.addHandler(console_handler)
    
    # Log startup message
    root_logger.info("="*70)
    root_logger.info("DOCUMENT EXTRACTION PIPELINE - LOG START")
    root_logger.info("="*70)
    root_logger.info(f"Log file: {log_filepath}")
    root_logger.info(f"Log level: {log_level}")
    root_logger.info(f"Timestamp: {run_timestamp}")
    root_logger.info("="*70)
    
    return str(log_filepath)


def setup_rag_logging(
    run_timestamp: str,
    log_level: str = "INFO"
) -> str:
    """
    Setup RAG-specific logging (convenience function)
    
    Args:
        run_timestamp: Timestamp for this run
        log_level: Logging level
    
    Returns:
        Path to log file
    """
    return setup_logging(
        log_dir="logs",
        log_level=log_level,
        run_timestamp=run_timestamp,
        console_output=True
    )


def log_rag_config(config: dict):
    """
    Log RAG configuration at startup
    
    Args:
        config: Configuration dictionary
    """
    logger = logging.getLogger(__name__)
    
    logger.info("RAG CONFIGURATION:")
    logger.info("-" * 50)
    
    # RAG settings
    rag_cfg = config.get('rag', {})
    logger.info(f"  Mode: {rag_cfg.get('mode', 'text').upper()}")
    logger.info(f"  Vision: {'Enabled' if rag_cfg.get('use_vision') else 'Disabled'}")
    logger.info(f"  Top-K: {rag_cfg.get('top_k', 3)}")
    logger.info(f"  Similarity Threshold: {rag_cfg.get('similarity_threshold', 0.7)}")
    
    if rag_cfg.get('mode') == 'multimodal':
        logger.info(f"  Text Weight: {rag_cfg.get('text_weight', 0.7)}")
        logger.info(f"  Visual Weight: {rag_cfg.get('visual_weight', 0.3)}")
    
    # Fields
    fields = config.get('fields', [])
    logger.info(f"  Fields to Extract ({len(fields)}): {', '.join(fields)}")
    
    logger.info("-" * 50)


def log_processing_start(provider: str, provider_id: str, num_files: int):
    """
    Log start of provider processing
    
    Args:
        provider: Provider name
        provider_id: Provider ID with timestamp
        num_files: Number of files to process
    """
    logger = logging.getLogger(__name__)
    logger.info("")
    logger.info("="*70)
    logger.info(f"PROCESSING PROVIDER: {provider}")
    logger.info("="*70)
    logger.info(f"Provider ID: {provider_id}")
    logger.info(f"Total Files: {num_files}")
    logger.info("-"*70)


def log_document_processing(
    filename: str,
    status: str,
    details: dict = None
):
    """
    Log individual document processing
    
    Args:
        filename: Document filename
        status: Processing status (success, failed, skipped)
        details: Additional details to log
    """
    logger = logging.getLogger(__name__)
    
    status_symbol = {
        'success': 'âœ“',
        'failed': 'âœ—',
        'skipped': 'âŠ˜'
    }.get(status.lower(), '?')
    
    logger.info(f"  {status_symbol} Document: {filename} [{status.upper()}]")
    
    if details:
        for key, value in details.items():
            logger.info(f"    - {key}: {value}")


def log_rag_extraction(
    filename: str,
    mode: str,
    used_rag: bool,
    has_vision: bool,
    similar_docs_count: int,
    avg_confidence: float
):
    """
    Log RAG extraction details
    
    Args:
        filename: Document filename
        mode: RAG mode (text, multimodal)
        used_rag: Whether RAG was used
        has_vision: Whether vision was used
        similar_docs_count: Number of similar documents found
        avg_confidence: Average confidence score
    """
    logger = logging.getLogger(__name__)
    
    logger.info(f"    RAG Extraction Details:")
    logger.info(f"      Mode: {mode.upper()}")
    logger.info(f"      Vision: {'Yes' if has_vision else 'No'}")
    logger.info(f"      Used RAG: {'Yes' if used_rag else 'No (first doc or fallback)'}")
    
    if used_rag:
        logger.info(f"      Similar Docs Found: {similar_docs_count}")
    
    logger.info(f"      Avg Confidence: {avg_confidence:.2%}")


def log_provider_summary(
    provider: str,
    total_processed: int,
    successful: int,
    failed: int,
    avg_confidence: float,
    total_cost: float
):
    """
    Log provider processing summary
    
    Args:
        provider: Provider name
        total_processed: Total documents processed
        successful: Successful extractions
        failed: Failed extractions
        avg_confidence: Average confidence
        total_cost: Total cost
    """
    logger = logging.getLogger(__name__)
    
    logger.info("")
    logger.info("-"*70)
    logger.info(f"PROVIDER SUMMARY: {provider}")
    logger.info("-"*70)
    logger.info(f"  Total Processed: {total_processed}")
    logger.info(f"  Successful: {successful}")
    logger.info(f"  Failed: {failed}")
    logger.info(f"  Success Rate: {(successful/total_processed*100):.1f}%")
    logger.info(f"  Avg Confidence: {avg_confidence:.2%}")
    logger.info(f"  Total Cost: ${total_cost:.4f}")
    logger.info("="*70)


def log_global_summary(
    total_providers: int,
    total_documents: int,
    total_cost: float,
    processing_time: float
):
    """
    Log global processing summary
    
    Args:
        total_providers: Total providers processed
        total_documents: Total documents processed
        total_cost: Total cost
        processing_time: Processing time in seconds
    """
    logger = logging.getLogger(__name__)
    
    logger.info("")
    logger.info("="*70)
    logger.info("GLOBAL SUMMARY")
    logger.info("="*70)
    logger.info(f"  Total Providers: {total_providers}")
    logger.info(f"  Total Documents: {total_documents}")
    logger.info(f"  Total Cost: ${total_cost:.4f}")
    logger.info(f"  Processing Time: {processing_time:.1f} seconds")
    logger.info(f"  Avg Time per Document: {(processing_time/total_documents):.1f}s")
    logger.info("="*70)
    logger.info("PROCESSING COMPLETED SUCCESSFULLY")
    logger.info("="*70)


def log_error(error_msg: str, exception: Exception = None):
    """
    Log error with optional exception details
    
    Args:
        error_msg: Error message
        exception: Exception object (optional)
    """
    logger = logging.getLogger(__name__)
    logger.error(error_msg)
    
    if exception:
        logger.error(f"Exception Type: {type(exception).__name__}")
        logger.error(f"Exception Details: {str(exception)}", exc_info=True)


# Example usage
if __name__ == "__main__":
    # Test logging setup
    run_timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    log_file = setup_rag_logging(run_timestamp, log_level="INFO")
    
    print(f"Log file created: {log_file}")
    
    # Test logging functions
    logger = logging.getLogger(__name__)
    
    # Test config logging
    test_config = {
        'rag': {
            'mode': 'multimodal',
            'use_vision': True,
            'top_k': 3,
            'text_weight': 0.7,
            'visual_weight': 0.3
        },
        'fields': ['name', 'passport_number', 'date_of_birth']
    }
    log_rag_config(test_config)
    
    # Test processing logs
    log_processing_start("test_provider", "test_20240213_120000", 3)
    
    log_document_processing(
        "passport.pdf",
        "success",
        {'pages': 2, 'size': '1.2 MB'}
    )
    
    log_rag_extraction(
        "passport.pdf",
        mode="multimodal",
        used_rag=True,
        has_vision=True,
        similar_docs_count=2,
        avg_confidence=0.96
    )
    
    log_provider_summary(
        "test_provider",
        total_processed=3,
        successful=3,
        failed=0,
        avg_confidence=0.95,
        total_cost=0.21
    )
    
    log_global_summary(
        total_providers=1,
        total_documents=3,
        total_cost=0.21,
        processing_time=45.2
    )
    
    print(f"âœ“ Test logging completed. Check: {log_file}")




      ---------------


      # text.py

      """
TEXT RAG - Text-Only Document Extraction
=========================================

OCR text is PRIMARY source.
Uses Azure AI Search vector search to find similar past documents.
Passes them as few-shot examples to GPT-4o for extraction.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import json
import logging
from typing import List, Dict, Any, Optional

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class TextRAGExtractor:
    """
    Text-Only RAG Extractor

    Flow:
        Document â†’ OCR (primary) â†’ Embed â†’ Search Similar â†’ GPT-4o Extract

    Mode: Fast, cost-effective, 92-95% accuracy
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(f"TextRAGExtractor ready | top_k={top_k} | threshold={similarity_threshold}")
        print(f"    âœ“ RAG Mode : TEXT-ONLY")
        print(f"    âœ“ Vision   : Disabled")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PUBLIC: Extract with RAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def extract_with_rag(
        self,
        document_text: str,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Text RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, embeddings
            }
        """
        logger.info(f"Text RAG extraction | doc={source_document} | provider={provider}")

        # Step 1 â€” Generate text embedding (OCR is primary)
        embedding = self.openai_manager.generate_embeddings(document_text)
        logger.info("  âœ“ Text embedding generated")

        # Step 2 â€” Search similar documents
        similar_docs = self._search_similar(embedding, provider, index_name)

        # Step 3 â€” Build prompt
        if similar_docs:
            print(f"    âœ“ RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  âœ“ {len(similar_docs)} similar docs retrieved")
            user_prompt = self._build_rag_prompt(document_text, similar_docs)
        else:
            print(f"    âš  RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  âš  No similar docs â€” standard extraction")
            user_prompt = self._build_standard_prompt(document_text)

        # Step 4 â€” GPT-4o extraction
        extracted_fields = self._call_gpt(user_prompt, document_type)

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':        False,
            'visual_description': '',
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding   # combined = text in text-only mode
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        document_type: Optional[str] = None,
        source_document: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG â€” used for first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            document_type:   Document type hint
            source_document: Source filename

        Returns:
            Same shape as extract_with_rag
        """
        logger.info(f"Text extraction (no RAG) | doc={source_document}")

        embedding   = self.openai_manager.generate_embeddings(document_text)
        user_prompt = self._build_standard_prompt(document_text)
        extracted_fields = self._call_gpt(user_prompt, document_type)

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          False,
            'similar_docs_count': 0,
            'has_vision':        False,
            'visual_description': '',
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding
            }
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PRIVATE HELPERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _search_similar(
        self,
        embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search for similar documents"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=["document_name", "content", "extracted_fields", "avg_confidence"]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _build_rag_prompt(
        self,
        document_text: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build user prompt with RAG few-shot examples"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass
            parts.append("")

        parts.append("â”€" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")
        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:8000])
        parts.append("")
        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(self, document_text: str) -> str:
        """Build standard prompt without RAG context"""
        return (
            f"Extract the following fields from this document:\n\n"
            f"OCR TEXT (Primary Source):\n{document_text[:8000]}\n\n"
            f"Fields to extract: {', '.join(self.fields)}\n\n"
            "Return ONLY a JSON object with field values and confidence scores."
        )

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> Dict[str, Any]:
        """Call GPT-4o and return parsed extraction result"""
        try:
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            # Track tokens
            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content)

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}


----

  #multimodal rag

  """
MULTIMODAL RAG - Vision + Text Document Extraction
====================================================

OCR text is PRIMARY source.
Vision (GPT-4 Vision) is SUPPLEMENTARY enhancement.
Combined embedding (text + visual) improves RAG similarity search.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import base64
import json
import logging
from typing import Dict, Any, List, Optional, Tuple

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class MultimodalRAGExtractor:
    """
    Multimodal RAG Extractor â€” Text + Vision

    Flow:
        Document â†’ OCR (primary)       â†’ Text Embed   â”€â”
                 â†’ Vision (supplement) â†’ Visual Embed â”€â”¤ â†’ Combined â†’ Search â†’ GPT-4o
                                                       â”€â”˜

    Mode: Higher accuracy (95-97%), 100% coverage (handles image-only docs)
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        text_weight: float = 0.7,
        visual_weight: float = 0.3
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
            text_weight:          Weight for text embedding in combined vector (default 0.7)
            visual_weight:        Weight for visual embedding in combined vector (default 0.3)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold
        self.text_weight          = text_weight
        self.visual_weight        = visual_weight

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(
            f"MultimodalRAGExtractor ready | "
            f"top_k={top_k} | text_w={text_weight} | visual_w={visual_weight}"
        )
        print(f"    âœ“ RAG Mode : MULTIMODAL (Text + Vision)")
        print(f"    âœ“ Vision   : Enabled")
        print(f"    âœ“ Weights  : text={text_weight} | visual={visual_weight}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PUBLIC: Extract with RAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def extract_with_rag(
        self,
        document_text: str,
        image_bytes: bytes,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Multimodal RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            image_bytes:     Raw image bytes for vision analysis
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, has_vision,
              visual_description, embeddings
            }
        """
        logger.info(f"Multimodal RAG | doc={source_document} | provider={provider}")

        # Step 1 â€” Vision analysis (supplementary)
        visual_description = ''
        if image_bytes:
            print(f"    ğŸ” Vision analysis (supplementary)...")
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']
                print(f"    âœ“ Vision       : {len(visual_description)} chars")
                logger.info(f"  âœ“ Vision complete ({len(visual_description)} chars)")
            else:
                print(f"    âš  Vision failed â€” OCR only (primary)")
                logger.warning(f"  âš  Vision failed: {result.get('error')}")
        else:
            print(f"    âš  No image bytes â€” OCR only (primary)")

        # Step 2 â€” Generate embeddings
        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )
        emb_mode = "text + visual" if visual_description else "text only (vision unavailable)"
        print(f"    âœ“ Embeddings   : {emb_mode}")
        logger.info(f"  âœ“ Embeddings generated: {emb_mode}")

        # Step 3 â€” Search similar documents
        similar_docs = self._search_similar(combined_emb, provider, index_name)

        # Step 4 â€” Build prompt
        if similar_docs:
            print(f"    âœ“ RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  âœ“ {len(similar_docs)} similar docs retrieved")
            user_prompt = self._build_rag_prompt(document_text, visual_description, similar_docs)
        else:
            print(f"    âš  RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  âš  No similar docs â€” standard extraction")
            user_prompt = self._build_standard_prompt(document_text, visual_description)

        # Step 5 â€” GPT-4o extraction
        extracted_fields = self._call_gpt(user_prompt, document_type)

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        image_bytes: Optional[bytes] = None,
        document_type: Optional[str] = None,
        source_document: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG â€” first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            image_bytes:     Image bytes for vision (optional)
            document_type:   Document type hint
            source_document: Source filename
        """
        logger.info(f"Multimodal extraction (no RAG) | doc={source_document}")

        visual_description = ''
        if image_bytes:
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']

        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )

        user_prompt      = self._build_standard_prompt(document_text, visual_description)
        extracted_fields = self._call_gpt(user_prompt, document_type)

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           False,
            'similar_docs_count': 0,
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PRIVATE HELPERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _analyze_vision(
        self,
        image_bytes: bytes,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """Analyze document image using GPT-4 Vision"""
        try:
            b64 = base64.b64encode(image_bytes).decode('utf-8')

            type_hints = {
                'passport': "\n\nFocus on: MRZ zone, country emblem, biographical page layout.",
                'license':  "\n\nFocus on: License number position, photo, endorsements.",
                'id_card':  "\n\nFocus on: ID number placement, official seals, front/back layout."
            }
            doc_hint = type_hints.get(document_type, '') if document_type else ''

            prompt = (
                "Analyze this identity document image and describe:\n"
                "1. Document type and layout\n"
                "2. Security features (holograms, watermarks, seals)\n"
                "3. Document condition and image quality\n"
                "4. Photo position and quality\n"
                "5. Any notable visual elements or anomalies\n\n"
                "Keep description concise (max 400 chars) â€” it supplements OCR text."
                + doc_hint
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.deployment_name,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url":    f"data:image/jpeg;base64,{b64}",
                                "detail": "high"
                            }
                        }
                    ]
                }],
                max_tokens=500,
                temperature=0.0
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return {
                'success':            True,
                'visual_description': response.choices[0].message.content
            }

        except Exception as e:
            logger.error(f"Vision analysis failed: {e}", exc_info=True)
            return {'success': False, 'visual_description': '', 'error': str(e)}

    def _generate_embeddings(
        self,
        document_text: str,
        visual_description: str
    ) -> Tuple[List[float], Optional[List[float]], List[float]]:
        """
        Generate text, visual, and combined embeddings

        Returns:
            (text_embedding, visual_embedding_or_None, combined_embedding)
        """
        text_emb = self.openai_manager.generate_embeddings(document_text)

        if visual_description:
            visual_emb   = self.openai_manager.generate_embeddings(visual_description)
            combined_emb = [
                self.text_weight * text_emb[i] + self.visual_weight * visual_emb[i]
                for i in range(len(text_emb))
            ]
            return text_emb, visual_emb, combined_emb
        else:
            return text_emb, None, text_emb   # fallback: combined = text

    def _search_similar(
        self,
        combined_embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search using combined embedding"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": combined_embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=[
                    "document_name", "content",
                    "extracted_fields", "visual_description", "avg_confidence"
                ]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _build_rag_prompt(
        self,
        document_text: str,
        visual_description: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build prompt with RAG few-shot examples + visual context"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass

            visual_ctx = doc.get('visual_description', '')
            if visual_ctx:
                parts.append(f"Visual   : {visual_ctx[:150]}")

            parts.append("")

        parts.append("â”€" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")

        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:7000])
        parts.append("")

        if visual_description:
            parts.append("VISUAL ANALYSIS (Supplementary):")
            parts.append(visual_description)
            parts.append("")

        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(
        self,
        document_text: str,
        visual_description: str
    ) -> str:
        """Standard prompt (no RAG) with optional visual context"""
        parts = [
            "Extract the following fields from this document:",
            "",
            "OCR TEXT (Primary Source):",
            document_text[:7000],
            ""
        ]
        if visual_description:
            parts += ["VISUAL ANALYSIS (Supplementary):", visual_description, ""]

        parts += [
            f"Fields to extract: {', '.join(self.fields)}",
            "Return ONLY a JSON object with field values and confidence scores."
        ]
        return "\n".join(parts)

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> Dict[str, Any]:
        """Call GPT-4o and return parsed extraction"""
        try:
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )
            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content)

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}


---------

  unified.py

  """
UNIFIED RAG - Router
=====================

Reads config.json and initializes the correct RAG extractor:
  - mode = "text"       â†’ TextRAGExtractor       (text_rag.py)
  - mode = "multimodal" â†’ MultimodalRAGExtractor  (multimodal_rag.py)

This file just routes â€” all logic lives in text_rag.py / multimodal_rag.py

DO NOT EDIT - Stable code
Users configure via config.json:

    "rag": {
        "mode": "text",           â† "text" or "multimodal"
        "use_vision": false,      â† true only when mode = "multimodal"
        "top_k": 3,
        "similarity_threshold": 0.7,
        "text_weight": 0.7,       â† only used in multimodal mode
        "visual_weight": 0.3      â† only used in multimodal mode
    }
"""

import logging
from typing import List, Dict, Any

from text_rag       import TextRAGExtractor
from multimodal_rag import MultimodalRAGExtractor

logger = logging.getLogger(__name__)


def create_rag_extractor(
    rag_config: Dict[str, Any],
    search_endpoint: str,
    search_api_key: str,
    openai_manager,
    fields: List[str]
):
    """
    Factory function â€” reads config and returns the correct RAG extractor

    Args:
        rag_config:       config['rag'] dictionary from config.json
        search_endpoint:  Azure AI Search endpoint
        search_api_key:   Azure AI Search API key
        openai_manager:   AzureOpenAIManager instance
        fields:           List of fields to extract

    Returns:
        TextRAGExtractor  OR  MultimodalRAGExtractor
    """
    mode       = rag_config.get('mode', 'text').lower()
    top_k      = rag_config.get('top_k', 3)
    threshold  = rag_config.get('similarity_threshold', 0.7)

    print(f"\n{'='*60}")
    print(f"  RAG EXTRACTOR INITIALIZING")
    print(f"{'='*60}")
    print(f"  Mode      : {mode.upper()}")

    # â”€â”€ TEXT mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if mode == 'text':
        print(f"  File      : text_rag.py â†’ TextRAGExtractor")
        logger.info(f"RAG mode: TEXT | top_k={top_k} | threshold={threshold}")

        return TextRAGExtractor(
            search_endpoint      = search_endpoint,
            search_api_key       = search_api_key,
            openai_manager       = openai_manager,
            fields               = fields,
            top_k                = top_k,
            similarity_threshold = threshold
        )

    # â”€â”€ MULTIMODAL mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif mode == 'multimodal':
        text_weight   = rag_config.get('text_weight',   0.7)
        visual_weight = rag_config.get('visual_weight', 0.3)

        print(f"  File      : multimodal_rag.py â†’ MultimodalRAGExtractor")
        print(f"  Weights   : text={text_weight} | visual={visual_weight}")
        logger.info(
            f"RAG mode: MULTIMODAL | top_k={top_k} | "
            f"text_w={text_weight} | visual_w={visual_weight}"
        )

        return MultimodalRAGExtractor(
            search_endpoint      = search_endpoint,
            search_api_key       = search_api_key,
            openai_manager       = openai_manager,
            fields               = fields,
            top_k                = top_k,
            similarity_threshold = threshold,
            text_weight          = text_weight,
            visual_weight        = visual_weight
        )

    # â”€â”€ Unknown mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    else:
        logger.warning(f"Unknown RAG mode '{mode}' â€” falling back to TEXT mode")
        print(f"  âš  Unknown mode '{mode}' â€” falling back to TEXT mode")

        return TextRAGExtractor(
            search_endpoint      = search_endpoint,
            search_api_key       = search_api_key,
            openai_manager       = openai_manager,
            fields               = fields,
            top_k                = top_k,
            similarity_threshold = threshold
        )


-------

  config

  {
  "AzureBlob": {
    "connection_string": "YOUR_BLOB_CONNECTION_STRING",
    "inputcontainer": "inputcontainer",
    "outputcontainer": "outputcontainer"
  },

  "AzureOpenAI": {
    "endpoint": "https://YOUR-GPT.openai.azure.com/",
    "api_key": "YOUR_GPT_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "gpt-4o"
  },

  "AzureEmbedding": {
    "endpoint": "https://YOUR-EMBEDDING.openai.azure.com/",
    "api_key": "YOUR_EMBEDDING_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "text-embedding-3-large",
    "dimension": 3072
  },

  "DocumentIntelligence": {
    "endpoint": "https://YOUR_DOC_INTEL.cognitiveservices.azure.com/",
    "key": "YOUR_DOC_INTEL_KEY"
  },

  "AzureAISearch": {
    "endpoint": "https://YOUR_SEARCH_SERVICE.search.windows.net",
    "api_key": "YOUR_SEARCH_KEY"
  },

  "fields": [
    "name",
    "passport_number",
    "date_of_birth",
    "document_number",
    "nationality",
    "issue_date",
    "expiry_date"
  ],

  "confidence_threshold": 0.90,

  "RAG": {
    "mode": "text",
    "top_k": 3,
    "similarity_threshold": 0.70,
    "text_weight": 0.70,
    "visual_weight": 0.30
  },

  "costs": {
    "gpt4o_input_per_1k": 0.0025,
    "gpt4o_output_per_1k": 0.01,
    "embedding_per_1k": 0.00013,
    "doc_intel_per_page": 0.01
  },

  "logging": {
    "log_dir": "logs",
    "log_level": "INFO"
  }
}
