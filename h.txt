# insert_helper.py
import os
import glob
import pandas as pd

class InsertSQLGenerator:
    def __init__(self, config: dict, input_folder: str, output_folder: str = "generated_inserts"):
        self.input_folder = input_folder
        self.output_folder = output_folder
        self.categories = config.get("categories", {})
        self.category_keys = list(self.categories.keys())

    def get_excel_files(self):
        patterns = ["*.xls", "*.xlsx", "*.xlsm"]
        files = []
        for p in patterns:
            files.extend(glob.glob(os.path.join(self.input_folder, p)))
        return files

    def load_all_sheets(self, file_path):
        try:
            xls = pd.ExcelFile(file_path)
            sheets = []
            for sheet_name in xls.sheet_names:
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    df.columns = df.columns.str.strip()
                    for cfg in self.categories.values():
                        for col_key in ["schema_col", "table_col", "column_col"]:
                            col = cfg.get(col_key)
                            if col in df.columns:
                                df[col] = df[col].ffill()
                    sheets.append(df)
                except Exception as e:
                    print(f"‚ö†Ô∏è Error reading sheet {sheet_name}: {e}")
            return pd.concat(sheets, ignore_index=True) if sheets else pd.DataFrame()
        except Exception as e:
            print(f"‚ùå Error loading {file_path}: {e}")
            return pd.DataFrame()

    def create_row_mappings(self, df):
        row_mappings = []
        valid_rows = df.dropna(how="all")
        for idx, row in valid_rows.iterrows():
            mapping = {}
            for cat in self.category_keys:
                cfg = self.categories[cat]
                schema = str(row.get(cfg["schema_col"], "NA")).strip()
                table = str(row.get(cfg["table_col"], "NA")).strip()
                column = str(row.get(cfg["column_col"], "NA")).strip()
                table_comment = str(row.get(cfg.get("table_comment_col", ""), "")).strip()
                column_comment = str(row.get(cfg.get("column_comment_col", ""), "")).strip()

                # Clean up NaN values
                schema = "NA" if schema in ["nan", "None", ""] else schema
                table = "NA" if table in ["nan", "None", ""] else table
                column = "NA" if column in ["nan", "None", ""] else column
                table_comment = "" if table_comment in ["nan", "None"] else table_comment
                column_comment = "" if column_comment in ["nan", "None"] else column_comment

                mapping[cat] = {
                    "table": (schema, table),
                    "column": column,
                    "table_comment": table_comment,
                    "column_comment": column_comment,
                    "has_na": schema == "NA" or table == "NA" or column == "NA"
                }
            
            # Add row index for debugging
            mapping["_row_index"] = idx + 1
            row_mappings.append(mapping)
        return row_mappings

    def generate_sql_for_row(self, mapping):
        """Generate SQL statements per row with STRICT case logic."""
        original = mapping.get("Original_SSR", {})
        edl = mapping.get("EDL", {})
        rdmof = mapping.get("RDMOF", {})

        # Extract NA status - TRUE means category has NA values
        original_na = original.get("has_na", True)
        edl_na = edl.get("has_na", True)  
        rdmof_na = rdmof.get("has_na", True)

        row_idx = mapping.get("_row_index", "?")
        
        sql_statements = []

        print(f"  Row {row_idx}: Original_NA={original_na}, EDL_NA={edl_na}, RDMOF_NA={rdmof_na}")

        # CASE 1: All values present ‚Üí Original‚ÜíEDL + EDL‚ÜíRDMOF (2 statements)
        if not original_na and not edl_na and not rdmof_na:
            print(f"    ‚Üí CASE 1: All present - generating 2 statements")
            sql_statements.append(self.build_sql(original, edl, "Original_SSR ‚Üí EDL"))
            sql_statements.append(self.build_sql(edl, rdmof, "EDL ‚Üí RDMOF"))

        # CASE 2: EDL is NA, others present ‚Üí Original‚ÜíRDMOF (1 statement)  
        elif not original_na and edl_na and not rdmof_na:
            print(f"    ‚Üí CASE 2: EDL is NA - generating Original‚ÜíRDMOF")
            sql_statements.append(self.build_sql(original, rdmof, "Original_SSR ‚Üí RDMOF (EDL is NA)"))

        # CASE 3: RDMOF is NA, others present ‚Üí Original‚ÜíEDL (1 statement)
        elif not original_na and not edl_na and rdmof_na:
            print(f"    ‚Üí CASE 3: RDMOF is NA - generating Original‚ÜíEDL")
            sql_statements.append(self.build_sql(original, edl, "Original_SSR ‚Üí EDL (RDMOF is NA)"))

        # CASE 4: Original is NA, others present ‚Üí EDL‚ÜíRDMOF (1 statement)
        elif original_na and not edl_na and not rdmof_na:
            print(f"    ‚Üí CASE 4: Original is NA - generating EDL‚ÜíRDMOF")
            sql_statements.append(self.build_sql(edl, rdmof, "EDL ‚Üí RDMOF (Original_SSR is NA)"))

        # CASE 5: Multiple categories have NA ‚Üí Skip with comment
        else:
            print(f"    ‚Üí CASE 5: Multiple NAs - generating comment only")
            sql_statements.append(self.build_commented_placeholder(mapping, "Multiple categories have NA values"))

        print(f"    ‚Üí Generated {len(sql_statements)} statement(s)")
        return sql_statements

    def build_sql(self, src, tgt, description=""):
        """Build SQL statement with proper validation"""
        src_schema, src_table = src.get("table", ("NA", "NA"))
        tgt_schema, tgt_table = tgt.get("table", ("NA", "NA"))
        src_col = src.get("column", "NA")
        tgt_col = tgt.get("column", "NA")

        # Build comments section
        comments = []
        if description:
            comments.append(f"-- {description}")
        
        # Add table/column comments if they exist
        for comment_source in [src, tgt]:
            table_comment = comment_source.get("table_comment", "")
            column_comment = comment_source.get("column_comment", "")
            if table_comment:
                comments.extend([f"-- {line.strip()}" for line in str(table_comment).splitlines() if line.strip()])
            if column_comment:
                comments.extend([f"-- {line.strip()}" for line in str(column_comment).splitlines() if line.strip()])

        comment_text = "\n".join(comments)
        if comment_text:
            comment_text += "\n"

        # Check if source or target has NA values
        if src.get("has_na") or tgt.get("has_na"):
            return (
                f"{comment_text}"
                f"-- WARNING: Mapping contains NA values\n"
                f"-- {src_schema}.{src_table}.{src_col} ‚Üí {tgt_schema}.{tgt_table}.{tgt_col}\n"
                f"-- INSERT INTO {tgt_schema}.{tgt_table} ({tgt_col})\n"
                f"-- SELECT DISTINCT {src_col} FROM {src_schema}.{src_table};\n"
            )
        else:
            return (
                f"{comment_text}"
                f"INSERT INTO {tgt_schema}.{tgt_table} ({tgt_col})\n"
                f"SELECT DISTINCT {src_col} FROM {src_schema}.{src_table};\n"
            )

    def build_commented_placeholder(self, mapping, reason):
        """Build commented placeholder for rows that can't generate valid SQL"""
        row_idx = mapping.get("_row_index", "?")
        return (
            f"-- Row {row_idx}: {reason}\n"
            f"-- Original_SSR: {'NA' if mapping.get('Original_SSR', {}).get('has_na', True) else 'Valid'}\n"
            f"-- EDL: {'NA' if mapping.get('EDL', {}).get('has_na', True) else 'Valid'}\n"
            f"-- RDMOF: {'NA' if mapping.get('RDMOF', {}).get('has_na', True) else 'Valid'}\n"
            f"-- No valid single transformation possible\n"
        )

    def process_file(self, file_path):
        print(f"üìÑ Loading file: {os.path.basename(file_path)}")
        df = self.load_all_sheets(file_path)
        if df.empty:
            print("  ‚ùå No data found")
            return [], set()

        row_maps = self.create_row_mappings(df)
        print(f"  üìä Processing {len(row_maps)} rows")
        
        all_sql = []
        unique_set = set()

        for i, row_map in enumerate(row_maps, 1):
            sql_list = self.generate_sql_for_row(row_map)
            for sql in sql_list:
                # Normalize for duplicate detection
                sql_normalized = " ".join(sql.split())
                if sql_normalized not in unique_set:
                    all_sql.append(sql)
                    unique_set.add(sql_normalized)
                else:
                    print(f"    ‚ö†Ô∏è Skipping duplicate statement")

        print(f"  ‚úÖ Generated {len(all_sql)} unique SQL statements")
        return all_sql, unique_set

    def run(self):
        os.makedirs(self.output_folder, exist_ok=True)
        files = self.get_excel_files()
        if not files:
            print(f"‚ùå No Excel files found in {self.input_folder}")
            return

        total_unique = 0
        print(f"üîÑ Processing {len(files)} Excel file(s)...")

        for f in files:
            print(f"\n" + "="*50)
            sql_list, unique_sql = self.process_file(f)

            if sql_list:
                out_file = os.path.splitext(os.path.basename(f))[0] + ".sql"
                out_path = os.path.join(self.output_folder, out_file)
                with open(out_path, "w", encoding="utf-8") as fw:
                    fw.write(f"-- Generated SQL from: {os.path.basename(f)}\n")
                    fw.write(f"-- Total unique statements: {len(sql_list)}\n")
                    fw.write(f"-- Expected pattern: 1+1+1+1+1+1+2+2 = 10 statements\n\n")
                    fw.write("\n".join(sql_list))
                total_unique += len(unique_sql)
                print(f"‚úÖ Generated {out_file}: {len(unique_sql)} unique statements")
            else:
                print(f"‚ö†Ô∏è No valid SQL generated for {os.path.basename(f)}")

        print(f"\nüìä Final Summary:")
        print(f"   Total unique statements: {total_unique}")
        print(f"   Expected: 10 statements")
        print(f"   Status: {'‚úÖ CORRECT' if total_unique == 10 else '‚ùå INCORRECT - Check logic'}")


# Example usage and config
"""
config = {
    "categories": {
        "Original_SSR": {
            "schema_col": "Original SSR - Schema",
            "table_col": "Original SSR - Physical Table Name", 
            "column_col": "Original SSR - Physical Column Name",
            "table_comment_col": "Original SSR - Table Comment",
            "column_comment_col": "Original SSR - Column Comment"
        },
        "EDL": {
            "schema_col": "EDL- Schema",
            "table_col": "EDL - Physical Table Name",
            "column_col": "EDL - Physical Column Name", 
            "table_comment_col": "EDL - Table Comment",
            "column_comment_col": "EDL - Column Comment"
        },
        "RDMOF": {
            "schema_col": "RDMOF - Schema",
            "table_col": "RDMOF - Physical Table Name",
            "column_col": "RDMOF - Physical Column Name",
            "table_comment_col": "RDMOF - Table Comment", 
            "column_comment_col": "RDMOF - Column Comment"
        }
    }
}

generator = InsertSQLGenerator(config, "input_folder", "output_folder")
generator.run()
"""
