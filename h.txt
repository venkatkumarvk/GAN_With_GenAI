{
  "processing": {
    "document_types": {
      "invoice": {
        "extraction_fields": ["invoice_number", "total_amount", "date", "vendor"],
        "prompt_module": "invoice_prompt",
        "userspecific": ["Invoice ID", "Total Cost", "Invoice Date", "Company Name"]
      },
      "eob": {
        "extraction_fields": ["eob_number", "patient_name", "service_date", "paid_amount", "provider"],
        "prompt_module": "eob_prompt",
        "userspecific": ["EOB Reference", "Patient", "Service Date", "Payment", "Healthcare Provider"]
      },
      "claim": {
        "extraction_fields": ["claim_number", "patient_id", "diagnosis_code", "procedure_code", "claim_amount"],
        "prompt_module": "claim_prompt"
      }
    }
  }
}

-----------------------

  def process_azure_pdf_files(config, api_type, azure_folder, doc_type, logger):
    """
    Process PDF files from Azure Blob Storage with archiving support.
    """
    # Get document-specific configuration including model config and userspecific headers
    extraction_fields, systemprompt, prompt_template, model_config, userspecific_headers = get_document_config(config, doc_type, api_type)
    logger.info(f"Processing {doc_type} documents with fields: {extraction_fields}")
    logger.info(f"Using prompt module: {config['processing']['document_types'][doc_type]['prompt_module']}")
    
    # Log userspecific headers if provided
    if userspecific_headers:
        logger.info(f"Using custom CSV headers: {userspecific_headers}")
    else:
        logger.info("Using default CSV headers (same as extraction fields)")
    
    logger.info(f"Using Azure OpenAI {api_type} API configuration:")
    logger.info(f"  Endpoint: {model_config.get('azure_endpoint')}")
    logger.info(f"  Deployment: {model_config.get('deployment_name')}")
    logger.info(f"  API Version: {model_config.get('api_version')}")
    logger.info(f"  Model Name: {model_config.get('modelname')}")
    
    # ... rest of initialization code remains same ...
    
    storage_helper = AzureStorageHelper(
        config["azure_storage"]["connection_string"],
        config["azure_storage"]["input_container"],
        config["azure_storage"]["output_container"],
        archive_container,
        logger
    )
    
    # Pass document type, extraction fields, AND userspecific headers to PDFProcessor
    pdf_processor = PDFProcessor(config, logger, doc_type, extraction_fields, userspecific_headers)
    
    # Initialize AI client with document-specific model configuration
    logger.info(f"Initializing Azure OpenAI Client with {api_type} API for {doc_type}")
    ai_client = AzureOpenAIClient(model_config, logger)

  ------

  def get_document_config(config, doc_type, api_type):
    """
    Get configuration for a specific document type and API type.
    
    Args:
        config: Full configuration dictionary
        doc_type: Document type ('invoice', 'eob', 'claim')
        api_type: API type ('general', 'batch')
    
    Returns:
        tuple: (extraction_fields, systemprompt, prompt_template, model_config, userspecific_headers)
    """
    document_types = config["processing"]["document_types"]
    
    if doc_type not in document_types:
        raise ValueError(f"Unsupported document type: {doc_type}. Available types: {list(document_types.keys())}")
    
    doc_config = document_types[doc_type]
    extraction_fields = doc_config["extraction_fields"]
    prompt_module_name = doc_config["prompt_module"]
    
    # Get userspecific headers if provided
    userspecific_headers = doc_config.get("userspecific", None)
    
    # Validate userspecific headers length if provided
    if userspecific_headers:
        if len(userspecific_headers) != len(extraction_fields):
            raise ValueError(f"userspecific headers count ({len(userspecific_headers)}) must match extraction_fields count ({len(extraction_fields)}) for document type: {doc_type}")
    
    # Get model configuration based on API type
    if api_type == "general":
        azure_openai_config = config["azure_openai_general"]
    elif api_type == "batch":
        azure_openai_config = config["azure_openai_batch"]
    else:
        raise ValueError(f"Unsupported API type: {api_type}. Use 'general' or 'batch'")
    
    # Check if document type has specific model configuration
    if doc_type in azure_openai_config.get("models", {}):
        model_config = azure_openai_config["models"][doc_type].copy()
    else:
        # Use default configuration
        model_config = azure_openai_config.get("default", {}).copy()
    
    # Dynamically import the prompt module
    try:
        prompt_module = importlib.import_module(prompt_module_name)
        
        # Get systemprompt and prompt from the module
        if not hasattr(prompt_module, 'systemprompt'):
            raise AttributeError(f"Module '{prompt_module_name}' missing 'systemprompt' variable")
        if not hasattr(prompt_module, 'prompt'):
            raise AttributeError(f"Module '{prompt_module_name}' missing 'prompt' variable")
            
        systemprompt = prompt_module.systemprompt
        prompt_template = prompt_module.prompt
        
    except ImportError as e:
        raise ImportError(f"Could not import prompt module '{prompt_module_name}': {str(e)}")
    except AttributeError as e:
        raise AttributeError(f"Error accessing prompt variables: {str(e)}")
    
    return extraction_fields, systemprompt, prompt_template, model_config, userspecific_headers

  ----
  class PDFProcessor:
    def __init__(self, config, logger, doc_type=None, extraction_fields=None, userspecific_headers=None):
        self.config = config
        self.logger = logger
        self.doc_type = doc_type or "invoice"
        self.extraction_fields = extraction_fields or config["processing"].get("extraction_fields", ["invoice_number", "total_amount", "date", "vendor"])
        self.userspecific_headers = userspecific_headers
        
        # Log the processor configuration
        self.logger.info(f"PDFProcessor initialized for document type: {self.doc_type}")
        self.logger.info(f"Extraction fields: {self.extraction_fields}")
        if self.userspecific_headers:
            self.logger.info(f"Custom CSV headers: {self.userspecific_headers}")
        else:
            self.logger.info("Using default CSV headers (same as extraction fields)")

  -----
  def create_csv_for_results(self, results, filename):
    """
    Create CSV content from extraction results, adapting to different document types.
    Uses userspecific headers if provided, otherwise uses extraction_fields.
    
    Returns:
        tuple: (csv_content, primary_field_value, secondary_field_value)
    """
    if not results:
        return None, "unknown", "unknown"
    
    try:
        # Create CSV header based on userspecific headers or extraction fields
        csv_lines = []
        
        # Determine which headers to use
        if self.userspecific_headers:
            field_headers = self.userspecific_headers
        else:
            field_headers = self.extraction_fields
        
        # Create header row
        header = ["filename", "page_number", "document_type"] + field_headers + ["confidence"]
        csv_lines.append(",".join(header))
        
        # Initialize field values for filename generation
        primary_field_value = "unknown"
        secondary_field_value = "unknown"
        
        # Process each result
        for page_num, category, extracted_data in results:
            row = [filename, str(page_num + 1), self.doc_type]
            
            # Extract field values based on document type
            if extracted_data and isinstance(extracted_data, dict):
                extractions = extracted_data.get("extractions", {})
                
                # Add field values to row (still use extraction_fields for data extraction)
                for field in self.extraction_fields:
                    field_data = extractions.get(field, {})
                    value = field_data.get("value", "N/A") if isinstance(field_data, dict) else str(field_data)
                    row.append(f'"{value}"')  # Quote values to handle commas
                    
                    # Set primary and secondary field values for filename
                    if field == self.extraction_fields[0] and value != "N/A":  # First field as primary
                        primary_field_value = str(value).replace('"', '').replace(',', '_')
                    elif len(self.extraction_fields) > 1 and field == self.extraction_fields[1] and value != "N/A":  # Second field as secondary
                        secondary_field_value = str(value).replace('"', '').replace(',', '_')
                
                # Add overall confidence
                confidence = extracted_data.get("overall_confidence", 0)
                row.append(str(confidence))
            else:
                # If no extracted data, fill with N/A
                row.extend(["N/A"] * len(self.extraction_fields))
                row.append("0")
            
            csv_lines.append(",".join(row))
        
        csv_content = "\n".join(csv_lines)
        
        # Log which headers were used
        if self.userspecific_headers:
            self.logger.debug(f"Created CSV with {len(results)} rows using custom headers: {field_headers}")
        else:
            self.logger.debug(f"Created CSV with {len(results)} rows using default headers: {field_headers}")
        
        return csv_content, primary_field_value, secondary_field_value
        
    except Exception as e:
        self.logger.error(f"Error creating CSV: {str(e)}")
        return None, "error", "error"

----

---
from datetime import datetime
import pandas as pd

def create_csv_for_results(self, extracted_results, filename):
    pdf_rows = []
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Use user-specific headers if available, else fall back
    if self.userspecific_headers is not None and len(self.userspecific_headers) > 0:
        fieldheader = self.userspecific_headers
    else:
        fieldheader = self.extraction_fields

    for page_num, category, data in extracted_results:
        if category != f"{self.doc_type}" or data is None:
            continue

        row_data = {
            "Filename": filename,
            "Page": page_num + 1,
            "Extraction_Timestamp": timestamp
        }

        for field in fieldheader:
            if field in data:
                field_data = data[field]
                if isinstance(field_data, dict):
                    value = field_data.get("value", "N/A")
                    confidence = field_data.get("confidence", 0)
                else:
                    value = field_data if field_data else "N/A"
                    confidence = 0

                row_data[field] = value
                row_data[f"{field}_Confidence"] = round(confidence * 100, 2)
            else:
                continue  # skip missing fields

        pdf_rows.append(row_data)

    if pdf_rows:
        pdf_df = pd.DataFrame(pdf_rows, dtype=str)

        # Automatically generate mapping if user-specific headers exist
        if self.userspecific_headers is not None and len(self.userspecific_headers) > 0:
            # Example display names (you can customize this mapping logic as needed)
            user_friendly_names = [f.replace("_", " ").title() for f in self.userspecific_headers]

            # Create the mapping
            rename_mapping = {}
            for original, friendly in zip(self.userspecific_headers, user_friendly_names):
                rename_mapping[original] = friendly
                rename_mapping[f"{original}_Confidence"] = f"{friendly} Confidence"

            # Add system fields if needed
            # (optional, rename Page, Filename, etc.)

            pdf_df = pdf_df.rename(columns=rename_mapping)

        pdf_csv = pdf_df.to_csv(index=False)
        return pdf_csv

    return None


---
if pdf_rows:
    pdf_df = pd.DataFrame(pdf_rows, dtype=str)

    # Rename columns if userspecific headers exist
    if self.userspecific_headers is not None and len(self.userspecific_headers) > 0:
        base_cols = ["Filename", "Page", "Extraction_Timestamp"]
        confidence_suffix = "_Confidence"

        rename_dict = {}
        for old_field, new_field in zip(self.extraction_fields, self.userspecific_headers):
            rename_dict[old_field] = new_field
            rename_dict[f"{old_field}{confidence_suffix}"] = f"{new_field}{confidence_suffix}"

        pdf_df.rename(columns=rename_dict, inplace=True)

    pdf_csv = pdf_df.to_csv(index=False)
    return pdf_csv

