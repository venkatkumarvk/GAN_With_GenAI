# insert_helper.py
import os
import json
import pandas as pd
from typing import List, Dict, Tuple, Optional
from collections import defaultdict

class InsertSQLGenerator:
    def __init__(self,
                 excel_file_path: str,
                 output_folder: str = "generated_inserts",
                 sheet_name: Optional[str] = None,
                 categories: Optional[Dict[str, Dict[str, str]]] = None):
        self.excel_file_path = excel_file_path
        self.output_folder = output_folder
        self.sheet_name = sheet_name
        self.categories = categories or {}
        self.category_keys = list(self.categories.keys())  # preserve order

    def load_excel_data(self) -> pd.DataFrame:
        """Load and clean Excel data"""
        df = pd.read_excel(self.excel_file_path, sheet_name=self.sheet_name)
        df.columns = df.columns.str.strip()
        
        # Forward fill for schema, table, and column names to handle merged cells
        for cfg in self.categories.values():
            for col_key in ['schema_col', 'table_col', 'column_col']:
                col = cfg.get(col_key)
                if col in df.columns:
                    df[col] = df[col].ffill()
        return df

    def extract_column_mappings(self, df: pd.DataFrame, cat: str) -> Dict[Tuple[str, str], List[str]]:
        """Extract schema.table -> [columns] mapping for a category"""
        cfg = self.categories[cat]
        schema_col = cfg['schema_col']
        table_col = cfg['table_col']
        column_col = cfg['column_col']

        result = defaultdict(list)
        filtered_df = df[
            df[schema_col].notna() &
            df[table_col].notna() &
            df[column_col].notna()
        ]

        for _, row in filtered_df.iterrows():
            schema = str(row[schema_col]).strip()
            table = str(row[table_col]).strip()
            column = str(row[column_col]).strip()
            result[(schema, table)].append(column)

        return result

    def generate_insert_sql(self, src_table: Tuple[str, str], src_col: str,
                           tgt_table: Tuple[str, str], tgt_col: str) -> str:
        """Generate a single INSERT statement"""
        src_schema, src_tab = src_table
        tgt_schema, tgt_tab = tgt_table

        return (
            f"-- Insert from {src_schema}.{src_tab}.{src_col} to {tgt_schema}.{tgt_tab}.{tgt_col}\n"
            f"INSERT INTO {tgt_schema}.{tgt_tab} ({tgt_col})\n"
            f"SELECT DISTINCT {src_col} FROM {src_schema}.{src_tab};\n"
        )

    def create_table_column_pairs(self, df: pd.DataFrame) -> List[Dict]:
        """Create ordered pairs of (source_category, target_category, table_mappings)"""
        pairs = []
        seen_mappings = set()  # Track unique mappings to prevent duplicates
        
        # Get all unique rows that have data across all categories
        valid_rows = df.dropna(subset=[
            col for cfg in self.categories.values() 
            for col in [cfg['schema_col'], cfg['table_col'], cfg['column_col']]
        ])
        
        if valid_rows.empty:
            print("‚ö†Ô∏è Warning: No valid rows found with data across all categories")
            return pairs

        # Group by row index to maintain relationships
        for idx, row in valid_rows.iterrows():
            row_mappings = {}
            
            # Extract table and column info for each category in this row
            for cat in self.category_keys:
                cfg = self.categories[cat]
                schema = str(row[cfg['schema_col']]).strip()
                table = str(row[cfg['table_col']]).strip()
                column = str(row[cfg['column_col']]).strip()
                
                row_mappings[cat] = {
                    'table': (schema, table),
                    'column': column
                }
            
            # Create a unique signature for this mapping to detect duplicates
            mapping_signature = tuple(
                (cat, info['table'][0], info['table'][1], info['column'])
                for cat, info in row_mappings.items()
            )
            
            if mapping_signature not in seen_mappings:
                pairs.append(row_mappings)
                seen_mappings.add(mapping_signature)
            else:
                print(f"üîÑ Skipping duplicate mapping at row {idx}")
        
        return pairs

    def run(self):
        """Main execution method"""
        print(f"üîÑ Loading data from: {self.excel_file_path}")
        df = self.load_excel_data()
        os.makedirs(self.output_folder, exist_ok=True)

        # Get table-column pairs that maintain row relationships
        table_column_pairs = self.create_table_column_pairs(df)
        
        if not table_column_pairs:
            print("‚ùå No valid data found to generate INSERT statements")
            return

        print(f"üìä Found {len(table_column_pairs)} data row(s) to process")

        all_insert_sqls = []
        files_generated = []

        # Generate INSERT statements for each category transition
        for i in range(len(self.category_keys) - 1):
            cat_src = self.category_keys[i]
            cat_tgt = self.category_keys[i + 1]

            insert_statements = []
            unique_sql_statements = set()  # Track unique SQL statements
            insert_count = 0
            duplicate_count = 0

            print(f"\nüîÑ Processing: {cat_src} ‚Üí {cat_tgt}")

            for pair_data in table_column_pairs:
                if cat_src in pair_data and cat_tgt in pair_data:
                    src_info = pair_data[cat_src]
                    tgt_info = pair_data[cat_tgt]
                    
                    sql = self.generate_insert_sql(
                        src_info['table'], 
                        src_info['column'],
                        tgt_info['table'], 
                        tgt_info['column']
                    )
                    
                    # Create a normalized version for duplicate detection
                    sql_normalized = ' '.join(sql.split())  # Remove extra whitespace
                    
                    if sql_normalized not in unique_sql_statements:
                        insert_statements.append(sql)
                        all_insert_sqls.append(sql)
                        unique_sql_statements.add(sql_normalized)
                        insert_count += 1
                    else:
                        duplicate_count += 1
                        print(f"üîÑ Skipping duplicate SQL: {src_info['table'][0]}.{src_info['table'][1]}.{src_info['column']} ‚Üí {tgt_info['table'][0]}.{tgt_info['table'][1]}.{tgt_info['column']}")

            # Write individual category file
            if insert_statements:
                file_name = f"{cat_src}_to_{cat_tgt}.sql"
                file_path = os.path.join(self.output_folder, file_name)
                
                with open(file_path, "w", encoding='utf-8') as f:
                    f.write(f"-- INSERT statements from {cat_src} to {cat_tgt}\n")
                    f.write(f"-- Generated {insert_count} unique statements")
                    if duplicate_count > 0:
                        f.write(f" (skipped {duplicate_count} duplicates)")
                    f.write(f"\n\n")
                    f.write("\n".join(insert_statements))
                
                files_generated.append(file_name)
                status = f"‚úÖ Generated {insert_count} unique INSERT statements"
                if duplicate_count > 0:
                    status += f" (skipped {duplicate_count} duplicates)"
                print(f"{status} ‚Üí {file_name}")
            else:
                print(f"‚ö†Ô∏è No INSERT statements generated for {cat_src} ‚Üí {cat_tgt}")

        # Write combined file
        if all_insert_sqls:
            combined_file = "all_insert_statements.sql"
            combined_path = os.path.join(self.output_folder, combined_file)
            
            with open(combined_path, "w", encoding='utf-8') as f:
                f.write(f"-- All INSERT statements combined\n")
                f.write(f"-- Total unique statements: {len(all_insert_sqls)}\n")
                f.write(f"-- Categories: {' ‚Üí '.join(self.category_keys)}\n\n")
                f.write("\n".join(all_insert_sqls))
            
            files_generated.append(combined_file)

        # Summary
        print(f"\nüì¶ Summary:")
        print(f"   ‚Ä¢ Total unique INSERT statements: {len(all_insert_sqls)}")
        print(f"   ‚Ä¢ Files generated: {len(files_generated)}")
        print(f"   ‚Ä¢ Output folder: {self.output_folder}")
        print(f"\nüìÅ Generated files:")
        for file_name in files_generated:
            print(f"   ‚Ä¢ {file_name}")

# main.py
import json
from insert_helper import InsertSQLGenerator

def main():
    try:
        with open("schema_config.json", 'r') as f:
            config = json.load(f)
        
        print("üöÄ Starting INSERT SQL Generation...")
        print(f"üìÑ Config loaded from: schema_config.json")
        
        generator = InsertSQLGenerator(
            excel_file_path=config["excel_file_path"],
            output_folder=config.get("output_folder", "generated_inserts"),
            sheet_name=config.get("sheet_name"),
            categories=config["categories"]
        )
        generator.run()
        
    except FileNotFoundError:
        print("‚ùå Error: schema_config.json not found")
        print("Please ensure the config file exists in the same directory")
    except KeyError as e:
        print(f"‚ùå Error: Missing required config key: {e}")
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")

if __name__ == "__main__":
    main()

# Example schema_config.json
"""
{
  "excel_file_path": "your_excel_file.xlsx",
  "sheet_name": "Sheet1",
  "output_folder": "generated_inserts",
  "categories": {
    "Original_SSR": {
      "schema_col": "Original SSR - Schema",
      "table_col": "Original SSR - Physical Table Name",
      "column_col": "Original SSR - Physical Column Name"
    },
    "EDL": {
      "schema_col": "EDL- Schema",
      "table_col": "EDL - Physical Table Name",
      "column_col": "EDL - Physical Column Name"
    },
    "RDMOF": {
      "schema_col": "RDMOF - Schema",
      "table_col": "RDMOF - Physical Table Name",
      "column_col": "RDMOF - Physical Column Name"
    }
  }
}
"""
