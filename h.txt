#multiple

import pandas as pd
import re
from typing import List, Tuple, Dict


class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str):
        self.excel_file_path = excel_file_path

    def load_excel_data(self) -> pd.DataFrame:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            sheets = excel_file.sheet_names
            print(f"üìÑ Available sheets: {sheets}")
            sheet = next((s for s in sheets if 'BROKER_GROUP_RELATION' in s.upper()), sheets[0])
            df = pd.read_excel(excel_file, sheet_name=sheet)
            print(f"‚úÖ Loaded sheet: {sheet} with {df.shape[0]} rows and {df.shape[1]} columns.")
            return df
        except Exception as e:
            print(f"‚ùå Error loading Excel file: {e}")
            return None

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype):
            return 'VARCHAR(255)'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'VARCHAR(1)'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'VARCHAR(255)'

    def extract_category_columns(self, df: pd.DataFrame) -> Dict[str, List[Tuple[str, str]]]:
        """
        Extract columns separately for RDMOF, EDL, and Original SSR.
        RDMOF uses datatype column; others default to VARCHAR(255)
        """
        output = {
            "RDMOF": [],
            "EDL": [],
            "Original_SSR": []
        }
        seen = {
            "RDMOF": set(),
            "EDL": set(),
            "Original_SSR": set()
        }

        column_map = {}
        for col in df.columns:
            col_upper = col.upper()
            if 'RDMOF' in col_upper and 'COLUMN' in col_upper:
                column_map['RDMOF_column'] = col
            elif 'EDL' in col_upper and 'COLUMN' in col_upper:
                column_map['EDL_column'] = col
            elif ('SSR' in col_upper or 'ORIGINAL' in col_upper) and 'COLUMN' in col_upper:
                column_map['SSR_column'] = col
            elif 'RDMOF' in col_upper and 'DATA' in col_upper:
                column_map['datatype'] = col

        if 'datatype' not in column_map:
            print("‚ö†Ô∏è 'RDMOF - Data Type' column not found.")

        for _, row in df.iterrows():
            rd_dtype = row.get(column_map.get('datatype', ''), None)

            # RDMOF
            if 'RDMOF_column' in column_map:
                col = row.get(column_map['RDMOF_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen["RDMOF"]:
                        dtype = self.map_datatype(rd_dtype)
                        output["RDMOF"].append((name, dtype))
                        seen["RDMOF"].add(name.lower())

            # EDL
            if 'EDL_column' in column_map:
                col = row.get(column_map['EDL_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen["EDL"]:
                        output["EDL"].append((name, 'VARCHAR(255)'))
                        seen["EDL"].add(name.lower())

            # SSR
            if 'SSR_column' in column_map:
                col = row.get(column_map['SSR_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen["Original_SSR"]:
                        output["Original_SSR"].append((name, 'VARCHAR(255)'))
                        seen["Original_SSR"].add(name.lower())

        return output

    def generate_schema_sql(self, table_name: str, columns: List[Tuple[str, str]]) -> str:
        if not columns:
            return f"-- No columns found for {table_name}"
        sql = f"-- {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.EDM_Reporting.{table_name} (\n"
        sql += ",\n".join([f"    [{col}] {dtype}" for col, dtype in columns])
        sql += "\n);"
        return sql

    def run(self):
        print("üöÄ Starting separate schema generation...")
        df = self.load_excel_data()
        if df is None:
            return

        category_columns = self.extract_category_columns(df)

        for category, columns in category_columns.items():
            table_name = f"{category.upper()}_TABLE"
            sql = self.generate_schema_sql(table_name, columns)

            print(f"\nüìÑ {category} Schema:\n{sql}")
            filename = f"{category.lower()}_table.sql"
            with open(filename, "w") as f:
                f.write(sql)
            print(f"‚úÖ Saved to file: {filename}")


# Usage
def main():
    excel_path = "your_excel_file.xlsx"  # Replace with your actual Excel path
    generator = DatabricksSchemaGenerator(excel_path)
    generator.run()

if __name__ == "__main__":
    main()


# singe file sql stored

import pandas as pd
import re
from typing import List, Tuple


class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str):
        self.excel_file_path = excel_file_path

    def load_excel_data(self) -> pd.DataFrame:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            sheets = excel_file.sheet_names
            print(f"üìÑ Available sheets: {sheets}")
            sheet = next((s for s in sheets if 'BROKER_GROUP_RELATION' in s.upper()), sheets[0])
            df = pd.read_excel(excel_file, sheet_name=sheet)
            print(f"‚úÖ Loaded sheet: {sheet} with {df.shape[0]} rows and {df.shape[1]} columns.")
            return df
        except Exception as e:
            print(f"‚ùå Error loading Excel file: {e}")
            return None

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype):
            return 'VARCHAR(255)'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'VARCHAR(1)'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'VARCHAR(255)'

    def extract_all_columns(self, df: pd.DataFrame) -> List[Tuple[str, str]]:
        """
        Extract and combine columns from RDMOF, EDL, SSR.
        Use RDMOF - Data Type only for RDMOF; others default to VARCHAR(255)
        """
        output = []
        seen = set()

        column_map = {}
        for col in df.columns:
            col_upper = col.upper()
            if 'RDMOF' in col_upper and 'COLUMN' in col_upper:
                column_map['RDMOF_column'] = col
            elif 'EDL' in col_upper and 'COLUMN' in col_upper:
                column_map['EDL_column'] = col
            elif ('SSR' in col_upper or 'ORIGINAL' in col_upper) and 'COLUMN' in col_upper:
                column_map['SSR_column'] = col
            elif 'RDMOF' in col_upper and 'DATA' in col_upper:
                column_map['datatype'] = col

        if 'datatype' not in column_map:
            print("‚ö†Ô∏è 'RDMOF - Data Type' column not found.")
            return output

        for _, row in df.iterrows():
            rd_dtype = row.get(column_map['datatype'], None)

            # RDMOF
            if 'RDMOF_column' in column_map:
                col = row.get(column_map['RDMOF_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen:
                        dtype = self.map_datatype(rd_dtype)
                        output.append((name, dtype))
                        seen.add(name.lower())

            # EDL
            if 'EDL_column' in column_map:
                col = row.get(column_map['EDL_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen:
                        output.append((name, 'VARCHAR(255)'))
                        seen.add(name.lower())

            # SSR
            if 'SSR_column' in column_map:
                col = row.get(column_map['SSR_column'])
                if pd.notna(col):
                    name = str(col).strip()
                    if name.lower() not in seen:
                        output.append((name, 'VARCHAR(255)'))
                        seen.add(name.lower())

        print(f"‚úÖ Extracted {len(output)} unique columns.")
        return output

    def generate_schema_sql(self, table_name: str, columns: List[Tuple[str, str]]) -> str:
        if not columns:
            return f"-- No columns found for schema generation."
        sql = f"-- {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.EDM_Reporting.{table_name} (\n"
        sql += ",\n".join([f"    [{col}] {dtype}" for col, dtype in columns])
        sql += "\n);"
        return sql

    def run(self):
        print("üöÄ Starting schema generation for BROKER_GROUP_RELATION...")
        df = self.load_excel_data()
        if df is None:
            return

        columns = self.extract_all_columns(df)
        sql = self.generate_schema_sql("BROKER_GROUP_RELATION", columns)

        print("\n" + "="*60)
        print("üìÑ GENERATED SQL SCHEMA")
        print("="*60)
        print(sql)

        with open("BROKER_GROUP_RELATION.sql", "w") as f:
            f.write(sql)
        print("\n‚úÖ Schema saved to 'BROKER_GROUP_RELATION.sql'")


# Usage
def main():
    excel_path = "your_excel_file.xlsx"  # üîÅ Replace with your actual file
    generator = DatabricksSchemaGenerator(excel_path)
    generator.run()

if __name__ == "__main__":
    main()
