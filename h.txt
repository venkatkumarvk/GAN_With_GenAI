import pandas as pd
import re
import os
from typing import List, Tuple, Dict
from collections import defaultdict


class DatabricksSchemaGenerator:
    def __init__(self, excel_file_path: str, output_base_folder: str = "generated_schemas"):
        self.excel_file_path = excel_file_path
        self.output_base_folder = output_base_folder
        self.tables_created = 0  # Track number of tables created

    def load_excel_data(self) -> pd.DataFrame:
        try:
            excel_file = pd.ExcelFile(self.excel_file_path)
            sheets = excel_file.sheet_names
            print(f"üìÑ Available sheets: {sheets}")
            sheet = next((s for s in sheets if 'BROKER_GROUP_RELATION' in s.upper()), sheets[0])
            df = pd.read_excel(excel_file, sheet_name=sheet)
            print(f"‚úÖ Loaded sheet: {sheet} with {df.shape[0]} rows and {df.shape[1]} columns.")
            return df
        except Exception as e:
            print(f"‚ùå Error loading Excel file: {e}")
            return None

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype):
            return 'VARCHAR(255)'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'VARCHAR(1)'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'VARCHAR(255)'

    def extract_tables_by_physical_name(self, df: pd.DataFrame) -> Dict[str, Dict[str, List[Tuple[str, str]]]]:
        result = {
            "EDL": defaultdict(list),
            "Original_SSR": defaultdict(list),
            "RDMOF": defaultdict(list)
        }
        seen_columns = {
            "EDL": defaultdict(set),
            "Original_SSR": defaultdict(set),
            "RDMOF": defaultdict(set)
        }

        column_map = {}
        for col in df.columns:
            col_upper = col.upper()
            if 'EDL' in col_upper and 'PHYSICAL' in col_upper and 'TABLE' in col_upper:
                column_map['EDL_table'] = col
            elif ('SSR' in col_upper or 'ORIGINAL' in col_upper) and 'PHYSICAL' in col_upper and 'TABLE' in col_upper:
                column_map['SSR_table'] = col
            elif 'RDMOF' in col_upper and 'PHYSICAL' in col_upper and 'TABLE' in col_upper:
                column_map['RDMOF_table'] = col
            elif 'EDL' in col_upper and 'COLUMN' in col_upper:
                column_map['EDL_column'] = col
            elif ('SSR' in col_upper or 'ORIGINAL' in col_upper) and 'COLUMN' in col_upper:
                column_map['SSR_column'] = col
            elif 'RDMOF' in col_upper and 'COLUMN' in col_upper:
                column_map['RDMOF_column'] = col
            elif 'RDMOF' in col_upper and 'DATA' in col_upper:
                column_map['datatype'] = col

        print(f"üîç Found column mappings: {column_map}")

        for _, row in df.iterrows():
            rd_dtype = row.get(column_map.get('datatype', ''), None)

            # EDL
            if 'EDL_table' in column_map and 'EDL_column' in column_map:
                table_name = row.get(column_map['EDL_table'])
                column_name = row.get(column_map['EDL_column'])
                if pd.notna(table_name) and pd.notna(column_name):
                    table_name = str(table_name).strip()
                    column_name = str(column_name).strip()
                    if column_name.lower() not in seen_columns["EDL"][table_name]:
                        result["EDL"][table_name].append((column_name, 'VARCHAR(255)'))
                        seen_columns["EDL"][table_name].add(column_name.lower())

            # SSR
            if 'SSR_table' in column_map and 'SSR_column' in column_map:
                table_name = row.get(column_map['SSR_table'])
                column_name = row.get(column_map['SSR_column'])
                if pd.notna(table_name) and pd.notna(column_name):
                    table_name = str(table_name).strip()
                    column_name = str(column_name).strip()
                    if column_name.lower() not in seen_columns["Original_SSR"][table_name]:
                        result["Original_SSR"][table_name].append((column_name, 'VARCHAR(255)'))
                        seen_columns["Original_SSR"][table_name].add(column_name.lower())

            # RDMOF
            if 'RDMOF_table' in column_map and 'RDMOF_column' in column_map:
                table_name = row.get(column_map['RDMOF_table'])
                column_name = row.get(column_map['RDMOF_column'])
                if pd.notna(table_name) and pd.notna(column_name):
                    table_name = str(table_name).strip()
                    column_name = str(column_name).strip()
                    if column_name.lower() not in seen_columns["RDMOF"][table_name]:
                        dtype = self.map_datatype(rd_dtype)
                        result["RDMOF"][table_name].append((column_name, dtype))
                        seen_columns["RDMOF"][table_name].add(column_name.lower())

        return result

    def create_folder_structure(self):
        folders = ["EDL", "Original_SSR", "RDMOF"]
        created_folders = []
        if not os.path.exists(self.output_base_folder):
            os.makedirs(self.output_base_folder)
            print(f"üìÅ Created base folder: {self.output_base_folder}")

        for folder in folders:
            folder_path = os.path.join(self.output_base_folder, folder)
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)
                created_folders.append(folder)
                print(f"üìÅ Created folder: {folder_path}")

        if created_folders:
            print(f"‚úÖ Created {len(created_folders)} category folders")

        return True

    def generate_schema_sql(self, table_name: str, columns: List[Tuple[str, str]], category: str) -> str:
        if not columns:
            return f"-- No columns found for {table_name}"
        sql = f"-- {category} - {table_name} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.EDM_Reporting.{table_name} (\n"
        sql += ",\n".join([f"    [{col}] {dtype}" for col, dtype in columns])
        sql += "\n);"
        return sql

    def run(self):
        print("üöÄ Starting schema generation by Physical Table Names...")
        self.create_folder_structure()
        df = self.load_excel_data()
        if df is None:
            return

        tables_by_category = self.extract_tables_by_physical_name(df)
        total_tables = 0
        category_stats = {}

        for category, tables in tables_by_category.items():
            category_tables = 0
            category_stats[category] = {}
            print(f"\n{'='*60}")
            print(f"üìä {category} CATEGORY")
            print(f"{'='*60}")
            if not tables:
                print(f"‚ö†Ô∏è No tables found for {category}")
                continue

            category_folder = os.path.join(self.output_base_folder, category)

            for table_name, columns in tables.items():
                if columns:
                    sql = self.generate_schema_sql(table_name, columns, category)
                    print(f"\nüìÑ {table_name} ({len(columns)} columns):\n{sql}")
                    filename = f"{table_name.lower()}.sql"
                    file_path = os.path.join(category_folder, filename)
                    with open(file_path, "w") as f:
                        f.write(sql)
                    print(f"‚úÖ Saved to: {file_path}")
                    category_tables += 1
                    category_stats[category][table_name] = len(columns)
                else:
                    print(f"‚ö†Ô∏è No columns found for {table_name} - skipping")

            total_tables += category_tables
            print(f"\nüìà {category} Summary: {category_tables} tables created in {category_folder}")

        self.tables_created = total_tables

        print(f"\n{'='*60}")
        print(f"üéØ FINAL SUMMARY")
        print(f"{'='*60}")
        print(f"üèóÔ∏è  Total tables created: {self.tables_created}")
        print(f"üìÅ Total SQL files generated: {self.tables_created}")
        print(f"üìÇ Output folder: {self.output_base_folder}\n")

        print(f"üìÇ Folder Structure:")
        print(f"   {self.output_base_folder}/")
        for category, tables in category_stats.items():
            if tables:
                print(f"   ‚îú‚îÄ‚îÄ {category}/")
                for table_name, column_count in tables.items():
                    print(f"   ‚îÇ   ‚îú‚îÄ‚îÄ {table_name.lower()}.sql ({column_count} columns)")
            else:
                print(f"   ‚îú‚îÄ‚îÄ {category}/ (empty)")
        print(f"{'='*60}")

    def get_table_count(self) -> int:
        return self.tables_created


# Usage
def main():
    excel_path = "your_excel_file.xlsx"  # Replace with your actual Excel file path
    output_folder = "generated_schemas"  # Replace with desired output folder

    generator = DatabricksSchemaGenerator(excel_path, output_folder)
    generator.run()

    print(f"\nüî¢ Total tables created: {generator.get_table_count()}")


if __name__ == "__main__":
    main()


#test (pyspark and pytest)
import os
import pytest
from pyspark.sql import SparkSession

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder \
        .appName("SchemaValidationTest") \
        .master("local[*]") \
        .config("spark.sql.catalogImplementation", "in-memory") \
        .getOrCreate()
    yield spark
    spark.stop()


def run_sql_file(spark, file_path):
    with open(file_path, "r") as f:
        sql_query = f.read()
    spark.sql(sql_query)


def test_schema_creation(spark):
    # Replace with a valid path to one of your generated .sql files
    sql_file_path = "generated_schemas/RDMOF/sample_table.sql"
    assert os.path.exists(sql_file_path), f"SQL file not found: {sql_file_path}"

    run_sql_file(spark, sql_file_path)

    # Extract table name from file name
    table_name = os.path.basename(sql_file_path).replace(".sql", "")
    db = "external_catalog.edm_reporting"

    # Validate the table exists
    tables = spark.sql(f"SHOW TABLES IN {db}").collect()
    found = any(row.tableName.lower() == table_name.lower() for row in tables)

    assert found, f"Table '{table_name}' not found in database '{db}'"

    # Clean up (optional)
    spark.sql(f"DROP TABLE IF EXISTS {db}.{table_name}")
############################
import sqlite3

def test_sqlite_create(sql_file):
    with open(sql_file, 'r') as f:
        sql = f.read()

    # Simplify for SQLite (remove catalog/schema and brackets)
    sql = sql.replace("external_catalog.EDM_Reporting.", "").replace("[", "").replace("]", "")
    sql = sql.replace("VARCHAR(255)", "TEXT").replace("BIGINT", "INTEGER").replace("DECIMAL", "REAL")

    try:
        conn = sqlite3.connect(":memory:")  # In-memory DB
        cursor = conn.cursor()
        cursor.execute(sql)
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()
        print("‚úÖ Created tables:", tables)
        return True
    except Exception as e:
        print("‚ùå SQL Execution failed:", e)
        return False

# Example usage
sql_path = "generated_schemas/RDMOF/sample_table.sql"
test_sqlite_create(sql_path)


########################3
import os
import glob
import pytest
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

@pytest.fixture(scope="module")
def engine():
    """Create an in-memory SQLite engine"""
    return create_engine("sqlite:///:memory:")

def run_sql_file(engine, sql_file_path):
    with open(sql_file_path, "r") as f:
        sql = f.read()

    with engine.connect() as conn:
        try:
            conn.execute(text(sql))
            return True, None
        except SQLAlchemyError as e:
            return False, str(e)

def get_created_table_name(sql_file_path: str) -> str:
    """Extract table name from SQL file"""
    with open(sql_file_path, "r") as f:
        for line in f:
            if "CREATE TABLE" in line.upper():
                # Example: CREATE TABLE IF NOT EXISTS my_table (
                parts = line.replace("(", "").split()
                if "TABLE" in parts:
                    idx = parts.index("TABLE")
                    return parts[idx + 1].replace("[", "").replace("]", "").strip()
    return None

def describe_table(engine, table_name):
    """Print table schema"""
    with engine.connect() as conn:
        try:
            result = conn.execute(text(f"PRAGMA table_info({table_name})"))
            rows = result.fetchall()
            print(f"\nüßæ Schema for table `{table_name}`:")
            for row in rows:
                print(f" - {row['name']} ({row['type']})")
        except Exception as e:
            print(f"‚ùå Error describing table {table_name}: {e}")

@pytest.mark.parametrize("sql_file", glob.glob("generated_schemas/**/*.sql", recursive=True))
def test_sql_execution_and_describe(engine, sql_file):
    assert os.path.exists(sql_file), f"‚ùå SQL file not found: {sql_file}"

    # Run the SQL file
    success, error = run_sql_file(engine, sql_file)
    assert success, f"‚ùå Failed to execute SQL file '{sql_file}':\n{error}"
    print(f"‚úÖ Successfully executed: {sql_file}")

    # Extract and describe table
    table_name = get_created_table_name(sql_file)
    assert table_name, f"‚ùå Could not extract table name from {sql_file}"

    describe_table(engine, table_name)
pip install sqlalchemy pytest


###############333
import os
import sqlite3
import re
import pytest


def sanitize_sql(sql_text: str) -> str:
    """
    Convert Databricks/ANSI SQL to SQLite-compatible SQL.
    Removes brackets, changes VARCHAR to TEXT, and removes catalog/schema names.
    """
    sql_text = re.sub(r'\[([^\]]+)\]', r'\1', sql_text)  # [col] -> col
    sql_text = re.sub(r'\bVARCHAR\(\d+\)', 'TEXT', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bDECIMAL\(\d+,\d+\)', 'REAL', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bBIGINT\b', 'INTEGER', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bINT\b', 'INTEGER', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bSTRING\b', 'TEXT', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bBINARY\b', 'BLOB', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bTIMESTAMP\b', 'TEXT', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bDATE\b', 'TEXT', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'CREATE TABLE IF NOT EXISTS\s+[^\s]+\.', 'CREATE TABLE IF NOT EXISTS ', sql_text, flags=re.IGNORECASE)
    return sql_text


def get_sql_files(folder_path: str):
    """Recursively collect .sql files from the schema output folder."""
    sql_files = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".sql"):
                sql_files.append(os.path.join(root, file))
    return sql_files


@pytest.mark.parametrize("sql_file", get_sql_files("generated_schemas"))
def test_sql_file_execution(sql_file):
    """Test if each SQL file can be executed in SQLite (after sanitization)."""
    with open(sql_file, "r") as f:
        sql = f.read()

    sanitized_sql = sanitize_sql(sql)

    try:
        conn = sqlite3.connect(":memory:")
        cursor = conn.cursor()
        cursor.executescript(sanitized_sql)
        print(f"‚úÖ Validated: {sql_file}")
    except sqlite3.Error as e:
        pytest.fail(f"‚ùå SQL file failed: {sql_file}\nError: {e}")
    finally:
        conn.close()

##duplicate columns
import os
import sqlite3
import re
import pytest

def sanitize_sql(sql_text: str) -> str:
    """
    Convert Databricks SQL to SQLite-compatible SQL.
    - Remove brackets: [col] -> col
    - Replace types: VARCHAR(255) -> TEXT, DECIMAL -> REAL, etc.
    - Remove schema/catalog from table names.
    """
    sql_text = re.sub(r'\[([^\]]+)\]', r'\1', sql_text)  # Remove square brackets
    sql_text = re.sub(r'\bVARCHAR\(\d+\)', 'TEXT', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bDECIMAL\(\d+,\s*\d+\)', 'REAL', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bBIGINT\b', 'INTEGER', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bINT\b', 'INTEGER', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bSTRING\b', 'TEXT', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bBINARY\b', 'BLOB', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bTIMESTAMP\b', 'TEXT', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'\bDATE\b', 'TEXT', sql_text, flags=re.IGNORECASE)
    sql_text = re.sub(r'CREATE TABLE IF NOT EXISTS\s+[^\s]+\.', 'CREATE TABLE IF NOT EXISTS ', sql_text, flags=re.IGNORECASE)
    return sql_text


def get_sql_files(folder_path: str):
    """Collect all .sql files recursively in the provided folder."""
    sql_files = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".sql"):
                sql_files.append(os.path.join(root, file))
    return sql_files


def has_duplicate_columns(sql: str) -> bool:
    """Check if the SQL has duplicate column names."""
    match = re.search(r'CREATE TABLE.*?\((.*?)\);', sql, re.DOTALL | re.IGNORECASE)
    if not match:
        return False

    columns_block = match.group(1)
    # Extract only column names (first word before space/comma)
    column_names = []
    for line in columns_block.split(","):
        parts = line.strip().split()
        if parts:
            col = parts[0].strip().lower()
            column_names.append(col)

    return len(column_names) != len(set(column_names))


@pytest.mark.parametrize("sql_file", get_sql_files("generated_schemas"))
def test_sql_file_execution(sql_file):
    """Sanitize, check for duplicates, and execute SQL against SQLite."""
    with open(sql_file, "r") as f:
        original_sql = f.read()

    sanitized_sql = sanitize_sql(original_sql)

    if has_duplicate_columns(sanitized_sql):
        pytest.fail(f"‚ùå Duplicate column found in: {sql_file}")

    try:
        conn = sqlite3.connect(":memory:")
        cursor = conn.cursor()
        cursor.executescript(sanitized_sql)
        print(f"‚úÖ Validated: {sql_file}")
    except sqlite3.Error as e:
        pytest.fail(f"‚ùå SQL file failed: {sql_file}\nError: {e}")
    finally:
        conn.close()

##sqlglot
import os
import pytest
import sqlglot
from sqlglot.errors import ParseError


def get_sql_files(folder_path: str):
    sql_files = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".sql"):
                sql_files.append(os.path.join(root, file))
    return sql_files


@pytest.mark.parametrize("sql_file", get_sql_files("generated_schemas"))
def test_sql_syntax_like_databricks(sql_file):
    """Parse SQL using sqlglot to simulate Databricks-style syntax."""
    with open(sql_file, "r") as f:
        sql = f.read()

    try:
        # Try parsing the SQL
        sqlglot.parse(sql, read="spark")  # you can also try `read="hive"` or `read="databricks"` if supported
        print(f"‚úÖ SQL Parsed: {sql_file}")
    except ParseError as e:
        pytest.fail(f"‚ùå Databricks-like SQL parse failed: {sql_file}\nError: {e}")
