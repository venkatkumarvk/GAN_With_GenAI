import os
import json
import hashlib
import logging
import traceback
from datetime import datetime
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

def load_config(config_path="config.json"):
    """
    Load configuration from JSON file with validation
    """
    try:
        with open(config_path, "r") as f:
            cfg = json.load(f)
        
        # Validate essential configuration
        validate_config(cfg)
        return cfg
    except FileNotFoundError:
        logging.error(f"Config file not found: {config_path}")
        raise
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON in config file: {config_path}")
        raise

def validate_config(cfg):
    """
    Validate configuration parameters
    """
    # Check Azure credentials
    if not cfg.get('azure_document_intelligence', {}).get('endpoint'):
        raise ValueError("Azure Document Intelligence endpoint is missing")
    
    if not cfg.get('azure_document_intelligence', {}).get('key'):
        raise ValueError("Azure Document Intelligence key is missing")
    
    # Validate categories
    if not cfg.get('categories'):
        raise ValueError("No document categories defined")
    
    # Validate paths
    required_paths = ['reference_dir', 'input_dir', 'output_dir']
    for path_key in required_paths:
        if not cfg.get('paths', {}).get(path_key):
            raise ValueError(f"{path_key} is not defined in configuration")

def compute_reference_hash(ref_dir):
    """
    Compute a comprehensive hash of reference directory contents
    """
    hasher = hashlib.sha256()
    
    for root, _, files in sorted(os.walk(ref_dir)):
        for f in sorted(files):
            path = os.path.join(root, f)
            
            try:
                with open(path, 'rb') as file:
                    hasher.update(file.read())
            except Exception as e:
                logging.warning(f"Could not read file {path}: {e}")
    
    return hasher.hexdigest()

def get_document_intelligence_client(cfg):
    """
    Initialize Azure Document Intelligence client
    """
    try:
        credential = AzureKeyCredential(cfg["azure_document_intelligence"]["key"])
        client = DocumentIntelligenceClient(
            endpoint=cfg["azure_document_intelligence"]["endpoint"],
            credential=credential
        )
        return client
    except Exception as e:
        logging.critical(f"Client initialization error: {e}")
        raise

def prepare_reference_metadata(ref_dir):
    """
    Generate a structured metadata of reference documents
    """
    reference_metadata = {}
    
    for main_category in os.listdir(ref_dir):
        main_path = os.path.join(ref_dir, main_category)
        if not os.path.isdir(main_path):
            continue
        
        reference_metadata[main_category] = {}
        
        for subcategory in os.listdir(main_path):
            subcat_path = os.path.join(main_path, subcategory)
            if not os.path.isdir(subcat_path):
                continue
            
            # Count documents in each subcategory
            doc_count = len([f for f in os.listdir(subcat_path) 
                             if os.path.isfile(os.path.join(subcat_path, f))])
            
            reference_metadata[main_category][subcategory] = {
                'document_count': doc_count,
                'documents': [f for f in os.listdir(subcat_path) 
                              if os.path.isfile(os.path.join(subcat_path, f))]
            }
    
    return reference_metadata

def check_reference_structure(ref_dir, cfg):
    """
    Validate reference folder structure
    """
    # Check if reference directory exists
    if not os.path.exists(ref_dir):
        logging.error(f"Reference directory not found: {ref_dir}")
        raise FileNotFoundError(f"Reference directory {ref_dir} does not exist")
    
    # Check if reference directory is empty
    if not os.listdir(ref_dir):
        logging.warning("Reference directory is empty")
        return False
    
    # Validate reference structure against configuration
    for main_category in cfg['categories']:
        main_path = os.path.join(ref_dir, main_category)
        
        # Check if main category directory exists
        if not os.path.exists(main_path):
            logging.warning(f"Main category directory missing: {main_path}")
            continue
        
        # Check subcategories
        for subcategory in cfg['categories'][main_category]:
            subcat_path = os.path.join(main_path, subcategory)
            
            # Check if subcategory directory exists
            if not os.path.exists(subcat_path):
                logging.warning(f"Subcategory directory missing: {subcat_path}")
                continue
            
            # Check if subcategory has documents
            docs = [f for f in os.listdir(subcat_path) 
                    if os.path.isfile(os.path.join(subcat_path, f))]
            
            if not docs:
                logging.warning(f"No documents in subcategory: {subcat_path}")
    
    return True

def prepare_directories(cfg):
    """
    Prepare necessary directories for processing
    """
    directories = [
        cfg['paths']['input_dir'],
        cfg['paths']['output_dir'],
        os.path.join(cfg['paths']['output_dir'], 'source'),
        os.path.join(cfg['paths']['output_dir'], 'classified'),
        os.path.join(cfg['paths']['output_dir'], 'unclassified')
    ]
    
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)


------
import os
import io
import json
import shutil
import logging
import traceback

from helper import (
    load_config, 
    get_document_intelligence_client,
    prepare_directories,
    check_reference_structure,
    prepare_reference_metadata
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

def classify_document(client, file_path, cfg):
    """
    Classify document using Document Intelligence
    """
    try:
        # Analyze document
        with open(file_path, 'rb') as doc_file:
            poller = client.begin_analyze_document(
                model_id='prebuilt-document',
                analyze_request=doc_file
            )
            result = poller.result()
        
        # Default classification
        main_category = 'unknown'
        subcategory = 'unknown'
        confidence_score = 0.0
        reasoning = "No specific reasoning"
        
        # Process analysis results
        if result.documents and len(result.documents) > 0:
            document = result.documents[0]
            
            # Extract document features
            if hasattr(document, 'fields'):
                for field_name, field in document.fields.items():
                    # Attempt to map to categories based on field information
                    for cat, subcats in cfg['categories'].items():
                        if any(subcat.lower() in str(field_name).lower() for subcat in subcats):
                            main_category = cat
                            subcategory = [
                                subcat for subcat in subcats 
                                if subcat.lower() in str(field_name).lower()
                            ][0]
                            confidence_score = getattr(field, 'confidence', 0.0)
                            reasoning = f"Matched by field: {field_name}"
                            break
        
        return main_category, subcategory, confidence_score, reasoning

    except Exception as e:
        logging.error(f"Document classification error: {e}")
        logging.error(traceback.format_exc())
        return 'unknown', 'unknown', 0.0, f"Classification error: {str(e)}"

def process_documents():
    """
    Document processing and classification
    """
    try:
        # Load configuration
        cfg = load_config()
        
        # Prepare directories
        prepare_directories(cfg)
        
        # Validate reference structure
        ref_dir = cfg['paths']['reference_dir']
        check_reference_structure(ref_dir, cfg)
        
        # Log reference document metadata
        reference_metadata = prepare_reference_metadata(ref_dir)
        logging.info("Reference Document Metadata:")
        logging.info(json.dumps(reference_metadata, indent=2))
        
        # Initialize Document Intelligence client
        client = get_document_intelligence_client(cfg)
        
        # Set up directories
        input_dir = cfg['paths']['input_dir']
        output_dir = cfg['paths']['output_dir']
        source_dir = os.path.join(output_dir, 'source')
        classified_dir = os.path.join(output_dir, 'classified')
        unclassified_dir = os.path.join(output_dir, 'unclassified')
        
        # Processing statistics
        stats = {
            'total_documents': 0,
            'classified_documents': 0,
            'unclassified_documents': 0
        }

        # Process each document
        for fname in os.listdir(input_dir):
            fpath = os.path.join(input_dir, fname)
            
            # Skip directories and hidden files
            if not os.path.isfile(fpath) or fname.startswith('.'):
                continue

            # Increment total documents
            stats['total_documents'] += 1

            try:
                # Copy original document to source directory
                shutil.copy(fpath, os.path.join(source_dir, fname))

                # Classify document
                main_cat, sub_cat, confidence, reasoning = classify_document(
                    client, 
                    fpath, 
                    cfg
                )

                # Determine destination based on classification
                confidence_threshold = cfg['classification'].get('confidence_threshold', 0.5)
                if confidence >= confidence_threshold:
                    # Classified document
                    dest_dir = os.path.join(classified_dir, main_cat, sub_cat)
                    stats['classified_documents'] += 1
                else:
                    # Unclassified document
                    dest_dir = unclassified_dir
                    stats['unclassified_documents'] += 1

                # Create destination directory
                os.makedirs(dest_dir, exist_ok=True)
                
                # Copy document
                dest_path = os.path.join(dest_dir, fname)
                shutil.copy(fpath, dest_path)
                
                # Create metadata
                metadata_path = os.path.join(dest_dir, f"{os.path.splitext(fname)[0]}_metadata.json")
                with open(metadata_path, 'w') as metadata_file:
                    json.dump({
                        "filename": fname,
                        "main_category": main_cat,
                        "subcategory": sub_cat,
                        "confidence_score": confidence,
                        "reasoning": reasoning
                    }, metadata_file, indent=2)

            except Exception as e:
                logging.error(f"Processing error for {fname}: {e}")
                logging.error(traceback.format_exc())

        # Log processing summary
        logging.info("\n--- Processing Summary ---")
        logging.info(json.dumps(stats, indent=2))

        print("\n✅ Document Processing Complete")

    except Exception as overall_error:
        logging.critical(f"Critical processing error: {overall_error}")
        logging.critical(traceback.format_exc())
        print("❌ Document Processing Failed. Check logs for details.")

if __name__ == "__main__":
    process_documents()
```

### User Workflow

1. **Configuration (`config.json`)**
   - Update Azure credentials
   - Define document categories
   - Set input/output paths

2. **Reference Folder Structure**
```
reference/
├── cms1500/
│   ├── cadwell/
│   │   ├── medical_form1.pdf
│   │   └── medical_form2.pdf
│   └── rhymlink/
│       ├── insurance_doc1.pdf
│       └── insurance_doc2.pdf
├── invoice/
│   ├── tesla/
│   │   ├── invoice1.pdf
│   │   └── invoice2.pdf
│   └── amazon/
│       ├── order_invoice1.pdf
│       └── order_invoice2.pdf
└── scheduling/
    ├── email/
    │   ├── appointment1.pdf
    │   └── appointment2.pdf
    └── iomrequest/
        ├── request1.pdf
        └── request2.pdf

