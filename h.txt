import os
import pyodbc
import logging
from datetime import datetime

class SQLLogger:
    def __init__(self, config):
        """
        Initialize SQL Logger
        
        :param config: Configuration dictionary
        """
        # Setup logging
        self.logger = logging.getLogger(__name__)
        
        # Configuration
        self.config = config
        self.sql_config = config.get('sql_server', {})
        self.logging_db = config.get('logging_db', {})
        
        # Check if SQL logging is enabled
        self.enabled = self.logging_db.get('enabled', False)
        self.table_name = self.logging_db.get('table_name', 'dbo.XLogDocumentClassification')
        
        # Connection string
        self.connection_string = self.sql_config.get('connection_string')
        
        # Validate configuration
        if not self.enabled:
            self.logger.warning("SQL Logging is disabled")
        
        if not self.connection_string:
            self.logger.error("No SQL connection string provided")

    def _get_connection(self):
        """
        Establish database connection
        
        :return: Database connection
        """
        try:
            # Establish connection
            conn = pyodbc.connect(self.connection_string)
            return conn
        except Exception as e:
            self.logger.error(f"SQL Connection Error: {e}")
            raise

    def log_document_processing(self, document_info):
        """
        Log document processing details to SQL database
        
        :param document_info: Dictionary containing document processing details
        :return: Unique DocumentProcessor_Key
        """
        # Check if logging is enabled
        if not self.enabled:
            return None

        try:
            # Establish connection
            conn = self._get_connection()
            cursor = conn.cursor()
            
            # Insert new row
            sql = f"""
            INSERT INTO {self.table_name} (
                Source, 
                APIType, 
                FileName, 
                SourceFilePath, 
                ReferenceFilePath, 
                OutputFilePathClassified,
                OutputFilePathUnClassified,
                OutputFilePathUnProcessed,
                ArchiveFilePath,
                Status, 
                StatusDesc, 
                CREATED_ON, 
                CREATED_BY,
                UPDATED_ON,
                UPDATED_BY
            ) VALUES (
                ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, GETDATE(), ?, GETDATE(), ?
            )
            """
            
            # Prepare values
            values = [
                document_info.get('source', 'Unknown'),
                document_info.get('api_type', 'Unknown'),
                document_info.get('filename', ''),
                document_info.get('source_file_path', ''),
                document_info.get('reference_file_path', ''),
                document_info.get('output_file_path_classified', ''),
                document_info.get('output_file_path_unclassified', ''),
                document_info.get('output_file_path_unprocessed', ''),
                document_info.get('archive_file_path', ''),
                document_info.get('status', 'BEGIN'),
                document_info.get('status_desc', 'Document Processing Started'),
                document_info.get('created_by', 'SystemUser'),
                document_info.get('updated_by', 'SystemUser')
            ]
            
            # Execute SQL
            cursor.execute(sql, values)
            conn.commit()
            
            # Get the last inserted ID
            cursor.execute("SELECT @@IDENTITY AS DocumentProcessor_Key")
            document_key = cursor.fetchone()[0]
            
            # Log successful insertion
            self.logger.info(f"Logged document processing: {document_key} - {document_info.get('filename')}")
            
            cursor.close()
            conn.close()
            
            return document_key
        
        except Exception as e:
            self.logger.error(f"SQL Logging Error: {e}")
            # Log detailed error information
            self.logger.error(f"Document Info: {document_info}")
            return None

    def update_document_processing(self, document_key, status_update):
        """
        Update existing document processing log
        
        :param document_key: Key of the document log to update
        :param status_update: Dictionary with status update information
        :return: Boolean indicating success of update
        """
        # Check if logging is enabled and document key is valid
        if not self.enabled or not document_key:
            self.logger.warning("SQL logging disabled or invalid document key")
            return False

        try:
            # Establish connection
            conn = self._get_connection()
            cursor = conn.cursor()
            
            # Prepare SQL update statement
            sql = f"""
            UPDATE {self.table_name}
            SET 
                Status = ?,
                StatusDesc = ?,
                OutputFilePathClassified = COALESCE(?, OutputFilePathClassified),
                OutputFilePathUnClassified = COALESCE(?, OutputFilePathUnClassified),
                OutputFilePathUnProcessed = COALESCE(?, OutputFilePathUnProcessed),
                ArchiveFilePath = COALESCE(?, ArchiveFilePath),
                UPDATED_ON = GETDATE(),
                UPDATED_BY = ?
            WHERE DocumentProcessor_Key = ?
            """
            
            # Prepare values
            values = [
                status_update.get('status', 'Processing'),
                status_update.get('status_desc', 'Ongoing Processing'),
                status_update.get('output_file_path_classified'),
                status_update.get('output_file_path_unclassified'),
                status_update.get('output_file_path_unprocessed'),
                status_update.get('archive_file_path'),
                status_update.get('updated_by', 'SystemUser'),
                document_key
            ]
            
            # Execute SQL
            cursor.execute(sql, values)
            conn.commit()
            
            # Log successful update
            self.logger.info(f"Updated document processing log: {document_key}")
            
            cursor.close()
            conn.close()
            
            return True
        
        except Exception as e:
            self.logger.error(f"SQL Update Error: {e}")
            # Log detailed error information
            self.logger.error(f"Document Key: {document_key}")
            self.logger.error(f"Status Update: {status_update}")
            return False

def initialize_sql_logger(config):
    """
    Initialize and return SQL Logger
    
    :param config: Configuration dictionary
    :return: Initialized SQLLogger instance
    """
    try:
        return SQLLogger(config)
    except Exception as e:
        logging.error(f"Failed to initialize SQL Logger: {e}")
        return None
-----
def process_local_documents(config, input_folder=None, confidence_threshold=0.6, processing_mode='general'):
    # Initialize SQL Logger
    sql_logger = initialize_sql_logger(config)
    
    # Process each document
    for fname in os.listdir(input_dir):
        try:
            # Prepare initial document info
            document_info = {
                'source': 'local',
                'api_type': processing_mode,
                'filename': fname,
                'source_file_path': fpath,
                'reference_file_path': reference_dir,
                'status': 'BEGIN',
                'status_desc': 'Document Processing Started'
            }
            
            # Log document and get key
            document_key = sql_logger.log_document_processing(document_info)
            
            # Existing processing logic
            # ... (your existing document processing code)
            
            # Prepare output paths
            classified_path = os.path.join(
                config['paths']['output_dir'], 
                'classified', 
                f"{os.path.splitext(fname)[0]}_classified.pdf"
            )
            
            unclassified_path = os.path.join(
                config['paths']['output_dir'], 
                'unclassified',
                f"{os.path.splitext(fname)[0]}_unclassified.pdf"
            )
            
            unprocessed_path = os.path.join(
                config['paths']['output_dir'], 
                'unprocessed',
                f"{os.path.splitext(fname)[0]}_error.txt"
            )
            
            archive_path = create_archive([fpath])
            
            # Update SQL log with processing results
            sql_logger.update_document_processing(document_key, {
                'status': 'Completed',
                'status_desc': 'Document processed successfully',
                'output_file_path_classified': classified_path,
                'output_file_path_unclassified': unclassified_path,
                'output_file_path_unprocessed': unprocessed_path,
                'archive_file_path': archive_path
            })
        
        except Exception as e:
            # Update status to error
            sql_logger.update_document_processing(document_key, {
                'status': 'Error',
                'status_desc': str(e),
                'output_file_path_unprocessed': unprocessed_path
            })


-----

{
    "paths": {
        "input_dir": "/path/to/input/documents",
        "output_dir": "/path/to/output/documents",
        "reference_dir": "/path/to/reference/documents",
        "log_dir": "/path/to/logs"
    },
    "logging": {
        "level": "INFO",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    },
    "categories": {
        "medical": [
            "insurance_claim",
            "prescription", 
            "medical_report", 
            "patient_record",
            "lab_result"
        ],
        "financial": [
            "invoice", 
            "bank_statement", 
            "tax_document",
            "receipt",
            "financial_report"
        ],
        "legal": [
            "contract", 
            "agreement", 
            "legal_notice",
            "court_document",
            "affidavit"
        ],
        "hr": [
            "resume",
            "offer_letter",
            "employment_contract",
            "performance_review"
        ]
    },
    "azure_openai": {
        "api_key": "@azurekeyvault(Openaiapikeygeneral)",
        "endpoint": "https://your-resource-name.openai.azure.com/",
        "deployment_name": "your-gpt-4o-general-deployment",
        "api_version": "2024-02-15-preview",
        "api_type": "azure",
        "max_tokens": 300,
        "temperature": 0.2,
        "top_p": 0.95
    },
    "azure_openai_batch": {
        "api_key": "@azurekeyvault(Openaiapikeygeneral)",
        "endpoint": "https://your-resource-name.openai.azure.com/",
        "deployment_name": "your-gpt-4o-batch-deployment",
        "api_version": "2024-02-15-preview",
        "api_type": "azure",
        "max_tokens": 300,
        "temperature": 0.2,
        "top_p": 0.95
    },
    "azure_storage": {
        "connection_string": "@azurekeyvault(AzureStorageConnectionString)",
        "input_container": "input-documents",
        "output_container": "processed-documents",
        "archive_container": "document-archives"
    },
    "azure_keyvault": {
        "vault_url": "https://your-keyvault-name.vault.azure.net/"
    },
    "classification": {
        "confidence_threshold": 0.6,
        "batch_size": 10
    },
    "sql_server": {
        "connection_string": "Driver={ODBC Driver 17 for SQL Server};Server=your-server;Database=your-database;UID=your-username;PWD=your-password;"
    },
    "logging_db": {
        "enabled": true,
        "table_name": "dbo.XLogDocumentClassification"
    },
    "token_pricing": {
        "gpt-4o": {
            "input": {
                "price_per_million": 2.50
            },
            "output": {
                "price_per_million": 10.00
            }
        }
    },
    "batch_config": {
        "input_container": "batch-input",
        "output_container": "batch-output",
        "polling_interval": 60
    }
}

-----
def process_azure_documents(config, input_folder, confidence_threshold=0.6, 
                             archive_enabled=True, 
                             clean_input_container=True, 
                             processing_mode='general'):
    # Initialize SQL Logger
    sql_logger = initialize_sql_logger(config)
    
    # Initialize Azure storage manager
    storage_manager = AzureStorageManager(config)
    
    # Initialize document classifier
    classifier = DocumentClassifier(config, processing_mode=processing_mode)
    
    # List blobs in input container/folder
    input_container = config['azure_storage']['input_container']
    blobs = storage_manager.list_blobs(input_container, prefix=input_folder)
    
    # Track processed files
    processed_files = []
    unprocessed_files = []

    # Process each blob
    for blob_name in blobs:
        # Initialize document key to None
        document_key = None
        
        try:
            # Prepare initial document logging info
            document_info = {
                'source': 'azure',
                'api_type': processing_mode,
                'filename': os.path.basename(blob_name),
                'source_file_path': f"azure://{input_container}/{blob_name}",
                'reference_file_path': config['paths']['reference_dir'],
                'status': 'BEGIN',
                'status_desc': 'Azure Blob Processing Started'
            }
            
            # Log document processing start
            document_key = sql_logger.log_document_processing(document_info)
            
            # Update status to processing
            sql_logger.update_document_processing(document_key, {
                'status': 'Processing',
                'status_desc': 'Downloading and processing blob'
            })

            # Download blob to local temporary directory
            local_path = os.path.join(local_input_dir, os.path.basename(blob_name))
            if not storage_manager.download_blob(input_container, blob_name, local_path):
                # Update status for download failure
                sql_logger.update_document_processing(document_key, {
                    'status': 'Error',
                    'status_desc': 'Failed to download blob',
                    'output_file_path_unprocessed': local_path
                })
                unprocessed_files.append(blob_name)
                continue

            # Prepare output paths
            source_dir = os.path.join(config['paths']['output_dir'], 'source')
            classified_dir = os.path.join(config['paths']['output_dir'], 'classified')
            unclassified_dir = os.path.join(config['paths']['output_dir'], 'unclassified')
            unprocessed_dir = os.path.join(config['paths']['output_dir'], 'unprocessed')
            
            # Ensure output directories exist
            for directory in [source_dir, classified_dir, unclassified_dir, unprocessed_dir]:
                os.makedirs(directory, exist_ok=True)

            # Copy source document
            source_path = os.path.join(source_dir, os.path.basename(local_path))
            shutil.copy2(local_path, source_path)

            # Initialize output paths
            classified_path = None
            unclassified_path = None
            unprocessed_path = None
            archive_path = None

            # Process document
            try:
                # Open PDF
                doc = fitz.open(local_path)
                total_pages = len(doc)

                # Track classified and unclassified pages
                classified_pages = []
                unclassified_pages = []

                # Process each page
                for page_num in range(total_pages):
                    try:
                        # Extract page image
                        page_image = extract_page_image(local_path, page_num)
                        
                        # Classify page
                        classification = classifier.classify_document(page_image, config['paths']['reference_dir'])
                        
                        # Check classification confidence
                        if (classification['confidence'] >= confidence_threshold 
                            and classification['main_category'] != 'unknown'):
                            classified_pages.append(page_num)
                        else:
                            unclassified_pages.append(page_num)
                    
                    except Exception as page_error:
                        logging.error(f"Error processing page {page_num} in {blob_name}: {page_error}")
                        unclassified_pages.append(page_num)

                # Close original document
                doc.close()

                # Create classified PDF if any classified pages
                if classified_pages:
                    # Determine category and subcategory from first classified page
                    first_classified_page = classified_pages[0]
                    page_image = extract_page_image(local_path, first_classified_page)
                    classification = classifier.classify_document(page_image, config['paths']['reference_dir'])
                    
                    # Prepare classified output path
                    classified_path = os.path.join(
                        classified_dir, 
                        classification['main_category'],
                        classification['subcategory'],
                        f"{os.path.splitext(os.path.basename(blob_name))[0]}_classified.pdf"
                    )
                    
                    # Ensure category directory exists
                    os.makedirs(os.path.dirname(classified_path), exist_ok=True)
                    
                    # Create PDF with classified pages
                    doc = fitz.open(local_path)
                    classified_doc = fitz.open()
                    for page_num in classified_pages:
                        classified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                    classified_doc.save(classified_path)
                    classified_doc.close()
                    doc.close()

                # Create unclassified PDF if any unclassified pages
                if unclassified_pages:
                    unclassified_path = os.path.join(
                        unclassified_dir,
                        f"{os.path.splitext(os.path.basename(blob_name))[0]}_unclassified.pdf"
                    )
                    
                    # Create PDF with unclassified pages
                    doc = fitz.open(local_path)
                    unclassified_doc = fitz.open()
                    for page_num in unclassified_pages:
                        unclassified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                    unclassified_doc.save(unclassified_path)
                    unclassified_doc.close()
                    doc.close()

            except Exception as processing_error:
                # Create unprocessed path
                unprocessed_path = os.path.join(
                    unprocessed_dir,
                    f"{os.path.splitext(os.path.basename(blob_name))[0]}_error.txt"
                )
                
                # Write error details
                with open(unprocessed_path, 'w') as error_file:
                    error_file.write(str(processing_error))
                
                # Raise the error to be caught by outer exception handler
                raise

            # Create archive if enabled
            if archive_enabled:
                archive_path = create_archive([local_path])

            # Update SQL log with processing results
            sql_logger.update_document_processing(document_key, {
                'status': 'Completed',
                'status_desc': 'Document processed successfully',
                'output_file_path_classified': classified_path,
                'output_file_path_unclassified': unclassified_path,
                'output_file_path_unprocessed': unprocessed_path,
                'archive_file_path': archive_path
            })
            
            # Clean up input container if specified
            if clean_input_container:
                storage_manager.delete_blob(input_container, blob_name)
            
            # Track processed file
            processed_files.append({
                'original_path': local_path,
                'source_path': source_path,
                'classified_path': classified_path,
                'unclassified_path': unclassified_path,
                'archive_path': archive_path
            })

        except Exception as doc_error:
            # Log processing error
            error_desc = str(doc_error)
            
            # Update SQL log with error status
            if document_key:
                sql_logger.update_document_processing(document_key, {
                    'status': 'Error',
                    'status_desc': error_desc,
                    'output_file_path_unprocessed': unprocessed_path
                })
            
            # Log the error
            logging.error(f"Error processing document {blob_name}: {doc_error}")
            
            # Add to unprocessed files
            unprocessed_files.append(blob_name)

    return processed_files, unprocessed_files



----
update

def process_azure_documents(config, input_folder, confidence_threshold=0.6, 
                             archive_enabled=True, 
                             clean_input_container=True, 
                             processing_mode='general'):
    # Initialize Azure Storage Manager
    storage_manager = AzureStorageManager(config)
    
    # Initialize SQL Logger
    sql_logger = initialize_sql_logger(config)
    
    # Initialize document classifier
    classifier = DocumentClassifier(config, processing_mode=processing_mode)
    
    # Get container configurations
    input_container = config['azure_storage']['input_container']
    output_container = config['azure_storage']['output_container']
    archive_container = config['azure_storage'].get('archive_container', 'document-archives')
    reference_container = config['azure_storage'].get('reference_container', 'reference-documents')
    
    # Ensure output container folders exist
    output_folders = ['source', 'classified', 'unclassified', 'unprocessed']
    
    # Create folders in output container if they don't exist
    for folder in output_folders:
        try:
            # Create a placeholder file to ensure folder exists in Azure Blob Storage
            placeholder_blob_name = f"{folder}/.placeholder"
            placeholder_content = b"This is a placeholder to create the folder structure."
            
            # Check if placeholder exists, if not, create it
            try:
                storage_manager.get_blob_client(output_container, placeholder_blob_name)
            except Exception:
                # Blob doesn't exist, so upload placeholder
                storage_manager.upload_blob(
                    output_container, 
                    placeholder_blob_name, 
                    io.BytesIO(placeholder_content)
                )
        except Exception as folder_error:
            logging.error(f"Error creating folder {folder} in output container: {folder_error}")
    
    # List blobs in input container/folder
    blobs = storage_manager.list_blobs(input_container, prefix=input_folder)
    
    # Track processed files
    processed_files = []
    unprocessed_files = []

    # Process each blob
    for blob_name in blobs:
        # Rest of the existing implementation remains the same
        # ... (previous implementation)

        # Example modification for classified path creation
        if classified_pages:
            # Ensure category and subcategory folders exist
            main_category = classification['main_category']
            subcategory = classification['subcategory']
            
            # Create category folder placeholders
            try:
                category_placeholder = f"classified/{main_category}/.placeholder"
                subcategory_placeholder = f"classified/{main_category}/{subcategory}/.placeholder"
                
                # Create category folder placeholder
                storage_manager.upload_blob(
                    output_container, 
                    category_placeholder, 
                    io.BytesIO(b"Category folder placeholder")
                )
                
                # Create subcategory folder placeholder
                storage_manager.upload_blob(
                    output_container, 
                    subcategory_placeholder, 
                    io.BytesIO(b"Subcategory folder placeholder")
                )
            except Exception as folder_error:
                logging.error(f"Error creating category folders: {folder_error}")
            
            # Prepare classified output path
            classified_filename = f"{local_filename}_classified.pdf"
            classified_path = f"classified/{main_category}/{subcategory}/{classified_filename}"

    # Rest of the implementation remains the same
