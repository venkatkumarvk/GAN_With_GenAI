helper.py

import os
import json
import pandas as pd
import re
from typing import List, Dict, Tuple, Optional
from collections import defaultdict

def load_config_from_file(config_file: str) -> Dict:
    try:
        with open(config_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading config file: {e}")
        return {}

class DatabricksSchemaGenerator:
    def __init__(self,
                 excel_file_path: str,
                 output_base_folder: str = "generated_schemas",
                 sheet_name: Optional[str] = None,
                 categories: Optional[Dict[str, Dict[str, str]]] = None,
                 generate_drop_statements: bool = False):
        self.excel_file_path = excel_file_path
        self.output_base_folder = output_base_folder
        self.sheet_name = sheet_name
        self.categories_config = categories or {}
        self.generate_drop_statements = generate_drop_statements
        self.tables_created = 0

    def map_datatype(self, datatype: str) -> str:
        if pd.isna(datatype) or not datatype or str(datatype).strip() == '':
            return 'VARCHAR(255)'
        dtype = str(datatype).upper().strip()

        if m := re.match(r'VARCHAR2\((\d+)\s*BYTE\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'VARCHAR2\((\d+)\)', dtype):
            return f'VARCHAR({m.group(1)})'
        if m := re.match(r'NUMBER\((\d+),\s*(\d+)\)', dtype):
            return f'DECIMAL({m.group(1)},{m.group(2)})'
        if m := re.match(r'NUMBER\((\d+)\)', dtype):
            return 'INT' if int(m.group(1)) <= 10 else 'BIGINT'
        if 'TIMESTAMP' in dtype:
            return 'TIMESTAMP'
        if 'DATE' in dtype:
            return 'DATE'
        if 'CHAR' in dtype:
            return 'VARCHAR(1)'
        if 'CLOB' in dtype:
            return 'STRING'
        if 'BLOB' in dtype:
            return 'BINARY'
        return 'VARCHAR(255)'

    def load_excel_data(self) -> pd.DataFrame:
        try:
            df = pd.read_excel(self.excel_file_path, sheet_name=self.sheet_name)
            
            # Print column names for debugging
            print("Available columns in Excel:")
            for col in df.columns:
                print(f"  - '{col}'")
            
            # Clean column names (remove extra spaces)
            df.columns = df.columns.str.strip()
            
            # Forward-fill missing schema/table names for each category
            for cat, cfg in self.categories_config.items():
                schema_col = cfg.get("schema_col")
                table_col = cfg.get("table_col")
                
                if schema_col and schema_col in df.columns:
                    df[schema_col] = df[schema_col].ffill()
                if table_col and table_col in df.columns:
                    df[table_col] = df[table_col].ffill()

            # Drop rows where any schema column is 'NA' or empty
            for cfg in self.categories_config.values():
                schema_col = cfg.get("schema_col")
                if schema_col and schema_col in df.columns:
                    df = df[
                        (df[schema_col].notna()) & 
                        (df[schema_col].astype(str).str.upper() != "NA") &
                        (df[schema_col].astype(str).str.strip() != "")
                    ]

            return df

        except Exception as e:
            print(f"Error loading Excel file: {e}")
            return pd.DataFrame()

    def extract_tables(self, df: pd.DataFrame) -> Dict[str, Dict[Tuple[str, str], List[Tuple[str, str]]]]:
        result = {}
        
        for cat, cfg in self.categories_config.items():
            schema_col = cfg.get('schema_col')
            table_col = cfg.get('table_col')
            column_col = cfg.get('column_col')
            datatype_col = cfg.get('datatype_col', '')

            print(f"\nProcessing category: {cat}")
            print(f"  Schema column: {schema_col}")
            print(f"  Table column: {table_col}")
            print(f"  Column column: {column_col}")
            print(f"  Datatype column: {datatype_col}")

            # Check if required columns exist
            missing_cols = []
            for col_name, col_val in [('schema_col', schema_col), ('table_col', table_col), ('column_col', column_col)]:
                if not col_val or col_val not in df.columns:
                    missing_cols.append(f"{col_name}: {col_val}")
            
            if missing_cols:
                print(f"  WARNING: Missing columns for {cat}: {missing_cols}")
                result[cat] = {}
                continue

            tables = defaultdict(list)
            seen = defaultdict(set)

            # Filter rows that have valid data for this category
            category_filter = (
                df[schema_col].notna() & 
                df[table_col].notna() & 
                df[column_col].notna() &
                (df[schema_col].astype(str).str.strip() != '') &
                (df[table_col].astype(str).str.strip() != '') &
                (df[column_col].astype(str).str.strip() != '')
            )
            
            filtered_df = df[category_filter]
            print(f"  Found {len(filtered_df)} valid rows for {cat}")

            for _, row in filtered_df.iterrows():
                schema = str(row[schema_col]).strip()
                table = str(row[table_col]).strip()
                column = str(row[column_col]).strip()
                
                # Handle datatype column (might not exist for all categories)
                if datatype_col and datatype_col in df.columns:
                    dtype_val = row.get(datatype_col, '')
                else:
                    dtype_val = ''

                # Skip if any required field is empty
                if not schema or not table or not column:
                    continue

                # Check for duplicates (case-insensitive)
                if column.lower() not in seen[(schema, table)]:
                    dtype = self.map_datatype(dtype_val) if dtype_val else 'VARCHAR(255)'
                    tables[(schema, table)].append((column, dtype))
                    seen[(schema, table)].add(column.lower())

            print(f"  Extracted {len(tables)} tables for {cat}")
            for (schema, table), cols in tables.items():
                print(f"    {schema}.{table}: {len(cols)} columns")

            result[cat] = tables

        return result

    def generate_drop_sql(self, schema: str, table: str, category: str) -> str:
        """Generate DROP TABLE statement"""
        sql = f"-- {category} - Drop {table} Table\n"
        sql += f"DROP TABLE IF EXISTS external_catalog.{schema}.{table};\n"
        return sql

    def generate_schema_sql(self, schema: str, table: str, columns: List[Tuple[str, str]], category: str) -> str:
        """Generate CREATE TABLE statement"""
        sql = f"-- {category} - {table} Table Schema\n"
        sql += f"CREATE TABLE IF NOT EXISTS external_catalog.{schema}.{table} (\n"
        sql += ",\n".join([f"    [{col}] {dtype}" for col, dtype in columns])
        sql += "\n);"
        return sql

    def generate_combined_sql(self, schema: str, table: str, columns: List[Tuple[str, str]], category: str) -> str:
        """Generate both DROP and CREATE statements"""
        drop_sql = self.generate_drop_sql(schema, table, category)
        create_sql = self.generate_schema_sql(schema, table, columns, category)
        return f"{drop_sql}\n{create_sql}"

    def create_folder_structure(self):
        folders = list(self.categories_config.keys()) + ["consolidated"]
        
        # Add subfolders for create, drop, and combined if needed
        if self.generate_drop_statements:
            extended_folders = []
            for folder in folders:
                extended_folders.extend([
                    os.path.join(folder, "create"),
                    os.path.join(folder, "drop"),
                    os.path.join(folder, "combined")
                ])
            folders.extend(extended_folders)
        
        os.makedirs(self.output_base_folder, exist_ok=True)
        for folder in folders:
            os.makedirs(os.path.join(self.output_base_folder, folder), exist_ok=True)

    def generate_category_consolidated_schema(self, category: str, tables: Dict[Tuple[str, str], List[Tuple[str, str]]], operation: str = "create") -> str:
        """Generate consolidated schema for a category with specified operation"""
        operation_upper = operation.upper()
        sql = f"-- {category} CATEGORY - CONSOLIDATED {operation_upper} SCHEMA\n\n"
        count = 0
        
        for (schema, table), cols in tables.items():
            if cols:
                if operation == "drop":
                    sql += self.generate_drop_sql(schema, table, category) + "\n"
                elif operation == "create":
                    sql += self.generate_schema_sql(schema, table, cols, category) + "\n\n"
                elif operation == "combined":
                    sql += self.generate_combined_sql(schema, table, cols, category) + "\n\n"
                count += 1
        
        sql += f"-- Total tables in {category} consolidated {operation} schema: {count}\n"
        return sql, count

    def generate_master_consolidated_schema(self, all_tables: Dict[str, Dict[Tuple[str, str], List[Tuple[str, str]]]], operation: str = "create") -> str:
        """Generate master consolidated schema with specified operation"""
        operation_upper = operation.upper()
        sql = f"-- MASTER CONSOLIDATED {operation_upper} SCHEMA\n\n"
        total = 0
        
        for cat, tables in all_tables.items():
            if tables:  # Only add category if it has tables
                sql += f"-- {cat} CATEGORY\n\n"
                for (schema, table), cols in tables.items():
                    if cols:
                        if operation == "drop":
                            sql += self.generate_drop_sql(schema, table, cat) + "\n"
                        elif operation == "create":
                            sql += self.generate_schema_sql(schema, table, cols, cat) + "\n\n"
                        elif operation == "combined":
                            sql += self.generate_combined_sql(schema, table, cols, cat) + "\n\n"
                        total += 1
        
        sql += f"-- Total tables in master consolidated {operation} schema: {total}\n"
        return sql

    def run(self):
        df = self.load_excel_data()
        if df.empty:
            print("No data loaded.")
            return

        print(f"Loaded {len(df)} rows from Excel file")
        
        self.create_folder_structure()
        all_tables = self.extract_tables(df)
        total_tables = 0

        for cat, tables in all_tables.items():
            count = 0
            
            if self.generate_drop_statements:
                # Create subfolders for different operations
                create_folder = os.path.join(self.output_base_folder, cat, "create")
                drop_folder = os.path.join(self.output_base_folder, cat, "drop")
                combined_folder = os.path.join(self.output_base_folder, cat, "combined")
            else:
                # Use the original folder structure
                create_folder = os.path.join(self.output_base_folder, cat)
            
            for (schema, table), cols in tables.items():
                if cols:
                    filename = f"{schema}_{table}.sql".lower()
                    
                    if self.generate_drop_statements:
                        # Generate CREATE files
                        create_sql = self.generate_schema_sql(schema, table, cols, cat)
                        with open(os.path.join(create_folder, filename), "w") as f:
                            f.write(create_sql)
                        
                        # Generate DROP files
                        drop_sql = self.generate_drop_sql(schema, table, cat)
                        with open(os.path.join(drop_folder, filename), "w") as f:
                            f.write(drop_sql)
                        
                        # Generate COMBINED files
                        combined_sql = self.generate_combined_sql(schema, table, cols, cat)
                        with open(os.path.join(combined_folder, filename), "w") as f:
                            f.write(combined_sql)
                    else:
                        # Generate only CREATE files (original behavior)
                        create_sql = self.generate_schema_sql(schema, table, cols, cat)
                        with open(os.path.join(create_folder, filename), "w") as f:
                            f.write(create_sql)
                    
                    count += 1

            # Generate consolidated schemas for this category
            if tables:
                if self.generate_drop_statements:
                    # Generate consolidated CREATE schema
                    consolidated_create_sql, cat_count = self.generate_category_consolidated_schema(cat, tables, "create")
                    with open(os.path.join(create_folder, f"{cat.lower()}_consolidated_create_{cat_count}.sql"), "w") as f:
                        f.write(consolidated_create_sql)
                    
                    # Generate consolidated DROP schema
                    consolidated_drop_sql, _ = self.generate_category_consolidated_schema(cat, tables, "drop")
                    with open(os.path.join(drop_folder, f"{cat.lower()}_consolidated_drop_{cat_count}.sql"), "w") as f:
                        f.write(consolidated_drop_sql)
                    
                    # Generate consolidated COMBINED schema
                    consolidated_combined_sql, _ = self.generate_category_consolidated_schema(cat, tables, "combined")
                    with open(os.path.join(combined_folder, f"{cat.lower()}_consolidated_combined_{cat_count}.sql"), "w") as f:
                        f.write(consolidated_combined_sql)
                    
                    print(f"Created {count} individual table files (CREATE/DROP/COMBINED) and 3 consolidated files for {cat}")
                else:
                    # Generate consolidated CREATE schema (original behavior)
                    consolidated_sql, cat_count = self.generate_category_consolidated_schema(cat, tables, "create")
                    with open(os.path.join(create_folder, f"{cat.lower()}_consolidated_{cat_count}.sql"), "w") as f:
                        f.write(consolidated_sql)
                    
                    print(f"Created {count} individual table files and 1 consolidated file for {cat}")
            else:
                print(f"No tables found for category: {cat}")

            total_tables += count

        # Generate master consolidated schemas
        consolidated_folder = os.path.join(self.output_base_folder, "consolidated")
        
        if self.generate_drop_statements:
            # Generate master CREATE schema
            master_create_sql = self.generate_master_consolidated_schema(all_tables, "create")
            with open(os.path.join(consolidated_folder, "all_tables_master_consolidated_create.sql"), "w") as f:
                f.write(master_create_sql)
            
            # Generate master DROP schema
            master_drop_sql = self.generate_master_consolidated_schema(all_tables, "drop")
            with open(os.path.join(consolidated_folder, "all_tables_master_consolidated_drop.sql"), "w") as f:
                f.write(master_drop_sql)
            
            # Generate master COMBINED schema
            master_combined_sql = self.generate_master_consolidated_schema(all_tables, "combined")
            with open(os.path.join(consolidated_folder, "all_tables_master_consolidated_combined.sql"), "w") as f:
                f.write(master_combined_sql)
            
            print(f"\nGenerated master consolidated files: CREATE, DROP, and COMBINED")
        else:
            # Generate master CREATE schema (original behavior)
            master_sql = self.generate_master_consolidated_schema(all_tables, "create")
            with open(os.path.join(consolidated_folder, "all_tables_master_consolidated.sql"), "w") as f:
                f.write(master_sql)

        print(f"\nTotal tables created: {total_tables}")
        print(f"Files generated in: {self.output_base_folder}")

# main.py
from helper import DatabricksSchemaGenerator, load_config_from_file

def main():
    config = load_config_from_file("schema_config.json")
    if not config:
        print("Invalid or missing config.")
        return

    generator = DatabricksSchemaGenerator(
        excel_file_path=config.get("excel_file_path"),
        output_base_folder=config.get("output_folder", "generated_schemas"),
        sheet_name=config.get("sheet_name"),
        categories=config.get("categories"),
        generate_drop_statements=config.get("generate_drop_statements", False)
    )
    generator.run()

if __name__ == "__main__":
    main()

# Updated schema_config.json
{
  "excel_file_path": "your_excel_file.xlsx",
  "sheet_name": "Sheet1",
  "output_folder": "generated_schemas",
  "generate_drop_statements": true,
  "categories": {
    "RDMOF": {
      "schema_col": "RDMOF - Schema",
      "table_col": "RDMOF - Physical Table Name",
      "column_col": "RDMOF - Physical Column Name",
      "datatype_col": "RDMOF - Data Type"
    },
    "EDL": {
      "schema_col": "EDL- Schema",
      "table_col": "EDL - Physical Table Name",
      "column_col": "EDL - Physical Column Name"
    },
    "Original_SSR": {
      "schema_col": "Original SSR - Schema",
      "table_col": "Original SSR - Physical Table Name",
      "column_col": "Original SSR - Physical Column Name"
    }
  }
}
