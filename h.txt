{
  "azure_document_intelligence": {
    "endpoint": "YOUR_DOCUMENT_INTELLIGENCE_ENDPOINT",
    "key": "YOUR_DOCUMENT_INTELLIGENCE_KEY",
    "model_version": "2024-02-15-preview"
  },
  "categories": {
    "cms1500": ["cadwell", "rhymlink"],
    "invoice": ["tesla", "amazon"],
    "scheduling": ["email", "iomrequest"]
  },
  "paths": {
    "reference_dir": "reference",
    "input_dir": "input_docs",
    "output_dir": "output",
    "model_cache_dir": "model_cache"
  },
  "model_training": {
    "min_documents_per_category": 5,
    "max_training_documents": 100,
    "confidence_threshold": 0.7
  }
}

----
help.py
import os
import json
import hashlib
import logging
import traceback
from datetime import datetime
from typing import Dict, Any, List

from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.core.polling import LROPoller

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

class DocumentIntelligenceModelTrainer:
    def __init__(self, config_path: str = 'config.json'):
        """
        Initialize Document Intelligence Model Trainer
        """
        # Load configuration
        with open(config_path, 'r') as f:
            self.cfg = json.load(f)
        
        # Setup logging
        self.logger = logging.getLogger(__name__)
        
        # Initialize Azure client
        self.client = self._initialize_client()
        
        # Ensure model cache directory
        os.makedirs(self.cfg['paths']['model_cache_dir'], exist_ok=True)

    def _initialize_client(self) -> DocumentIntelligenceClient:
        """
        Initialize Azure Document Intelligence Client
        """
        try:
            credential = AzureKeyCredential(self.cfg["azure_document_intelligence"]["key"])
            return DocumentIntelligenceClient(
                endpoint=self.cfg["azure_document_intelligence"]["endpoint"],
                credential=credential
            )
        except Exception as e:
            self.logger.critical(f"Client initialization error: {e}")
            raise

    def _compute_reference_hash(self, ref_dir: str) -> str:
        """
        Compute hash of reference documents
        """
        hasher = hashlib.sha256()
        
        for root, _, files in sorted(os.walk(ref_dir)):
            for file in sorted(files):
                path = os.path.join(root, file)
                with open(path, 'rb') as f:
                    hasher.update(f.read())
        
        return hasher.hexdigest()

    def _prepare_training_data(self) -> Dict[str, List[str]]:
        """
        Collect and validate training documents
        """
        ref_dir = self.cfg['paths']['reference_dir']
        training_data = {}
        
        for main_category, subcategories in self.cfg['categories'].items():
            training_data[main_category] = {}
            
            for subcategory in subcategories:
                subcat_path = os.path.join(ref_dir, main_category, subcategory)
                
                # Collect documents
                documents = [
                    os.path.join(subcat_path, doc) 
                    for doc in os.listdir(subcat_path) 
                    if os.path.isfile(os.path.join(subcat_path, doc))
                ]
                
                # Validate document count
                if len(documents) < self.cfg['model_training']['min_documents_per_category']:
                    self.logger.warning(f"Insufficient documents in {main_category}/{subcategory}")
                    continue
                
                # Limit documents
                documents = documents[:self.cfg['model_training']['max_training_documents']]
                training_data[main_category][subcategory] = documents
        
        return training_data

    def train_custom_model(self) -> str:
        """
        Train custom Document Intelligence model
        """
        try:
            # Prepare training data
            training_data = self._prepare_training_data()
            
            # Validate training data
            if not training_data:
                self.logger.error("No valid training data found")
                return None
            
            # Generate model ID
            model_id = f"custom_doc_classifier_{hashlib.md5(json.dumps(training_data).encode()).hexdigest()}"
            
            # Prepare training files
            training_files = []
            for main_cat, subcategories in training_data.items():
                for subcat, docs in subcategories.items():
                    training_files.extend(docs)
            
            # Open training files
            training_file_handles = [open(file, 'rb') for file in training_files]
            
            try:
                # Start model training
                poller: LROPoller = self.client.begin_build_model(
                    model_id=model_id,
                    training_files=training_file_handles
                )
                
                # Wait for training to complete
                result = poller.result()
                
                # Log model details
                self.logger.info(f"Custom model trained: {result.model_id}")
                return result.model_id
            
            finally:
                # Close all file handles
                for file in training_file_handles:
                    file.close()
        
        except Exception as e:
            self.logger.error(f"Model training failed: {e}")
            return None

    def check_and_train(self) -> bool:
        """
        Check if model needs retraining
        """
        ref_dir = self.cfg['paths']['reference_dir']
        hash_file = os.path.join(ref_dir, ".reference_hash")
        
        try:
            # Compute current reference hash
            current_hash = self._compute_reference_hash(ref_dir)
            
            # Check existing hash
            if os.path.exists(hash_file):
                with open(hash_file, 'r') as f:
                    last_hash = f.read().strip()
            else:
                last_hash = ''
            
            # Compare hashes
            if current_hash != last_hash:
                # Train new model
                new_model_id = self.train_custom_model()
                
                if new_model_id:
                    # Update hash file
                    with open(hash_file, 'w') as f:
                        f.write(current_hash)
                
                return True
            
            return False
        
        except Exception as e:
            self.logger.error(f"Reference check error: {e}")
            return False

    def get_latest_model_id(self) -> str:
        """
        Retrieve latest trained model ID
        """
        model_cache_dir = self.cfg['paths']['model_cache_dir']
        model_files = [f for f in os.listdir(model_cache_dir) if f.endswith('_metadata.json')]
        
        if not model_files:
            raise ValueError("No trained models found")
        
        # Get most recent model
        latest_model_file = max(model_files, key=lambda f: os.path.getctime(os.path.join(model_cache_dir, f)))
        
        with open(os.path.join(model_cache_dir, latest_model_file), 'r') as f:
            model_metadata = json.load(f)
        
        return model_metadata['model_id']

---
main.py

import os
import shutil
import logging
import fitz
from helper import DocumentIntelligenceModelTrainer

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s: %(message)s',
    filename='document_processing.log',
    filemode='a'
)

def classify_document(client, file_path, model_id, cfg):
    """
    Classify document using Document Intelligence
    """
    try:
        # Open document
        with open(file_path, "rb") as doc_file:
            # Use custom model
            poller = client.begin_analyze_document(
                model_id,
                doc_file
            )
            result = poller.result()
        
        # Default classification
        main_category = 'unknown'
        subcategory = 'unknown'
        confidence_score = 0.0
        
        # Process results
        if result.documents and result.documents[0].fields:
            document = result.documents[0]
            
            # Implement classification logic
            for cat, subcats in cfg['categories'].items():
                for subcat in subcats:
                    # Example classification logic
                    if any(subcat in str(field) for field in document.fields.values()):
                        main_category = cat
                        subcategory = subcat
                        confidence_score = document.confidence
                        break
        
        return main_category, subcategory, confidence_score

    except Exception as e:
        logging.error(f"Document classification error: {e}")
        return 'unknown', 'unknown', 0.0

def process_documents(trainer):
    """
    Document processing workflow
    """
    # Check and train model if needed
    trainer.check_and_train()
    
    # Get latest model ID
    model_id = trainer.get_latest_model_id()
    
    # Initialize client
    client = trainer._initialize_client()
    
    # Configuration
    cfg = trainer.cfg
    input_dir = cfg['paths']['input_dir']
    output_dir = cfg['paths']['output_dir']
    
    # Process each document
    for filename in os.listdir(input_dir):
        filepath = os.path.join(input_dir, filename)
        
        # Open PDF
        doc = fitz.open(filepath)
        
        # Page-level classification results
        page_results = []
        
        for page_num in range(len(doc)):
            # Extract page
            page = doc.load_page(page_num)
            
            # Convert page to image
            pix = page.get_pixmap()
            img_path = f"temp_page_{page_num}.png"
            pix.save(img_path)
            
            # Classify page
            try:
                main_cat, sub_cat, confidence = classify_document(
                    client, 
                    img_path, 
                    model_id,
                    cfg
                )
                
                page_results.append({
                    'page_number': page_num + 1,
                    'main_category': main_cat,
                    'subcategory': sub_cat,
                    'confidence': confidence
                })
            
            except Exception as e:
                logging.error(f"Page classification error: {e}")
            
            # Clean up temporary image
            os.remove(img_path)
        
        # Process classified pages
        _save_classified_document(filepath, page_results, cfg)

def _save_classified_document(filepath, page_results, cfg):
    """
    Save classified document pages
    """
    output_dir = cfg['paths']['output_dir']
    confidence_threshold = cfg['model_training']['confidence_threshold']
    
    # Open original document
    doc = fitz.open(filepath)
    
    # Group classified and unclassified pages
    classified_pages = {}
    unclassified_pages = []
    
    for result in page_results:
        if result['confidence'] >= confidence_threshold:
            key = (result['main_category'], result['subcategory'])
            if key not in classified_pages:
                classified_pages[key] = []
            classified_pages[key].append(result['page_number'] - 1)
        else:
            unclassified_pages.append(result['page_number'] - 1)
    
    # Save classified pages
    for (main_cat, sub_cat), pages in classified_pages.items():
        classified_doc = fitz.open()
        for page_num in pages:
            classified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
        
        # Create output directory
        dest_dir = os.path.join(output_dir, 'classified', main_cat, sub_cat)
        os.makedirs(dest_dir, exist_ok=True)
        
        # Save document
        output_filename = f"{os.path.splitext(os.path.basename(filepath))[0]}_classified_pages.pdf"
        classified_doc.save(os.path.join(dest_dir, output_filename))
        classified_doc.close()
    
    # Save unclassified pages
    if unclassified_pages:
        unclassified_doc = fitz.open()
        for page_num in unclassified_pages:
            unclassified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
        
        # Create output directory
        dest_dir = os.path.join(output_dir, 'unclassified')
        os.makedirs(dest_dir, exist_ok=True)
        
        # Save document
        output_filename = f"{os.path.splitext(os.path.basename(filepath))[0]}_unclassified_pages.pdf"
        unclassified_doc.save(os.path.join(dest_dir, output_filename))
        unclassified_doc.close()
    
    # Close original document
    doc.close()

def main():
    # Initialize model trainer
    trainer = DocumentIntelligenceModelTrainer()
    
    # Process documents
    process_documents(trainer)

if __name__ == "__main__":
    main()
