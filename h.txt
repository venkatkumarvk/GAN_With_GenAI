import os
import re
import shutil
import logging
import traceback
from datetime import datetime
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
import PyPDF2

# Configuration
INPUT_FOLDER = r"path/to/input/pdfs"  # Input PDF folder
OUTPUT_BASE_FOLDER = r"path/to/output"  # Base output folder
CONFIDENCE_THRESHOLD = 0.6  # 60% confidence threshold
ENDPOINT = "your_azure_endpoint"
KEY = "your_azure_key"
MODEL_ID = "your_custom_model_id"

# Setup logging
def setup_logging():
    # Create logs directory if it doesn't exist
    log_dir = os.path.join(OUTPUT_BASE_FOLDER, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    log_filename = os.path.join(log_dir, f'document_classification_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger()

# Logger setup
logger = setup_logging()

# Create DocumentIntelligenceClient
document_intelligence_client = DocumentIntelligenceClient(
    endpoint=ENDPOINT,
    credential=AzureKeyCredential(KEY)
)

# Robust PDF validation
def is_valid_pdf(file_path):
    """
    Validate PDF file
    - Checks file extension
    - Attempts to open and read PDF
    """
    try:
        # Check file extension
        if not file_path.lower().endswith('.pdf'):
            return False
        
        # Attempt to open PDF
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            # Check if PDF has pages
            if len(pdf_reader.pages) == 0:
                logger.warning(f"Empty PDF: {file_path}")
                return False
            
            return True
    except Exception as e:
        logger.error(f"Invalid PDF {file_path}: {e}")
        return False

# Sanitize filename
def sanitize_filename(filename):
    """
    Sanitize filename to remove problematic characters
    """
    # Replace or remove problematic characters
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # Remove leading/trailing spaces and periods
    filename = filename.strip('. ')
    return filename

# Extract specific pages from a PDF
def extract_pages(input_path, page_numbers, original_filename, doc_type):
    """
    Extract specific pages from a PDF using a temporary file
    
    :param input_path: Path to the source PDF
    :param page_numbers: List of page numbers to extract (1-indexed)
    :param original_filename: Original filename to preserve naming
    :param doc_type: Document type for folder organization
    :return: Tuple of (temp_output_path, output_filename, classified_output_folder)
    """
    try:
        # Create necessary directories
        temp_dir = os.path.join(OUTPUT_BASE_FOLDER, 'temp')
        classified_base_dir = os.path.join(OUTPUT_BASE_FOLDER, 'classified')
        os.makedirs(temp_dir, exist_ok=True)
        os.makedirs(classified_base_dir, exist_ok=True)
        
        # Create type-specific folder in classified output
        classified_type_folder = os.path.join(classified_base_dir, doc_type)
        os.makedirs(classified_type_folder, exist_ok=True)
        
        # Generate output filename maintaining original name
        base_name = os.path.splitext(original_filename)[0]
        pages_str = '_'.join(map(str, page_numbers))
        output_filename = f"{base_name}_pages{pages_str}_classified.pdf"
        
        # Temporary output path
        temp_output_path = os.path.join(temp_dir, output_filename)
        
        # Final classified output path
        classified_output_path = os.path.join(classified_type_folder, output_filename)
        
        # Read the input PDF
        with open(input_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            writer = PyPDF2.PdfWriter()

            # Add specified pages (adjusting for 0-indexing)
            for page_num in page_numbers:
                writer.add_page(reader.pages[page_num - 1])

            # Write the temporary PDF
            with open(temp_output_path, 'wb') as output_file:
                writer.write(output_file)

        return temp_output_path, output_filename, classified_output_path
    except Exception as e:
        logger.error(f"Error extracting pages from {input_path}: {e}")
        logger.error(traceback.format_exc())
        return None, None, None

# Process a single PDF file
def process_single_pdf(input_path, output_folders):
    """
    Process a single PDF file with comprehensive error handling
    """
    # Tracking variables for this file
    file_stats = {
        'processed': False,
        'classified_pages': [],
        'errors': []
    }
    
    try:
        # Generate safe filename
        filename = os.path.basename(input_path)
        sanitized_filename = sanitize_filename(filename)
        
        # Detailed logging
        logger.info(f"Processing file: {input_path}")
        
        # Copy to source folder
        source_output_path = os.path.join(output_folders['source'], sanitized_filename)
        shutil.copy2(input_path, source_output_path)
        
        # Open the PDF file
        with open(input_path, "rb") as file:
            # Read PDF to get total page count
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
        
        # Classify each page individually
        classified_page_groups = {}
        
        for page_num in range(1, total_pages + 1):
            try:
                # Reset file pointer
                with open(input_path, "rb") as file:
                    # Begin document classification for this page
                    poller = document_intelligence_client.begin_classify_document(
                        classifier_id=MODEL_ID,
                        body=file
                    )
                
                # Get classification result
                result = poller.result()
                
                # Check classification for this page
                for document in result.documents:
                    # Log confidence for debugging
                    logger.info(f"Page {page_num}: Confidence = {document.confidence}")
                    
                    if document.confidence >= CONFIDENCE_THRESHOLD:
                        doc_type = document.doc_type.lower().replace(' ', '_')
                        
                        # Group pages by their document type
                        if doc_type not in classified_page_groups:
                            classified_page_groups[doc_type] = []
                        
                        classified_page_groups[doc_type].append({
                            'page_num': page_num,
                            'confidence': document.confidence
                        })
                        
                        # Track classified pages
                        file_stats['classified_pages'].append({
                            'page_num': page_num,
                            'type': doc_type,
                            'confidence': document.confidence
                        })
                        
                        break
            except Exception as classification_error:
                error_details = {
                    'page': page_num,
                    'error': 'Classification failed',
                    'details': str(classification_error)
                }
                file_stats['errors'].append(error_details)
                logger.error(f"Classification error for {input_path}, page {page_num}: {classification_error}")
        
        # Process classified pages
        if classified_page_groups:
            # Process each document type
            for doc_type, page_info in classified_page_groups.items():
                # Prepare classified page numbers
                classified_nums = [page['page_num'] for page in page_info]
                
                # Extract classified pages
                temp_pdf, output_filename, classified_output_path = extract_pages(
                    input_path, 
                    classified_nums, 
                    filename,
                    doc_type
                )
                
                if temp_pdf and classified_output_path:
                    try:
                        # Move from temporary to classified folder
                        shutil.move(temp_pdf, classified_output_path)
                        
                        # Logging
                        logger.info(f"Classified File: {filename}")
                        logger.info(f"  - Document Type: {doc_type}")
                        logger.info(f"  - Classified Pages: {classified_nums}")
                        logger.info(f"  - Output: {classified_output_path}")
                        
                        # Mark as processed
                        file_stats['processed'] = True
                    except Exception as move_error:
                        logger.error(f"Error moving classified file: {move_error}")
                        file_stats['errors'].append({
                            'error': 'File move failed',
                            'details': str(move_error)
                        })
        else:
            # Move to unclassified folder
            unclassified_output_path = os.path.join(output_folders['unclassified'], sanitized_filename)
            shutil.copy2(input_path, unclassified_output_path)
            
            # Logging
            logger.info(f"Unclassified File: {filename}")
            logger.info(f"  - Moved to: {unclassified_output_path}")
    
    except Exception as e:
        # Catch any unexpected errors
        error_details = {
            'error': 'Unexpected processing error',
            'details': str(e),
            'traceback': traceback.format_exc()
        }
        file_stats['errors'].append(error_details)
        logger.error(f"Unexpected error processing {input_path}: {e}")
        logger.error(traceback.format_exc())
    
    return file_stats

# Main processing function
def process_pdf_folder(input_folder, output_base_folder):
    # Setup output folders
    output_folders = {
        'source': os.path.join(output_base_folder, 'source'),
        'classified': os.path.join(output_base_folder, 'classified'),
        'unclassified': os.path.join(output_base_folder, 'unclassified')
    }
    
    # Create output folders
    for folder in output_folders.values():
        os.makedirs(folder, exist_ok=True)
    
    # Create temporary folder
    temp_folder = os.path.join(output_base_folder, 'temp')
    os.makedirs(temp_folder, exist_ok=True)
    
    # Logging variables
    stats = {
        'total_files_found': 0,
        'total_files_processed': 0,
        'classified_files': 0,
        'unclassified_files': 0,
        'total_pages_processed': 0,
        'classified_pages': 0,
        'document_types': {},
        'errors': []
    }
    
    # Log start of processing
    logger.info(f"Starting comprehensive document classification")
    logger.info(f"Input Folder: {input_folder}")
    logger.info(f"Output Folder: {output_base_folder}")
    logger.info(f"Confidence Threshold: {CONFIDENCE_THRESHOLD}")
    
    # Ensure input folder exists
    if not os.path.exists(input_folder):
        logger.error(f"Input folder {input_folder} does not exist!")
        return
    
    # Comprehensive file discovery
    all_files = []
    for root, dirs, files in os.walk(input_folder):
        for file in files:
            full_path = os.path.join(root, file)
            if is_valid_pdf(full_path):
                all_files.append(full_path)
    
    logger.info(f"Total PDF files found: {len(all_files)}")
    
    # Process all discovered files
    for input_path in all_files:
        # Increment total files found
        stats['total_files_found'] += 1
        
        # Process each PDF
        file_stats = process_single_pdf(input_path, output_folders)
        
        # Update overall stats
        if file_stats['processed']:
            stats['total_files_processed'] += 1
            stats['classified_files'] += 1
            
            # Track classified pages
            for page in file_stats['classified_pages']:
                doc_type = page['type']
                stats['classified_pages'] += 1
                
                if doc_type not in stats['document_types']:
                    stats['document_types'][doc_type] = 0
                stats['document_types'][doc_type] += 1
        else:
            stats['unclassified_files'] += 1
        
        # Track errors
        if file_stats['errors']:
            stats['errors'].extend(file_stats['errors'])
    
    # Print and log processing summary
    summary_message = f"""
--- Processing Summary ---
Total Files Found: {stats['total_files_found']}
Total Files Processed: {stats['total_files_processed']}
Classified Files: {stats['classified_files']}
Unclassified Files: {stats['unclassified_files']}
Classified Pages: {stats['classified_pages']}
Document Types:
{chr(10).join(f"  - {doc_type}: {count}" for doc_type, count in stats['document_types'].items())}

Errors: {len(stats['errors'])}
"""
    print(summary_message)
    logger.info(summary_message)
    
    # Log detailed errors if any
    if stats['errors']:
        error_log_path = os.path.join(output_base_folder, 'logs', 'processing_errors.log')
        with open(error_log_path, 'w') as error_file:
            for error in stats['errors']:
                error_file.write(f"Error: {error.get('error', 'Unknown')}\n")
                error_file.write(f"Details: {error.get('details', 'No details')}\n")
                if 'traceback' in error:
                    error_file.write(f"Traceback:\n{error['traceback']}\n")
                error_file.write("\n" + "="*50 + "\n\n")
        logger.info(f"Detailed error log saved to {error_log_path}")
    
    # Clean up temporary folder
    try:
        # Remove temporary folder contents
        if os.path.exists(temp_folder):
            for item in os.listdir(temp_folder):
                item_path = os.path.join(temp_folder, item)
                if os.path.isfile(item_path):
                    os.unlink(item_path)
                elif os.path.isdir(item_path):
                    shutil.rmtree(item_path)
            
            # Remove the temp folder itself
            os.rmdir(temp_folder)
        
        logger.info(f"Temporary folder {temp_folder} cleaned up")
    except Exception as cleanup_error:
        logger.error(f"Error cleaning up temporary folder: {cleanup_error}")

# Run the processing
process_pdf_folder(INPUT_FOLDER, OUTPUT_BASE_FOLDER)
