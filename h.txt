config.json
{
  "AzureBlob": {
    "connection_string": "YOUR_BLOB_CONNECTION_STRING",
    "inputcontainer": "testinputcontainer",
    "outputcontainer": "testoutputcontainer"
  },

  "AzureOpenAI": {
    "endpoint": "https://YOUR-GPT.openai.azure.com/",
    "api_key": "YOUR_GPT_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "gpt-5-deployment"
  },

  "AzureEmbedding": {
    "endpoint": "https://YOUR-EMBEDDING.openai.azure.com/",
    "api_key": "YOUR_EMBEDDING_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "embedding-deployment",
    "dimension": 1536
  },

  "AzureAISearch": {
    "endpoint": "https://YOUR-SEARCH.search.windows.net",
    "api_key": "YOUR_ADMIN_KEY"
  },

  "DocumentIntelligence": {
    "endpoint": "https://YOUR-DOCINTEL.cognitiveservices.azure.com/",
    "api_key": "YOUR_DOC_INTEL_KEY"
  },

  "confidence_threshold": 0.90,

  "fields": [
    "FullName",
    "DateOfBirth",
    "DocumentNumber",
    "Address"
  ]
}

helper.py
import json
import os
from datetime import datetime

def load_config():
    with open("config.json") as f:
        return json.load(f)

def now():
    return datetime.utcnow().strftime("%Y-%m-%d_%H-%M-%S")

def supported(filename):
    return filename.lower().endswith((".pdf", ".doc", ".docx", ".jpg", ".jpeg", ".png"))

def sanitize(name):
    return name.lower().replace("_", "-").replace(" ", "-")[:50]

documentintelligence_ocr.py
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

def extract_text(config, file_bytes):
    client = DocumentAnalysisClient(
        config["DocumentIntelligence"]["endpoint"],
        AzureKeyCredential(config["DocumentIntelligence"]["api_key"])
    )

    poller = client.begin_analyze_document("prebuilt-read", file_bytes)
    result = poller.result()

    text = ""
    for page in result.pages:
        for line in page.lines:
            text += line.content + "\n"

    return text.strip()

Embedding.py
from openai import AzureOpenAI

def generate_embedding(config, text):
    client = AzureOpenAI(
        api_key=config["AzureEmbedding"]["api_key"],
        azure_endpoint=config["AzureEmbedding"]["endpoint"],
        api_version=config["AzureEmbedding"]["api_version"]
    )

    response = client.embeddings.create(
        model=config["AzureEmbedding"]["deployment_name"],
        input=text
    )

    return response.data[0].embedding

AIsearch.py
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import *
from azure.core.credentials import AzureKeyCredential

def create_index(config, index_name):
    dimension = config["AzureEmbedding"]["dimension"]

    index_client = SearchIndexClient(
        config["AzureAISearch"]["endpoint"],
        AzureKeyCredential(config["AzureAISearch"]["api_key"])
    )

    fields = [
        SimpleField(name="id", type=SearchFieldDataType.String, key=True),
        SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
        SearchField(
            name="embedding",
            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
            searchable=True,
            vector_search_dimensions=dimension,
            vector_search_configuration="vector-config"
        )
    ]

    vector_search = VectorSearch(
        algorithm_configurations=[
            HnswAlgorithmConfiguration(name="vector-config")
        ]
    )

    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)

    try:
        index_client.create_index(index)
        print(f"Index created: {index_name}")
    except:
        print(f"Index exists: {index_name}")

def upload_doc(config, index_name, doc):
    client = SearchClient(
        config["AzureAISearch"]["endpoint"],
        index_name,
        AzureKeyCredential(config["AzureAISearch"]["api_key"])
    )

    client.upload_documents([doc])

rag.py
import json
from openai import AzureOpenAI

def extract_fields(config, context):
    client = AzureOpenAI(
        api_key=config["AzureOpenAI"]["api_key"],
        azure_endpoint=config["AzureOpenAI"]["endpoint"],
        api_version=config["AzureOpenAI"]["api_version"]
    )

    prompt = f"""
Extract the following fields.
Return STRICT JSON.

Fields:
{config["fields"]}

Text:
{context}
"""

    response = client.chat.completions.create(
        model=config["AzureOpenAI"]["deployment_name"],
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    content = response.choices[0].message.content.strip()

    try:
        return json.loads(content)
    except:
        print("Invalid JSON from LLM:")
        print(content)
        return {}

process_doc.py
from azure.storage.blob import BlobServiceClient
from helper import supported, sanitize, now
from documentintelligence_ocr import extract_text
from Embedding import generate_embedding
from AIsearch import create_index, upload_doc
from rag import extract_fields
import json

def process_provider(config, provider, files, ocr, outputfolder):

    print(f"Processing Provider: {provider}")

    index_name = sanitize(provider)
    create_index(config, index_name)

    blob_service = BlobServiceClient.from_connection_string(
        config["AzureBlob"]["connection_string"]
    )

    output_container = blob_service.get_container_client(
        config["AzureBlob"]["outputcontainer"]
    )

    high = []
    low = []

    for file in files:

        if not supported(file):
            print(f"Unsupported file skipped: {file}")
            continue

        print(f"Reading: {file}")

        blob = blob_service.get_blob_client(
            config["AzureBlob"]["inputcontainer"],
            file
        )

        data = blob.download_blob().readall()

        text = extract_text(config, data) if ocr else data.decode(errors="ignore")

        if not text:
            print("Empty OCR text")
            continue

        embedding = generate_embedding(config, text)

        upload_doc(config, index_name, {
            "id": file,
            "content": text,
            "embedding": embedding
        })

        result = extract_fields(config, text)

        row = {
            "id": provider,
            "extractiondatetime": now()
        }

        is_high = True

        for field in config["fields"]:
            value = result.get(field, "")
            confidence = 0.95 if value else 0.0

            row[field] = value
            row[field+"_confidence"] = confidence
            row[field+"_sourcedocument"] = file

            if confidence < config["confidence_threshold"]:
                is_high = False

        if is_high:
            high.append(row)
        else:
            low.append(row)

    save_results(output_container, outputfolder, provider, high, low)

save_results (add below process_provider)
def save_results(container, outputfolder, provider, high, low):

    for label, dataset in [("Highconfidence", high), ("Lowconfidence", low)]:

        if not dataset:
            continue

        csv_path = f"{outputfolder}/{label}/processedcsvresult/{provider}.csv"
        json_path = f"{outputfolder}/{label}/processedjsonresult/{provider}.json"

        headers = dataset[0].keys()

        csv_data = ",".join(headers) + "\n"

        for row in dataset:
            csv_data += ",".join(str(row[h]) for h in headers) + "\n"

        container.upload_blob(csv_path, csv_data, overwrite=True)
        container.upload_blob(json_path, json.dumps(dataset), overwrite=True)

        print(f"{label} output saved for {provider}")

main.py
import argparse
from helper import load_config
from process_doc import process_provider
from azure.storage.blob import BlobServiceClient

def main():

    parser = argparse.ArgumentParser()
    parser.add_argument("--inputfolder", default=None)
    parser.add_argument("--outputfolder", required=True)
    parser.add_argument("--ocr", default="true")

    args = parser.parse_args()
    ocr_enabled = args.ocr.lower() == "true"

    config = load_config()

    blob_service = BlobServiceClient.from_connection_string(
        config["AzureBlob"]["connection_string"]
    )

    container = blob_service.get_container_client(
        config["AzureBlob"]["inputcontainer"]
    )

    prefix = args.inputfolder.strip("/") + "/" if args.inputfolder else ""

    blobs = container.list_blobs(name_starts_with=prefix)

    providers = {}

    for blob in blobs:
        parts = blob.name.split("/")

        if prefix:
            provider = parts[1] if len(parts) > 1 else None
        else:
            provider = parts[0]

        if provider:
            providers.setdefault(provider, []).append(blob.name)

    if not providers:
        print("No providers found.")
        return

    for provider, files in providers.items():
        process_provider(config, provider, files, ocr_enabled, args.outputfolder)

if __name__ == "__main__":
    main()
