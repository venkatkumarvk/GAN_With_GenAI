#main.py

"""
Complete RAG-Enhanced Document Processing with Consolidation
- ONE row per provider with unique ID
- Proper Azure AI Search storage
- Consistent IDs across all outputs
"""

import os
import uuid
import json
import pandas as pd
from datetime import datetime
import logging
from typing import List, Dict, Any
import hashlib

from helper import (ConfigManager, AzureBlobManager, DocumentIntelligenceManager, 
                   AzureOpenAIManager, AzureAISearchManager)
from rag_extraction import RAGExtractor

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def generate_provider_id(provider_name: str, run_timestamp: str) -> str:
    """
    Generate unique, consistent ID for provider
    Format: providername_YYYYMMDD_HHMMSS
    Example: anand_20240211_143022
    """
    # Clean provider name for ID
    clean_name = provider_name.lower().replace(' ', '_').replace('-', '_')
    clean_name = ''.join(c for c in clean_name if c.isalnum() or c == '_')
    
    # Format: providername_YYYYMMDD_HHMMSS
    provider_id = f"{clean_name}_{run_timestamp}"
    
    return provider_id


class DocumentConsolidator:
    """Consolidates multiple documents into one final row per provider"""
    
    @staticmethod
    def consolidate_documents(
        documents: List[Dict[str, Any]], 
        fields: List[str],
        strategy: str = "voting"
    ) -> Dict[str, Any]:
        """
        Consolidate multiple documents into ONE final row
        
        Args:
            documents: List of processed documents
            fields: List of field names to consolidate
            strategy: Consolidation strategy ("voting" or "highest_confidence")
            
        Returns:
            Single consolidated row with best values
        """
        
        if not documents:
            return {}
        
        consolidated = {
            'total_documents_processed': len(documents),
            'document_names': [doc['document_name'] for doc in documents],
            'consolidation_strategy': strategy,
            'document_ids': [doc['id'] for doc in documents]  # Track all doc IDs
        }
        
        # Consolidate each field
        for field in fields:
            field_values = []
            
            # Collect all values for this field across documents
            for doc in documents:
                field_data = doc.get('extracted_fields', {}).get(field, {})
                if isinstance(field_data, dict) and field_data.get('value'):
                    field_values.append({
                        'value': field_data.get('value', ''),
                        'confidence': field_data.get('confidence', 0.0),
                        'source': doc.get('document_name', 'unknown')
                    })
            
            if not field_values:
                # Field not found in any document
                consolidated[field] = ''
                consolidated[f'{field}_confidence'] = 0.0
                consolidated[f'{field}_source_document'] = 'Not found'
                continue
            
            # Apply consolidation strategy
            if strategy == "voting":
                # Group by value and boost confidence if multiple sources agree
                value_groups = {}
                for fv in field_values:
                    val = fv['value']
                    if val not in value_groups:
                        value_groups[val] = []
                    value_groups[val].append(fv)
                
                # Pick value with best score (count Ã— avg_confidence)
                best_value = None
                best_confidence = 0
                best_sources = []
                
                for value, instances in value_groups.items():
                    avg_conf = sum(i['confidence'] for i in instances) / len(instances)
                    # Boost confidence if multiple sources agree
                    boost = (len(instances) - 1) * 0.03  # +3% per additional source
                    boosted_conf = min(0.99, avg_conf + boost)
                    
                    if boosted_conf > best_confidence:
                        best_confidence = boosted_conf
                        best_value = value
                        best_sources = [i['source'] for i in instances]
                
                consolidated[field] = best_value
                consolidated[f'{field}_confidence'] = round(best_confidence, 3)
                consolidated[f'{field}_source_document'] = '|'.join(best_sources)
                
            else:  # highest_confidence
                # Pick the value with highest confidence
                best = max(field_values, key=lambda x: x['confidence'])
                consolidated[field] = best['value']
                consolidated[f'{field}_confidence'] = round(best['confidence'], 3)
                consolidated[f'{field}_source_document'] = best['source']
        
        # Calculate overall statistics
        all_confidences = [
            consolidated.get(f'{field}_confidence', 0.0) 
            for field in fields
            if consolidated.get(f'{field}_confidence', 0.0) > 0
        ]
        
        consolidated['avg_confidence_overall'] = (
            round(sum(all_confidences) / len(all_confidences), 3) 
            if all_confidences else 0.0
        )
        
        # Count extraction methods
        rag_count = sum(1 for doc in documents if 'RAG' in doc.get('extraction_method', ''))
        consolidated['rag_enhanced_count'] = rag_count
        consolidated['standard_extraction_count'] = len(documents) - rag_count
        
        # Total pages
        consolidated['total_pages'] = sum(doc.get('page_count', 0) for doc in documents)
        
        return consolidated


def main(use_rag: bool = True):
    """
    Main processing pipeline with document consolidation and proper Azure AI Search storage
    """
    
    # Load configuration
    cfg = ConfigManager('config.json')
    blob_cfg = cfg.get("AzureBlob")
    docint_cfg = cfg.get("DocumentIntelligence")
    openai_cfg = cfg.get("AzureOpenAI")
    embedding_cfg = cfg.get("AzureEmbedding")
    search_cfg = cfg.get("AzureAISearch")
    fields = cfg.get("fields")
    confidence_threshold = cfg.config.get("confidence_threshold", 0.90)
    costs_cfg = cfg.get("costs")
    
    # RAG configuration
    rag_config = cfg.config.get("RAG", {})
    use_rag_extraction = rag_config.get("enabled", use_rag)
    rag_top_k = rag_config.get("top_k", 3)
    rag_similarity_threshold = rag_config.get("similarity_threshold", 0.70)
    consolidation_strategy = rag_config.get("consolidation_strategy", "voting")
    
    print(f"\n{'='*70}")
    print("RAG-ENHANCED DOCUMENT PROCESSING WITH CONSOLIDATION")
    print(f"{'='*70}")
    print(f"RAG Extraction: {'ENABLED' if use_rag_extraction else 'DISABLED'}")
    print(f"Consolidation Strategy: {consolidation_strategy}")
    print(f"Confidence Threshold: {confidence_threshold}")
    print(f"Fields: {', '.join(fields)}")
    print(f"{'='*70}\n")

    # Initialize managers
    blob_manager = AzureBlobManager(
        blob_cfg['connection_string'],
        blob_cfg['inputcontainer'],
        blob_cfg['outputcontainer']
    )
    
    doc_intel_manager = DocumentIntelligenceManager(
        docint_cfg['endpoint'],
        docint_cfg['key']
    )
    
    openai_manager = AzureOpenAIManager(
        gpt_endpoint=openai_cfg['endpoint'],
        gpt_api_key=openai_cfg['api_key'],
        gpt_api_version=openai_cfg['api_version'],
        gpt_deployment=openai_cfg['deployment_name'],
        embedding_endpoint=embedding_cfg['endpoint'],
        embedding_api_key=embedding_cfg['api_key'],
        embedding_api_version=embedding_cfg['api_version'],
        embedding_deployment=embedding_cfg['deployment_name'],
        embedding_dimension=embedding_cfg['dimension']
    )
    
    search_manager = AzureAISearchManager(
        search_cfg['endpoint'],
        search_cfg['api_key']
    )
    
    # Initialize RAG extractor
    rag_extractor = RAGExtractor(
        search_endpoint=search_cfg['endpoint'],
        search_api_key=search_cfg['api_key'],
        openai_manager=openai_manager,
        fields=fields,
        top_k=rag_top_k,
        similarity_threshold=rag_similarity_threshold,
        use_rag=use_rag_extraction
    ) if use_rag_extraction else None

    # Global summary
    global_summary = {
        'total_providers': 0,
        'total_documents': 0,
        'high_confidence_providers': 0,
        'low_confidence_providers': 0,
        'total_cost': 0.0,
        'providers': {}
    }

    # Generate timestamp
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Storage for consolidated rows
    high_confidence_rows = []
    low_confidence_rows = []

    providers = blob_manager.get_providers()
    print(f"Found {len(providers)} provider(s): {providers}\n")

    for provider in providers:
        print(f"\n{'='*70}")
        print(f"PROCESSING PROVIDER: {provider.upper()}")
        print(f"{'='*70}")
        
        # Generate unique provider ID
        provider_id = generate_provider_id(provider, run_timestamp)
        print(f"Provider ID: {provider_id}")
        
        # Process all documents for this provider
        provider_results = process_provider(
            provider=provider,
            provider_id=provider_id,
            blob_manager=blob_manager,
            doc_intel_manager=doc_intel_manager,
            openai_manager=openai_manager,
            search_manager=search_manager,
            rag_extractor=rag_extractor,
            fields=fields,
            embedding_cfg=embedding_cfg,
            use_rag=use_rag_extraction
        )
        
        if not provider_results['documents']:
            print(f"  âŠ˜ No documents successfully processed for {provider}")
            continue
        
        # CONSOLIDATE all documents into ONE row
        consolidated_row = DocumentConsolidator.consolidate_documents(
            documents=provider_results['documents'],
            fields=fields,
            strategy=consolidation_strategy
        )
        
        # Add provider info with unique ID
        consolidated_row['provider_id'] = provider_id  # Unique ID
        consolidated_row['provider'] = provider
        consolidated_row['extraction_datetime'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Determine if high or low confidence
        avg_conf = consolidated_row.get('avg_confidence_overall', 0.0)
        
        if avg_conf >= confidence_threshold:
            high_confidence_rows.append(consolidated_row)
            category = "HIGH CONFIDENCE"
            global_summary['high_confidence_providers'] += 1
        else:
            low_confidence_rows.append(consolidated_row)
            category = "LOW CONFIDENCE"
            global_summary['low_confidence_providers'] += 1
        
        # Update summary
        global_summary['total_providers'] += 1
        global_summary['total_documents'] += consolidated_row['total_documents_processed']
        global_summary['providers'][provider] = {
            'provider_id': provider_id,
            'documents': consolidated_row['total_documents_processed'],
            'avg_confidence': avg_conf,
            'category': category
        }
        
        print(f"\nâœ“ Provider '{provider}' consolidated:")
        print(f"  - Provider ID: {provider_id}")
        print(f"  - Documents Processed: {consolidated_row['total_documents_processed']}")
        print(f"  - Average Confidence: {avg_conf:.3f}")
        print(f"  - Category: {category}")
        print(f"  - RAG Enhanced: {consolidated_row['rag_enhanced_count']}")
        print(f"  - Standard: {consolidated_row['standard_extraction_count']}")
        
        # STORE CONSOLIDATED ROW IN AZURE AI SEARCH
        store_consolidated_in_search(
            provider=provider,
            provider_id=provider_id,
            consolidated_row=consolidated_row,
            documents=provider_results['documents'],
            search_manager=search_manager,
            fields=fields
        )
        
        # SAVE PROVIDER-SPECIFIC COST TRACKING
        save_provider_costs(
            provider=provider,
            provider_id=provider_id,
            total_documents=consolidated_row['total_documents_processed'],
            openai_manager=openai_manager,
            costs_cfg=costs_cfg,
            blob_manager=blob_manager
        )
    
    # Save consolidated results
    save_consolidated_results(
        high_confidence_rows=high_confidence_rows,
        low_confidence_rows=low_confidence_rows,
        fields=fields,
        blob_manager=blob_manager,
        run_timestamp=run_timestamp
    )
    
    # Calculate costs
    total_cost_info = openai_manager.calculate_cost(costs_cfg)
    global_summary['total_cost'] = total_cost_info['total_cost']
    global_summary['cost_breakdown'] = total_cost_info
    
    # Save global summary
    save_global_summary(blob_manager, global_summary, run_timestamp)
    
    # Print final summary
    print_final_summary(global_summary, high_confidence_rows, low_confidence_rows)


def process_provider(
    provider, provider_id, blob_manager, doc_intel_manager, openai_manager,
    search_manager, rag_extractor, fields, embedding_cfg, use_rag
):
    """Process all documents for a provider"""
    
    files = blob_manager.get_provider_files(provider)
    print(f"Found {len(files)} files for provider '{provider}'")
    
    # Create vector index with provider name (not provider_id)
    try:
        index_name = search_manager.create_index(provider, embedding_cfg['dimension'])
        print(f"âœ“ Vector index ready: '{index_name}'")
        vector_index_success = True
    except Exception as e:
        print(f"âœ— Vector index failed: {e}")
        logger.error(f"Index creation error: {e}", exc_info=True)
        vector_index_success = False
    
    # Process each document
    processed_documents = []
    
    for idx, file in enumerate(files, 1):
        print(f"\n  [{idx}/{len(files)}] Processing: {file['filename']}")
        
        result = process_single_document(
            file=file,
            provider=provider,
            provider_id=provider_id,
            blob_manager=blob_manager,
            doc_intel_manager=doc_intel_manager,
            openai_manager=openai_manager,
            search_manager=search_manager,
            rag_extractor=rag_extractor,
            fields=fields,
            embedding_cfg=embedding_cfg,
            vector_index_success=vector_index_success,
            use_rag=use_rag
        )
        
        if result:
            processed_documents.append(result)
    
    return {'documents': processed_documents}


def process_single_document(
    file, provider, provider_id, blob_manager, doc_intel_manager, openai_manager,
    search_manager, rag_extractor, fields, embedding_cfg,
    vector_index_success, use_rag
):
    """Process a single document"""
    
    doc_id = str(uuid.uuid4())
    blob_name = file['name']
    filename = file['filename']
    
    # Step 1: OCR
    try:
        base64_data = blob_manager.download_blob_as_base64(blob_name)
        ocr_result = doc_intel_manager.analyze_document(base64_data, file['extension'])
        
        if not ocr_result['success']:
            print(f"    âœ— OCR failed")
            return None
        
        text_content = ocr_result['text']
        print(f"    âœ“ OCR: {ocr_result['page_count']} pages, {len(text_content)} chars")
    except Exception as e:
        print(f"    âœ— OCR error: {e}")
        logger.error(f"OCR error for {filename}: {e}", exc_info=True)
        return None
    
    if not text_content or len(text_content.strip()) < 50:
        print(f"    âŠ˜ Insufficient text")
        return None
    
    # Step 2: Field Extraction
    try:
        if use_rag and rag_extractor:
            extraction_result = rag_extractor.extract_with_rag(
                document_text=text_content,
                provider=provider,
                source_document=filename,
                document_type=None
            )
            
            method = extraction_result.get('extraction_method', 'Unknown')
            similar_count = extraction_result.get('similar_docs_count', 0)
            
            if 'RAG' in method:
                print(f"    âœ“ RAG Extraction: {similar_count} similar docs")
            else:
                print(f"    âœ“ Standard Extraction")
        else:
            extraction_result = openai_manager.extract_fields(text_content, fields, filename)
            extraction_result['extraction_method'] = 'Standard (no RAG)'
            extraction_result['similar_docs_count'] = 0
            print(f"    âœ“ Standard Extraction")
        
        extracted_fields = extraction_result.get('extracted_fields', {})
        
    except Exception as e:
        print(f"    âœ— Extraction error: {e}")
        logger.error(f"Extraction error for {filename}: {e}", exc_info=True)
        return None
    
    # Step 3: Generate Embeddings
    try:
        embedding_vector = openai_manager.generate_embeddings(text_content)
        if embedding_vector and len(embedding_vector) == embedding_cfg['dimension']:
            print(f"    âœ“ Embeddings: {len(embedding_vector)} dims")
        else:
            print(f"    âš  Embeddings: unexpected dimension")
            embedding_vector = [0.0] * embedding_cfg['dimension']
    except Exception as e:
        print(f"    âœ— Embedding error: {e}")
        logger.error(f"Embedding error for {filename}: {e}", exc_info=True)
        embedding_vector = [0.0] * embedding_cfg['dimension']
    
    # Step 4: Upload INDIVIDUAL DOCUMENT to Vector Database
    if vector_index_success and embedding_vector and any(v != 0.0 for v in embedding_vector):
        try:
            search_doc = {
                "id": doc_id,
                "provider_id": provider_id,  # Add provider ID
                "provider": provider,
                "content": text_content[:50000],
                "document_name": filename,
                "file_extension": file['extension'],
                "page_count": ocr_result.get('page_count', 0),
                "extraction_datetime": datetime.utcnow().isoformat(),
                "extracted_fields": json.dumps(extracted_fields),
                "content_vector": embedding_vector
            }
            
            # Upload to search
            search_manager.upload_documents(provider, [search_doc])
            print(f"    âœ“ Uploaded to search index '{provider}'")
            
        except Exception as e:
            print(f"    âœ— Search upload failed: {e}")
            logger.error(f"Search upload error for {filename}: {e}", exc_info=True)
    else:
        if not vector_index_success:
            print(f"    âŠ˜ Skipped search upload (index not available)")
        else:
            print(f"    âŠ˜ Skipped search upload (no valid embeddings)")
    
    return {
        'id': doc_id,
        'provider': provider,
        'provider_id': provider_id,
        'document_name': filename,
        'file_extension': file['extension'],
        'page_count': ocr_result.get('page_count', 0),
        'content': text_content,
        'extracted_fields': extracted_fields,
        'extraction_method': extraction_result.get('extraction_method', 'Unknown'),
        'similar_docs_used': extraction_result.get('similar_docs_count', 0),
        'embeddings': embedding_vector
    }


def store_consolidated_in_search(
    provider, provider_id, consolidated_row, documents,
    search_manager, fields
):
    """
    Store the CONSOLIDATED row in Azure AI Search as a summary document
    This is in addition to individual documents already uploaded
    """
    
    try:
        # Create consolidated content (summary of all documents)
        all_content = "\n\n--- DOCUMENT BREAK ---\n\n".join([
            f"Document: {doc['document_name']}\n{doc['content'][:5000]}"
            for doc in documents
        ])
        
        # Create average embedding from all document embeddings
        if documents and all('embeddings' in doc for doc in documents):
            all_embeddings = [doc['embeddings'] for doc in documents]
            # Average the embeddings
            avg_embedding = [
                sum(emb[i] for emb in all_embeddings) / len(all_embeddings)
                for i in range(len(all_embeddings[0]))
            ]
        else:
            avg_embedding = [0.0] * 3072  # Default
        
        # Create consolidated search document
        consolidated_search_doc = {
            "id": f"{provider_id}-consolidated",  # Use provider_id format
            "provider_id": provider_id,
            "provider": provider,
            "document_type": "consolidated",  # Mark as consolidated
            "content": all_content[:50000],
            "document_name": f"{provider}-consolidated",
            "total_documents": consolidated_row['total_documents_processed'],
            "extraction_datetime": datetime.utcnow().isoformat(),
            "extracted_fields": json.dumps({
                field: {
                    'value': consolidated_row.get(field, ''),
                    'confidence': consolidated_row.get(f'{field}_confidence', 0.0),
                    'source': consolidated_row.get(f'{field}_source_document', '')
                }
                for field in fields
            }),
            "avg_confidence": consolidated_row.get('avg_confidence_overall', 0.0),
            "content_vector": avg_embedding
        }
        
        # Upload consolidated document
        search_manager.upload_documents(provider, [consolidated_search_doc])
        print(f"    âœ“ Stored consolidated row in search index '{provider}'")
        
    except Exception as e:
        print(f"    âœ— Failed to store consolidated row in search: {e}")
        logger.error(f"Consolidated search storage error: {e}", exc_info=True)


def save_provider_costs(
    provider, provider_id, total_documents,
    openai_manager, costs_cfg, blob_manager
):
    """
    Save provider-specific cost tracking
    Filename: providername_datetime_costs.json
    """
    
    try:
        token_usage = openai_manager.get_token_usage()
        cost_info = openai_manager.calculate_cost(costs_cfg)
        
        # Estimate costs per provider (proportional to token usage)
        cost_data = {
            'provider_id': provider_id,
            'provider': provider,
            'total_documents': total_documents,
            'costs': {
                'gpt4o_cost': cost_info['total_cost'],
                'ocr_cost': total_documents * costs_cfg.get('doc_intel_per_page', 0.01) * 2,  # Estimate 2 pages/doc
                'embedding_cost': token_usage.get('total_tokens', 0) * costs_cfg.get('embedding_per_1k', 0.00013) / 1000,
                'total_estimated': cost_info['total_cost'] + (total_documents * 0.02)
            },
            'usage': {
                'total_tokens': token_usage.get('total_tokens', 0),
                'prompt_tokens': token_usage.get('prompt_tokens', 0),
                'completion_tokens': token_usage.get('completion_tokens', 0)
            },
            'cost_breakdown': cost_info
        }
        
        # Save with provider_id as filename: providername_datetime_costs.json
        cost_path = f"CostTracking/{provider_id}_costs.json"
        blob_manager.upload_to_blob(json.dumps(cost_data, indent=2), cost_path, 'application/json')
        print(f"    âœ“ Cost tracking: {cost_path}")
        
    except Exception as e:
        print(f"    âœ— Cost tracking failed: {e}")
        logger.error(f"Cost tracking error for {provider}: {e}", exc_info=True)


def save_consolidated_results(
    high_confidence_rows, low_confidence_rows, fields,
    blob_manager, run_timestamp
):
    """Save consolidated results with provider_id based filenames"""
    
    print(f"\n{'='*70}")
    print("SAVING CONSOLIDATED RESULTS")
    print(f"{'='*70}")
    
    # Define column order - provider_id FIRST
    base_columns = ['provider_id', 'provider', 'extraction_datetime', 'total_documents_processed']
    
    # Field columns (field, field_confidence, field_source_document)
    field_columns = []
    for field in fields:
        field_columns.extend([field, f'{field}_confidence', f'{field}_source_document'])
    
    # Metadata columns
    meta_columns = [
        'avg_confidence_overall', 'rag_enhanced_count', 'standard_extraction_count',
        'total_pages', 'consolidation_strategy'
    ]
    
    all_columns = base_columns + field_columns + meta_columns
    
    # Save High Confidence - ONE FILE PER PROVIDER
    if high_confidence_rows:
        for row in high_confidence_rows:
            provider_id = row['provider_id']
            
            # CSV - filename: providername_datetime.csv
            high_df = pd.DataFrame([row])
            high_df = high_df[[col for col in all_columns if col in high_df.columns]]
            
            csv_path = f"HighConfidence/processedcsvresult/{provider_id}.csv"
            blob_manager.upload_dataframe_as_csv(high_df, csv_path)
            print(f"âœ“ High Confidence CSV: {csv_path}")
            
            # JSON - filename: providername_datetime.json
            json_data = {
                'provider_id': provider_id,
                'timestamp': run_timestamp,
                'confidence_level': 'high',
                'provider_data': row
            }
            json_path = f"HighConfidence/processedjsonresult/{provider_id}.json"
            blob_manager.upload_to_blob(json.dumps(json_data, indent=2), json_path, 'application/json')
            print(f"âœ“ High Confidence JSON: {json_path}")
    
    # Save Low Confidence - ONE FILE PER PROVIDER
    if low_confidence_rows:
        for row in low_confidence_rows:
            provider_id = row['provider_id']
            
            # CSV
            low_df = pd.DataFrame([row])
            low_df = low_df[[col for col in all_columns if col in low_df.columns]]
            
            csv_path = f"LowConfidence/processedcsvresult/{provider_id}.csv"
            blob_manager.upload_dataframe_as_csv(low_df, csv_path)
            print(f"âœ“ Low Confidence CSV: {csv_path}")
            
            # JSON
            json_data = {
                'provider_id': provider_id,
                'timestamp': run_timestamp,
                'confidence_level': 'low',
                'provider_data': row
            }
            json_path = f"LowConfidence/processedjsonresult/{provider_id}.json"
            blob_manager.upload_to_blob(json.dumps(json_data, indent=2), json_path, 'application/json')
            print(f"âœ“ Low Confidence JSON: {json_path}")


def save_global_summary(blob_manager, global_summary, run_timestamp):
    """Save global summary"""
    
    summary_path = f"CostTracking/global_summary_{run_timestamp}.json"
    blob_manager.upload_to_blob(json.dumps(global_summary, indent=2), summary_path, 'application/json')
    print(f"âœ“ Global summary: {summary_path}")


def print_final_summary(global_summary, high_conf_rows, low_conf_rows):
    """Print final summary"""
    
    print(f"\n{'='*70}")
    print("FINAL SUMMARY")
    print(f"{'='*70}")
    print(f"Total Providers:        {global_summary['total_providers']}")
    print(f"Total Documents:        {global_summary['total_documents']}")
    print(f"")
    print(f"High Confidence:        {len(high_conf_rows)} provider(s)")
    print(f"Low Confidence:         {len(low_conf_rows)} provider(s)")
    print(f"")
    print(f"Total Cost:             ${global_summary['total_cost']:.4f}")
    print(f"{'='*70}")
    print("\nâœ“ Processing Complete!")
    print(f"\nðŸ“Š Azure AI Search Status:")
    print(f"  - Each provider has its own index")
    print(f"  - Individual documents + consolidated summary stored")
    print(f"  - Check Azure Portal for document counts")


if __name__ == "__main__":
    import sys
    
    use_rag = True
    if len(sys.argv) > 1:
        use_rag = sys.argv[1].lower() in ['true', '1', 'yes', 'rag']
    
    print(f"Starting pipeline with RAG={'ENABLED' if use_rag else 'DISABLED'}")
    main(use_rag=use_rag)

-----------------------------------
Main Consolidate.py
"""
Complete RAG-Enhanced Document Processing with Consolidation
Multiple documents â†’ ONE consolidated row per provider
"""

import os
import uuid
import json
import pandas as pd
from datetime import datetime
import logging
from typing import List, Dict, Any

from helper import (ConfigManager, AzureBlobManager, DocumentIntelligenceManager, 
                   AzureOpenAIManager, AzureAISearchManager)
from rag_extraction import RAGExtractor

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DocumentConsolidator:
    """Consolidates multiple documents into one final row per provider"""
    
    @staticmethod
    def consolidate_documents(
        documents: List[Dict[str, Any]], 
        fields: List[str],
        strategy: str = "voting"
    ) -> Dict[str, Any]:
        """
        Consolidate multiple documents into ONE final row
        
        Strategies:
        - "highest_confidence": Pick value with highest confidence for each field
        - "voting": If multiple docs agree, boost confidence
        
        Args:
            documents: List of processed documents
            fields: List of field names to consolidate
            strategy: Consolidation strategy
            
        Returns:
            Single consolidated row with best values
        """
        
        if not documents:
            return {}
        
        consolidated = {
            'total_documents_processed': len(documents),
            'document_names': [doc['document_name'] for doc in documents],
            'consolidation_strategy': strategy
        }
        
        # Consolidate each field
        for field in fields:
            field_values = []
            
            # Collect all values for this field across documents
            for doc in documents:
                field_data = doc.get('extracted_fields', {}).get(field, {})
                if isinstance(field_data, dict) and field_data.get('value'):
                    field_values.append({
                        'value': field_data.get('value', ''),
                        'confidence': field_data.get('confidence', 0.0),
                        'source': doc.get('document_name', 'unknown')
                    })
            
            if not field_values:
                # Field not found in any document
                consolidated[field] = ''
                consolidated[f'{field}_confidence'] = 0.0
                consolidated[f'{field}_source_document'] = 'Not found'
                continue
            
            # Apply consolidation strategy
            if strategy == "voting":
                # Group by value and boost confidence if multiple sources agree
                value_groups = {}
                for fv in field_values:
                    val = fv['value']
                    if val not in value_groups:
                        value_groups[val] = []
                    value_groups[val].append(fv)
                
                # Pick value with best score (count Ã— avg_confidence)
                best_value = None
                best_confidence = 0
                best_sources = []
                
                for value, instances in value_groups.items():
                    avg_conf = sum(i['confidence'] for i in instances) / len(instances)
                    # Boost confidence if multiple sources agree
                    boost = (len(instances) - 1) * 0.03  # +3% per additional source
                    boosted_conf = min(0.99, avg_conf + boost)
                    
                    if boosted_conf > best_confidence:
                        best_confidence = boosted_conf
                        best_value = value
                        best_sources = [i['source'] for i in instances]
                
                consolidated[field] = best_value
                consolidated[f'{field}_confidence'] = round(best_confidence, 3)
                consolidated[f'{field}_source_document'] = '|'.join(best_sources)
                
            else:  # highest_confidence
                # Pick the value with highest confidence
                best = max(field_values, key=lambda x: x['confidence'])
                consolidated[field] = best['value']
                consolidated[f'{field}_confidence'] = round(best['confidence'], 3)
                consolidated[f'{field}_source_document'] = best['source']
        
        # Calculate overall statistics
        all_confidences = [
            consolidated.get(f'{field}_confidence', 0.0) 
            for field in fields
            if consolidated.get(f'{field}_confidence', 0.0) > 0
        ]
        
        consolidated['avg_confidence_overall'] = (
            round(sum(all_confidences) / len(all_confidences), 3) 
            if all_confidences else 0.0
        )
        
        # Count extraction methods
        rag_count = sum(1 for doc in documents if 'RAG' in doc.get('extraction_method', ''))
        consolidated['rag_enhanced_count'] = rag_count
        consolidated['standard_extraction_count'] = len(documents) - rag_count
        
        # Total pages
        consolidated['total_pages'] = sum(doc.get('page_count', 0) for doc in documents)
        
        return consolidated


def main(use_rag: bool = True):
    """
    Main processing pipeline with document consolidation
    
    OUTPUT: ONE row per provider with consolidated data from all documents
    """
    
    # Load configuration
    cfg = ConfigManager('config.json')
    blob_cfg = cfg.get("AzureBlob")
    docint_cfg = cfg.get("DocumentIntelligence")
    openai_cfg = cfg.get("AzureOpenAI")
    embedding_cfg = cfg.get("AzureEmbedding")
    search_cfg = cfg.get("AzureAISearch")
    fields = cfg.get("fields")
    confidence_threshold = cfg.config.get("confidence_threshold", 0.90)
    costs_cfg = cfg.get("costs")
    
    # RAG configuration
    rag_config = cfg.config.get("RAG", {})
    use_rag_extraction = rag_config.get("enabled", use_rag)
    rag_top_k = rag_config.get("top_k", 3)
    rag_similarity_threshold = rag_config.get("similarity_threshold", 0.70)
    consolidation_strategy = rag_config.get("consolidation_strategy", "voting")
    
    print(f"\n{'='*70}")
    print("RAG-ENHANCED DOCUMENT PROCESSING WITH CONSOLIDATION")
    print(f"{'='*70}")
    print(f"RAG Extraction: {'ENABLED' if use_rag_extraction else 'DISABLED'}")
    print(f"Consolidation Strategy: {consolidation_strategy}")
    print(f"Confidence Threshold: {confidence_threshold}")
    print(f"Fields: {', '.join(fields)}")
    print(f"{'='*70}\n")

    # Initialize managers
    blob_manager = AzureBlobManager(
        blob_cfg['connection_string'],
        blob_cfg['inputcontainer'],
        blob_cfg['outputcontainer']
    )
    
    doc_intel_manager = DocumentIntelligenceManager(
        docint_cfg['endpoint'],
        docint_cfg['key']
    )
    
    openai_manager = AzureOpenAIManager(
        gpt_endpoint=openai_cfg['endpoint'],
        gpt_api_key=openai_cfg['api_key'],
        gpt_api_version=openai_cfg['api_version'],
        gpt_deployment=openai_cfg['deployment_name'],
        embedding_endpoint=embedding_cfg['endpoint'],
        embedding_api_key=embedding_cfg['api_key'],
        embedding_api_version=embedding_cfg['api_version'],
        embedding_deployment=embedding_cfg['deployment_name'],
        embedding_dimension=embedding_cfg['dimension']
    )
    
    search_manager = AzureAISearchManager(
        search_cfg['endpoint'],
        search_cfg['api_key']
    )
    
    # Initialize RAG extractor
    rag_extractor = RAGExtractor(
        search_endpoint=search_cfg['endpoint'],
        search_api_key=search_cfg['api_key'],
        openai_manager=openai_manager,
        fields=fields,
        top_k=rag_top_k,
        similarity_threshold=rag_similarity_threshold,
        use_rag=use_rag_extraction
    ) if use_rag_extraction else None

    # Global summary
    global_summary = {
        'total_providers': 0,
        'total_documents': 0,
        'high_confidence_providers': 0,
        'low_confidence_providers': 0,
        'total_cost': 0.0,
        'providers': {}
    }

    # Generate timestamp
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Storage for consolidated rows
    high_confidence_rows = []
    low_confidence_rows = []

    providers = blob_manager.get_providers()
    print(f"Found {len(providers)} provider(s): {providers}\n")

    for provider in providers:
        print(f"\n{'='*70}")
        print(f"PROCESSING PROVIDER: {provider.upper()}")
        print(f"{'='*70}")
        
        # Process all documents for this provider
        provider_results = process_provider(
            provider=provider,
            blob_manager=blob_manager,
            doc_intel_manager=doc_intel_manager,
            openai_manager=openai_manager,
            search_manager=search_manager,
            rag_extractor=rag_extractor,
            fields=fields,
            embedding_cfg=embedding_cfg,
            use_rag=use_rag_extraction
        )
        
        if not provider_results['documents']:
            print(f"  âŠ˜ No documents successfully processed for {provider}")
            continue
        
        # CONSOLIDATE all documents into ONE row
        consolidated_row = DocumentConsolidator.consolidate_documents(
            documents=provider_results['documents'],
            fields=fields,
            strategy=consolidation_strategy
        )
        
        # Add provider info
        consolidated_row['provider'] = provider
        consolidated_row['extraction_datetime'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Determine if high or low confidence
        avg_conf = consolidated_row.get('avg_confidence_overall', 0.0)
        
        if avg_conf >= confidence_threshold:
            high_confidence_rows.append(consolidated_row)
            category = "HIGH CONFIDENCE"
            global_summary['high_confidence_providers'] += 1
        else:
            low_confidence_rows.append(consolidated_row)
            category = "LOW CONFIDENCE"
            global_summary['low_confidence_providers'] += 1
        
        # Update summary
        global_summary['total_providers'] += 1
        global_summary['total_documents'] += consolidated_row['total_documents_processed']
        global_summary['providers'][provider] = {
            'documents': consolidated_row['total_documents_processed'],
            'avg_confidence': avg_conf,
            'category': category
        }
        
        print(f"\nâœ“ Provider '{provider}' consolidated:")
        print(f"  - Documents Processed: {consolidated_row['total_documents_processed']}")
        print(f"  - Average Confidence: {avg_conf:.3f}")
        print(f"  - Category: {category}")
        print(f"  - RAG Enhanced: {consolidated_row['rag_enhanced_count']}")
        print(f"  - Standard: {consolidated_row['standard_extraction_count']}")
    
    # Save consolidated results
    save_consolidated_results(
        high_confidence_rows=high_confidence_rows,
        low_confidence_rows=low_confidence_rows,
        fields=fields,
        blob_manager=blob_manager,
        run_timestamp=run_timestamp
    )
    
    # Calculate costs
    total_cost_info = openai_manager.calculate_cost(costs_cfg)
    global_summary['total_cost'] = total_cost_info['total_cost']
    global_summary['cost_breakdown'] = total_cost_info
    
    # Save global summary
    save_global_summary(blob_manager, global_summary, run_timestamp)
    
    # Print final summary
    print_final_summary(global_summary, high_confidence_rows, low_confidence_rows)


def process_provider(
    provider, blob_manager, doc_intel_manager, openai_manager,
    search_manager, rag_extractor, fields, embedding_cfg, use_rag
):
    """Process all documents for a provider"""
    
    files = blob_manager.get_provider_files(provider)
    print(f"Found {len(files)} files for provider '{provider}'")
    
    # Create vector index
    try:
        index_name = search_manager.create_index(provider, embedding_cfg['dimension'])
        print(f"âœ“ Vector index ready: '{index_name}'")
        vector_index_success = True
    except Exception as e:
        print(f"âœ— Vector index failed: {e}")
        vector_index_success = False
    
    # Process each document
    processed_documents = []
    
    for idx, file in enumerate(files, 1):
        print(f"\n  [{idx}/{len(files)}] Processing: {file['filename']}")
        
        result = process_single_document(
            file=file,
            provider=provider,
            blob_manager=blob_manager,
            doc_intel_manager=doc_intel_manager,
            openai_manager=openai_manager,
            search_manager=search_manager,
            rag_extractor=rag_extractor,
            fields=fields,
            embedding_cfg=embedding_cfg,
            vector_index_success=vector_index_success,
            use_rag=use_rag
        )
        
        if result:
            processed_documents.append(result)
    
    return {'documents': processed_documents}


def process_single_document(
    file, provider, blob_manager, doc_intel_manager, openai_manager,
    search_manager, rag_extractor, fields, embedding_cfg,
    vector_index_success, use_rag
):
    """Process a single document"""
    
    doc_id = str(uuid.uuid4())
    blob_name = file['name']
    filename = file['filename']
    
    # Step 1: OCR
    try:
        base64_data = blob_manager.download_blob_as_base64(blob_name)
        ocr_result = doc_intel_manager.analyze_document(base64_data, file['extension'])
        
        if not ocr_result['success']:
            print(f"    âœ— OCR failed")
            return None
        
        text_content = ocr_result['text']
        print(f"    âœ“ OCR: {ocr_result['page_count']} pages, {len(text_content)} chars")
    except Exception as e:
        print(f"    âœ— OCR error: {e}")
        return None
    
    if not text_content or len(text_content.strip()) < 50:
        print(f"    âŠ˜ Insufficient text")
        return None
    
    # Step 2: Field Extraction
    try:
        if use_rag and rag_extractor:
            extraction_result = rag_extractor.extract_with_rag(
                document_text=text_content,
                provider=provider,
                source_document=filename,
                document_type=None
            )
            
            method = extraction_result.get('extraction_method', 'Unknown')
            similar_count = extraction_result.get('similar_docs_count', 0)
            
            if 'RAG' in method:
                print(f"    âœ“ RAG Extraction: {similar_count} similar docs")
            else:
                print(f"    âœ“ Standard Extraction")
        else:
            extraction_result = openai_manager.extract_fields(text_content, fields, filename)
            extraction_result['extraction_method'] = 'Standard (no RAG)'
            extraction_result['similar_docs_count'] = 0
            print(f"    âœ“ Standard Extraction")
        
        extracted_fields = extraction_result.get('extracted_fields', {})
        
    except Exception as e:
        print(f"    âœ— Extraction error: {e}")
        return None
    
    # Step 3: Generate Embeddings
    try:
        embedding_vector = openai_manager.generate_embeddings(text_content)
        print(f"    âœ“ Embeddings: {len(embedding_vector)} dims")
    except Exception as e:
        print(f"    âœ— Embedding error: {e}")
        embedding_vector = [0.0] * embedding_cfg['dimension']
    
    # Step 4: Upload to Vector Database
    if vector_index_success and embedding_vector:
        try:
            search_doc = {
                "id": doc_id,
                "content": text_content[:50000],
                "provider": provider,
                "document_name": filename,
                "file_extension": file['extension'],
                "page_count": ocr_result.get('page_count', 0),
                "extraction_datetime": datetime.utcnow().isoformat(),
                "extracted_fields": json.dumps(extracted_fields),
                "content_vector": embedding_vector
            }
            search_manager.upload_documents(provider, [search_doc])
            print(f"    âœ“ Uploaded to search index")
        except Exception as e:
            print(f"    âœ— Search upload failed: {e}")
    
    return {
        'id': doc_id,
        'provider': provider,
        'document_name': filename,
        'file_extension': file['extension'],
        'page_count': ocr_result.get('page_count', 0),
        'content': text_content,
        'extracted_fields': extracted_fields,
        'extraction_method': extraction_result.get('extraction_method', 'Unknown'),
        'similar_docs_used': extraction_result.get('similar_docs_count', 0),
        'embeddings': embedding_vector
    }


def save_consolidated_results(
    high_confidence_rows, low_confidence_rows, fields,
    blob_manager, run_timestamp
):
    """Save consolidated results (ONE row per provider)"""
    
    print(f"\n{'='*70}")
    print("SAVING CONSOLIDATED RESULTS")
    print(f"{'='*70}")
    
    # Define column order
    base_columns = ['provider', 'extraction_datetime', 'total_documents_processed']
    
    # Field columns (field, field_confidence, field_source_document)
    field_columns = []
    for field in fields:
        field_columns.extend([field, f'{field}_confidence', f'{field}_source_document'])
    
    # Metadata columns
    meta_columns = [
        'avg_confidence_overall', 'rag_enhanced_count', 'standard_extraction_count',
        'total_pages', 'consolidation_strategy'
    ]
    
    all_columns = base_columns + field_columns + meta_columns
    
    # Save High Confidence
    if high_confidence_rows:
        high_df = pd.DataFrame(high_confidence_rows)
        # Reorder columns
        high_df = high_df[[col for col in all_columns if col in high_df.columns]]
        
        csv_path = f"HighConfidence/processedcsvresult/consolidated_{run_timestamp}.csv"
        blob_manager.upload_dataframe_as_csv(high_df, csv_path)
        print(f"âœ“ High Confidence CSV: {csv_path}")
        print(f"  â†’ {len(high_confidence_rows)} provider(s)")
        
        # JSON
        json_data = {
            'timestamp': run_timestamp,
            'confidence_level': 'high',
            'total_providers': len(high_confidence_rows),
            'providers': high_confidence_rows
        }
        json_path = f"HighConfidence/processedjsonresult/consolidated_{run_timestamp}.json"
        blob_manager.upload_to_blob(json.dumps(json_data, indent=2), json_path, 'application/json')
        print(f"âœ“ High Confidence JSON: {json_path}")
    
    # Save Low Confidence
    if low_confidence_rows:
        low_df = pd.DataFrame(low_confidence_rows)
        low_df = low_df[[col for col in all_columns if col in low_df.columns]]
        
        csv_path = f"LowConfidence/processedcsvresult/consolidated_{run_timestamp}.csv"
        blob_manager.upload_dataframe_as_csv(low_df, csv_path)
        print(f"âœ“ Low Confidence CSV: {csv_path}")
        print(f"  â†’ {len(low_confidence_rows)} provider(s)")
        
        # JSON
        json_data = {
            'timestamp': run_timestamp,
            'confidence_level': 'low',
            'total_providers': len(low_confidence_rows),
            'providers': low_confidence_rows
        }
        json_path = f"LowConfidence/processedjsonresult/consolidated_{run_timestamp}.json"
        blob_manager.upload_to_blob(json.dumps(json_data, indent=2), json_path, 'application/json')
        print(f"âœ“ Low Confidence JSON: {json_path}")


def save_global_summary(blob_manager, global_summary, run_timestamp):
    """Save global summary"""
    
    summary_path = f"CostTracking/global_summary_{run_timestamp}.json"
    blob_manager.upload_to_blob(json.dumps(global_summary, indent=2), summary_path, 'application/json')
    print(f"âœ“ Global summary: {summary_path}")


def print_final_summary(global_summary, high_conf_rows, low_conf_rows):
    """Print final summary"""
    
    print(f"\n{'='*70}")
    print("FINAL SUMMARY")
    print(f"{'='*70}")
    print(f"Total Providers:        {global_summary['total_providers']}")
    print(f"Total Documents:        {global_summary['total_documents']}")
    print(f"")
    print(f"High Confidence:        {len(high_conf_rows)} provider(s)")
    print(f"Low Confidence:         {len(low_conf_rows)} provider(s)")
    print(f"")
    print(f"Total Cost:             ${global_summary['total_cost']:.4f}")
    print(f"{'='*70}")
    print("\nâœ“ Processing Complete!")
    print(f"\nOutput Structure:")
    print(f"  - ONE row per provider (consolidated from all documents)")
    print(f"  - High confidence providers: {len(high_conf_rows)}")
    print(f"  - Low confidence providers: {len(low_conf_rows)}")


if __name__ == "__main__":
    import sys
    
    use_rag = True
    if len(sys.argv) > 1:
        use_rag = sys.argv[1].lower() in ['true', '1', 'yes', 'rag']
    
    print(f"Starting pipeline with RAG={'ENABLED' if use_rag else 'DISABLED'}")
    main(use_rag=use_rag)


------

config


{
  "AzureBlob": {
    "connection_string": "YOUR_BLOB_CONNECTION_STRING",
    "inputcontainer": "inputcontainer",
    "outputcontainer": "outputcontainer"
  },
  "AzureOpenAI": {
    "endpoint": "https://YOUR-GPT.openai.azure.com/",
    "api_key": "YOUR_GPT_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "gpt-4o"
  },
  "AzureEmbedding": {
    "endpoint": "https://YOUR-EMBEDDING.openai.azure.com/",
    "api_key": "YOUR_EMBEDDING_KEY",
    "api_version": "2024-02-15-preview",
    "deployment_name": "text-embedding-3-large",
    "dimension": 3072
  },
  "DocumentIntelligence": {
    "endpoint": "https://YOUR_DOC_INTEL.cognitiveservices.azure.com/",
    "key": "YOUR_DOC_INTEL_KEY"
  },
  "AzureAISearch": {
    "endpoint": "https://YOUR_SEARCH.search.windows.net",
    "api_key": "YOUR_SEARCH_KEY"
  },
  "fields": [
    "name",
    "passport_number",
    "date_of_birth",
    "document_number",
    "nationality",
    "issue_date",
    "expiry_date"
  ],
  "confidence_threshold": 0.90,
  "RAG": {
    "enabled": true,
    "top_k": 3,
    "similarity_threshold": 0.70,
    "two_pass_mode": false,
    "consolidation_strategy": "voting",
    "description": "Consolidation strategies: 'voting' (recommended) or 'highest_confidence'"
  },
  "costs": {
    "gpt4o_input_per_1k": 0.0025,
    "gpt4o_output_per_1k": 0.01,
    "embedding_per_1k": 0.00013,
    "doc_intel_per_page": 0.01
  }
}



------


helper

import os
import json
import base64
import logging
import hashlib
from datetime import datetime
from io import BytesIO
from typing import List, Dict, Any, Tuple

import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential

from azure.storage.blob import BlobServiceClient, ContentSettings
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchableField, SearchField, SearchFieldDataType,  # ADDED SearchField
    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,
    SemanticConfiguration, SemanticField, SemanticPrioritizedFields, SemanticSearch
)
from openai import AzureOpenAI

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)


class ConfigManager:
    def __init__(self, config_path: str = 'config.json'):
        self.config_path = config_path
        self.config = self.load_config()

    def load_config(self) -> Dict[str, Any]:
        with open(self.config_path, 'r') as f:
            cfg = json.load(f)
        return cfg

    def get(self, section: str, key: str = None) -> Any:
        if key:
            return self.config.get(section, {}).get(key)
        return self.config.get(section)


class AzureBlobManager:
    def __init__(self, connection_string: str, input_container: str, output_container: str):
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self.input_container = input_container
        self.output_container = output_container
        self.supported_extensions = {'.pdf', '.doc', '.docx', '.png', '.jpg', '.jpeg', '.tiff', '.tif'}

    def get_providers(self) -> List[str]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs()
        providers = set()
        for blob in blobs:
            parts = blob.name.split('/')
            if parts[0]:
                providers.add(parts[0])
        return sorted(list(providers))

    def get_provider_files(self, provider: str) -> List[Dict[str, str]]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs(name_starts_with=f"{provider}/")
        files = []
        for blob in blobs:
            ext = os.path.splitext(blob.name)[1].lower()
            if ext in self.supported_extensions:
                files.append({
                    'name': blob.name,
                    'filename': os.path.basename(blob.name),
                    'provider': provider,
                    'size': blob.size,
                    'extension': ext
                })
        return files

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def download_blob_as_base64(self, blob_name: str) -> str:
        blob_client = self.blob_service_client.get_blob_client(self.input_container, blob_name)
        data = blob_client.download_blob().readall()
        return base64.b64encode(data).decode('utf-8')

    def upload_to_blob(self, data, blob_path: str, content_type: str = 'text/plain'):
        """Upload data to blob storage"""
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        content_settings = ContentSettings(content_type=content_type)
        if isinstance(data, str):
            data = data.encode('utf-8')
        blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
        logger.info(f"Uploaded to blob: {blob_path}")

    def upload_dataframe_as_csv(self, df: pd.DataFrame, blob_path: str):
        csv_buffer = BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_data = csv_buffer.getvalue()
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        content_settings = ContentSettings(content_type='text/csv')
        blob_client.upload_blob(csv_data, overwrite=True, content_settings=content_settings)
        logger.info(f"Uploaded CSV to blob: {blob_path}")


class DocumentIntelligenceManager:
    def __init__(self, endpoint: str, key: str):
        self.client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def analyze_document(self, base64_data: str, file_extension: str) -> Dict[str, Any]:
        try:
            content_type_map = {
                '.pdf': 'application/pdf',
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.tiff': 'image/tiff',
                '.tif': 'image/tiff',
                '.bmp': 'image/bmp'
            }
            content_type = content_type_map.get(file_extension.lower(), 'application/pdf')
            document_bytes = base64.b64decode(base64_data)
            
            logger.info(f"Starting OCR for {file_extension}, size: {len(document_bytes)} bytes")
            
            poller = self.client.begin_analyze_document(
                model_id="prebuilt-read",
                analyze_request=document_bytes,
                content_type=content_type
            )
            
            result = poller.result()
            text = ''
            page_count = 0
            
            if hasattr(result, 'pages') and result.pages:
                page_count = len(result.pages)
                for page in result.pages:
                    if hasattr(page, 'lines') and page.lines:
                        for line in page.lines:
                            if hasattr(line, 'content'):
                                text += line.content + "\n"
            
            logger.info(f"OCR completed: {page_count} pages, {len(text)} characters")
            
            return {
                'success': True,
                'text': text.strip(),
                'page_count': page_count
            }
            
        except Exception as e:
            logger.error(f"OCR failed: {e}", exc_info=True)
            return {
                'success': False,
                'text': '',
                'page_count': 0,
                'error': str(e)
            }


class AzureOpenAIManager:
    """Manages both GPT extraction and embeddings with separate clients"""
    
    def __init__(self, gpt_endpoint: str, gpt_api_key: str, gpt_api_version: str, gpt_deployment: str,
                 embedding_endpoint: str, embedding_api_key: str, embedding_api_version: str, 
                 embedding_deployment: str, embedding_dimension: int = 3072):
        
        # GPT-4o client for field extraction
        self.gpt_client = AzureOpenAI(
            azure_endpoint=gpt_endpoint,
            api_key=gpt_api_key,
            api_version=gpt_api_version
        )
        self.gpt_deployment = gpt_deployment
        
        # Separate embedding client (important for separate deployment)
        self.embedding_client = AzureOpenAI(
            azure_endpoint=embedding_endpoint,
            api_key=embedding_api_key,
            api_version=embedding_api_version
        )
        self.embedding_deployment = embedding_deployment
        self.embedding_dimension = embedding_dimension
        
        # Token tracking
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def extract_fields(self, text: str, fields: List[str], source_document: str) -> Dict[str, Any]:
        try:
            system_prompt = f"""You are a document extraction expert. Extract the following fields: {', '.join(fields)}.

Return ONLY a JSON object with this structure:
{{
    "field_name": {{"value": "extracted_value", "confidence": 0.95}},
    ...
}}

Be precise with confidence scores."""
            
            user_prompt = f"Document text:\n\n{text[:8000]}"
            
            response = self.gpt_client.chat.completions.create(
                model=self.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                max_tokens=2000
            )
            
            self.total_tokens += response.usage.total_tokens
            self.prompt_tokens += response.usage.prompt_tokens
            self.completion_tokens += response.usage.completion_tokens

            content = response.choices[0].message.content.strip()
            
            # Remove markdown code blocks
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()
            
            data = json.loads(content)
            
            # Normalize data format
            normalized_data = {}
            for field_name in data:
                field_value = data[field_name]
                
                if isinstance(field_value, dict) and 'value' in field_value:
                    normalized_data[field_name] = field_value
                    normalized_data[field_name]['source_document'] = source_document
                else:
                    # Wrap direct values
                    normalized_data[field_name] = {
                        'value': str(field_value),
                        'confidence': 0.5,
                        'source_document': source_document
                    }
            
            logger.info(f"Extracted {len(normalized_data)} fields successfully")
            
            return {
                'success': True,
                'extracted_fields': normalized_data,
                'raw_response': content
            }
            
        except Exception as e:
            logger.error(f"Field extraction failed: {e}", exc_info=True)
            return {
                'success': False,
                'extracted_fields': {},
                'raw_response': '',
                'error': str(e)
            }

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def generate_embeddings(self, text: str) -> List[float]:
        """Generate embeddings using separate embedding deployment"""
        try:
            # Validate input
            if not text or not isinstance(text, str):
                logger.warning("Invalid text for embeddings, using placeholder")
                text = "No content available"
            
            text = str(text).strip()
            
            if len(text) < 3:
                logger.warning("Text too short, using placeholder")
                text = "No content available"
            
            # Truncate if needed
            if len(text) > 30000:
                text = text[:30000]
                logger.info("Text truncated for embedding")
            
            # Call embedding endpoint (separate from GPT)
            response = self.embedding_client.embeddings.create(
                model=self.embedding_deployment,
                input=text
            )
            
            embeddings = response.data[0].embedding
            logger.info(f"Generated embeddings: dimension={len(embeddings)}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}", exc_info=True)
            # Return zero vector as fallback
            logger.warning(f"Returning zero vector of dimension {self.embedding_dimension}")
            return [0.0] * self.embedding_dimension

    def get_token_usage(self) -> Dict[str, int]:
        return {
            'total_tokens': self.total_tokens,
            'prompt_tokens': self.prompt_tokens,
            'completion_tokens': self.completion_tokens
        }

    def calculate_cost(self, costs_config: Dict[str, float]) -> Dict[str, float]:
        input_cost = (self.prompt_tokens / 1000) * costs_config.get('gpt4o_input_per_1k', 0)
        output_cost = (self.completion_tokens / 1000) * costs_config.get('gpt4o_output_per_1k', 0)
        total_cost = input_cost + output_cost
        return {
            'input_cost': input_cost,
            'output_cost': output_cost,
            'total_cost': total_cost
        }


class AzureAISearchManager:
    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.credential = AzureKeyCredential(api_key)
        self.index_client = SearchIndexClient(endpoint=endpoint, credential=self.credential)

    def get_index_name(self, provider: str) -> str:
        # Sanitize provider name for Azure AI Search
        sanitized = provider.lower().replace(' ', '-').replace('_', '-')
        sanitized = ''.join(c for c in sanitized if c.isalnum() or c == '-')
        return sanitized.strip('-')

    def create_index(self, provider: str, embedding_dimension: int = 3072):
        """Create search index with vector field"""
        index_name = self.get_index_name(provider)
        
        logger.info(f"Creating index '{index_name}' with vector dimension {embedding_dimension}")
        
        # Define fields including vector field using SearchField
        fields = [
            SimpleField(name="id", type=SearchFieldDataType.String, key=True),
            SearchableField(name="content", type=SearchFieldDataType.String),
            SearchableField(name="provider", type=SearchFieldDataType.String, filterable=True),
            SimpleField(name="page_count", type=SearchFieldDataType.Int32, filterable=True),
            # CRITICAL: Use SearchField for vector field
            SearchField(
                name="content_vector",
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                searchable=True,
                vector_search_dimensions=embedding_dimension,
                vector_search_profile_name="vector-profile"
            )
        ]
        
        # Vector search configuration
        vector_search = VectorSearch(
            algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
            profiles=[VectorSearchProfile(name="vector-profile", algorithm_configuration_name="hnsw-config")]
        )
        
        # Create index
        index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
        self.index_client.create_or_update_index(index)
        
        logger.info(f"Index '{index_name}' created successfully")
        return index_name

    def upload_documents(self, provider: str, documents: List[Dict[str, Any]]):
        """Upload documents to search index"""
        if not documents:
            logger.warning("No documents to upload")
            return
        
        index_name = self.get_index_name(provider)
        
        logger.info(f"Uploading {len(documents)} documents to index '{index_name}'")
        
        try:
            client = SearchClient(
                endpoint=self.endpoint,
                index_name=index_name,
                credential=self.credential
            )
            
            result = client.upload_documents(documents=documents)
            
            # Check results
            success_count = sum(1 for r in result if r.succeeded)
            logger.info(f"Uploaded {success_count}/{len(documents)} documents successfully")
            
            if success_count < len(documents):
                failed = [r for r in result if not r.succeeded]
                for fail in failed:
                    logger.error(f"Failed to upload document: {fail.key} - {fail.error_message}")
            
        except Exception as e:
            logger.error(f"Document upload failed: {e}", exc_info=True)
            raise


----

prompt

"""
RAG-Enhanced Document Extraction Prompts
Uses vector search to find similar documents and improve extraction accuracy
"""

import json
from typing import List, Dict, Any


class ExtractionPromptBuilder:
    """Builds prompts for RAG-enhanced field extraction"""
    
    def __init__(self, fields: List[str]):
        self.fields = fields
    
    def build_system_prompt(self) -> str:
        """Build system prompt for extraction"""
        field_list = ", ".join(self.fields)
        
        return f"""You are an expert document extraction system specialized in identity documents, passports, licenses, and official records.

Your task is to extract the following fields from documents:
{field_list}

EXTRACTION RULES:
1. Extract EXACT values as they appear in the document
2. For each field, provide:
   - value: The extracted text (or null if not found)
   - confidence: Your confidence score (0.0 to 1.0)
3. Use similar document examples provided to improve accuracy
4. If a field is not present, return null for value and 0.0 for confidence
5. Be conservative with confidence scores - only use >0.9 when certain

CONFIDENCE SCORING GUIDE:
- 0.95-1.0: Clear, unambiguous text that exactly matches expected format
- 0.80-0.94: Text is clear but format slightly unusual
- 0.60-0.79: Text is readable but context suggests some uncertainty
- 0.40-0.59: Partial match or low quality OCR
- 0.20-0.39: Very uncertain, possibly misread
- 0.0-0.19: Field not found or illegible

OUTPUT FORMAT:
Return ONLY a JSON object with this exact structure:
{{
    "field_name": {{"value": "extracted_value", "confidence": 0.95}},
    "another_field": {{"value": "extracted_value", "confidence": 0.87}},
    ...
}}

Do NOT include any explanations, markdown formatting, or additional text.
Just the pure JSON object."""
    
    def build_extraction_prompt_without_rag(self, document_text: str) -> str:
        """Build extraction prompt WITHOUT RAG context (fallback)"""
        return f"""Extract the required fields from this document:

DOCUMENT TEXT:
{document_text[:8000]}

Remember to return ONLY the JSON object with extracted fields and confidence scores."""
    
    def build_extraction_prompt_with_rag(
        self,
        document_text: str,
        similar_documents: List[Dict[str, Any]],
        top_k: int = 3
    ) -> str:
        """Build extraction prompt WITH RAG context from similar documents"""
        
        # Limit to top K similar documents
        similar_docs = similar_documents[:top_k]
        
        # Build examples section
        examples_text = self._format_similar_documents(similar_docs)
        
        prompt = f"""You have access to {len(similar_docs)} similar documents that have already been processed. Use these as reference examples to improve extraction accuracy.

SIMILAR DOCUMENT EXAMPLES:
{examples_text}

---

NOW EXTRACT FROM THIS NEW DOCUMENT:

DOCUMENT TEXT:
{document_text[:8000]}

Use the similar documents above as examples of:
- How fields typically appear in this type of document
- Expected formats and patterns
- Typical confidence levels based on text quality

Remember to return ONLY the JSON object with extracted fields and confidence scores."""
        
        return prompt
    
    def _format_similar_documents(self, similar_docs: List[Dict[str, Any]]) -> str:
        """Format similar documents as examples in the prompt"""
        examples = []
        
        for idx, doc in enumerate(similar_docs, 1):
            # Get extracted fields from the document
            extracted_fields_str = doc.get('extracted_fields', '{}')
            
            # Parse if it's a JSON string
            if isinstance(extracted_fields_str, str):
                try:
                    extracted_fields = json.loads(extracted_fields_str)
                except:
                    extracted_fields = {}
            else:
                extracted_fields = extracted_fields_str
            
            # Get document metadata
            doc_name = doc.get('document_name', f'Document {idx}')
            similarity_score = doc.get('@search.score', 0.0)
            content_preview = doc.get('content', '')[:200]
            
            # Format this example
            example = f"""
EXAMPLE {idx} (Similarity: {similarity_score:.2f})
Document: {doc_name}
Content Preview: {content_preview}...

Extracted Fields:
{json.dumps(extracted_fields, indent=2)}
"""
            examples.append(example)
        
        return "\n".join(examples)
    
    def build_validation_prompt(
        self,
        document_text: str,
        initial_extraction: Dict[str, Any],
        similar_documents: List[Dict[str, Any]]
    ) -> str:
        """Build prompt for validation/refinement pass using RAG"""
        
        examples_text = self._format_similar_documents(similar_documents[:3])
        
        return f"""You are validating and refining extracted fields.

SIMILAR DOCUMENTS (as reference):
{examples_text}

---

CURRENT DOCUMENT TEXT:
{document_text[:5000]}

INITIAL EXTRACTION (to validate):
{json.dumps(initial_extraction, indent=2)}

VALIDATION TASK:
1. Compare initial extraction against similar document patterns
2. Identify any fields that seem incorrect or have wrong confidence
3. Refine values and confidence scores based on examples
4. Return the CORRECTED extraction in the same JSON format

Return ONLY the corrected JSON object."""


class PromptTemplates:
    """Pre-defined prompt templates for common document types"""
    
    @staticmethod
    def passport_prompt() -> str:
        """Specialized prompt for passport documents"""
        return """PASSPORT DOCUMENT EXTRACTION

Key fields to extract:
- Full name (as shown on passport)
- Passport number (alphanumeric, 7-10 characters typically)
- Date of birth (format: DD MMM YYYY or DD/MM/YYYY)
- Nationality
- Issue date
- Expiry date
- Place of birth (if shown)

Special considerations:
- Passport numbers may include letters and numbers
- Dates are often in DD MMM YYYY format (e.g., 15 JAN 1990)
- Look for MRZ (Machine Readable Zone) at bottom for verification
- Names may be split across multiple lines"""
    
    @staticmethod
    def drivers_license_prompt() -> str:
        """Specialized prompt for driver's license"""
        return """DRIVER'S LICENSE EXTRACTION

Key fields to extract:
- Full name
- License number
- Date of birth
- Issue date
- Expiry date
- Address (if present)
- License class/categories

Special considerations:
- License numbers vary by state/country
- May contain multiple date fields (issue, expiry, DOB)
- Address may span multiple lines"""
    
    @staticmethod
    def id_card_prompt() -> str:
        """Specialized prompt for ID cards"""
        return """ID CARD EXTRACTION

Key fields to extract:
- Full name
- ID number
- Date of birth
- Issue date
- Expiry date
- Address
- Nationality

Special considerations:
- ID formats vary significantly by country
- May have both front and back text
- Look for official stamps or holograms mentioned in text"""


class PromptOptimizer:
    """Optimize prompts based on document type and field requirements"""
    
    @staticmethod
    def optimize_for_document_type(
        base_prompt: str,
        document_type: str = None,
        ocr_quality: str = "high"
    ) -> str:
        """Optimize prompt based on document characteristics"""
        
        optimizations = []
        
        # Document type specific optimization
        if document_type:
            if document_type.lower() == "passport":
                optimizations.append(PromptTemplates.passport_prompt())
            elif document_type.lower() in ["license", "drivers_license", "dl"]:
                optimizations.append(PromptTemplates.drivers_license_prompt())
            elif document_type.lower() in ["id", "id_card", "national_id"]:
                optimizations.append(PromptTemplates.id_card_prompt())
        
        # OCR quality optimization
        if ocr_quality.lower() == "low":
            optimizations.append("""
OCR QUALITY NOTE: This text may contain OCR errors.
- Be more conservative with confidence scores
- Look for context clues to verify extracted values
- Consider common OCR mistakes (0/O, 1/I, 5/S, etc.)""")
        
        # Combine optimizations
        if optimizations:
            return base_prompt + "\n\nADDITIONAL CONTEXT:\n" + "\n".join(optimizations)
        
        return base_prompt
    
    @staticmethod
    def add_field_hints(base_prompt: str, field_hints: Dict[str, str]) -> str:
        """Add specific hints for certain fields"""
        
        if not field_hints:
            return base_prompt
        
        hints_text = "\n\nFIELD-SPECIFIC HINTS:"
        for field, hint in field_hints.items():
            hints_text += f"\n- {field}: {hint}"
        
        return base_prompt + hints_text


# Example usage and test functions
def example_usage():
    """Example of how to use the prompt builder"""
    
    # Define fields to extract
    fields = ["name", "passport_number", "date_of_birth", "nationality", "issue_date", "expiry_date"]
    
    # Create prompt builder
    builder = ExtractionPromptBuilder(fields)
    
    # Build system prompt
    system_prompt = builder.build_system_prompt()
    print("SYSTEM PROMPT:")
    print(system_prompt)
    print("\n" + "="*80 + "\n")
    
    # Example document text
    document_text = """
    PASSPORT
    
    Type: P
    Country Code: USA
    Passport No.: 123456789
    Surname: DOE
    Given Names: JOHN MICHAEL
    Nationality: UNITED STATES OF AMERICA
    Date of Birth: 15 JAN 1985
    Sex: M
    Place of Birth: NEW YORK, USA
    Date of Issue: 01 MAR 2020
    Date of Expiry: 01 MAR 2030
    """
    
    # Example similar documents from vector search
    similar_documents = [
        {
            'document_name': 'passport_001.pdf',
            '@search.score': 0.92,
            'content': 'PASSPORT Type P Country USA...',
            'extracted_fields': json.dumps({
                'name': {'value': 'JANE SMITH', 'confidence': 0.95},
                'passport_number': {'value': '987654321', 'confidence': 0.98},
                'date_of_birth': {'value': '20 MAR 1990', 'confidence': 0.95}
            })
        },
        {
            'document_name': 'passport_002.pdf',
            '@search.score': 0.88,
            'content': 'PASSPORT Type P Country GBR...',
            'extracted_fields': json.dumps({
                'name': {'value': 'ROBERT JOHNSON', 'confidence': 0.92},
                'passport_number': {'value': 'AB1234567', 'confidence': 0.94},
                'date_of_birth': {'value': '05 JUL 1988', 'confidence': 0.90}
            })
        }
    ]
    
    # Build prompt WITHOUT RAG (fallback)
    simple_prompt = builder.build_extraction_prompt_without_rag(document_text)
    print("SIMPLE PROMPT (No RAG):")
    print(simple_prompt)
    print("\n" + "="*80 + "\n")
    
    # Build prompt WITH RAG (enhanced)
    rag_prompt = builder.build_extraction_prompt_with_rag(document_text, similar_documents)
    print("RAG-ENHANCED PROMPT:")
    print(rag_prompt)
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    example_usage()


------

rag

"""
RAG-Enhanced Document Extraction
Uses vector similarity search to find similar documents and improve extraction accuracy
"""

import logging
from typing import List, Dict, Any, Optional
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt import ExtractionPromptBuilder, PromptOptimizer

logger = logging.getLogger(__name__)


class RAGExtractor:
    """
    RAG-Enhanced extractor that uses vector similarity search
    to find similar documents and improve extraction accuracy
    """
    
    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,  # AzureOpenAIManager instance
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        use_rag: bool = True
    ):
        self.search_endpoint = search_endpoint
        self.search_credential = AzureKeyCredential(search_api_key)
        self.openai_manager = openai_manager
        self.fields = fields
        self.top_k = top_k
        self.similarity_threshold = similarity_threshold
        self.use_rag = use_rag
        
        # Initialize prompt builder
        self.prompt_builder = ExtractionPromptBuilder(fields)
        
        logger.info(f"RAGExtractor initialized with top_k={top_k}, use_rag={use_rag}")
    
    def extract_with_rag(
        self,
        document_text: str,
        provider: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using RAG-enhanced prompting
        
        Args:
            document_text: The OCR extracted text
            provider: Provider name (for index selection)
            source_document: Source document name
            document_type: Optional document type hint (passport, license, etc.)
            
        Returns:
            Dictionary with extracted_fields, similar_docs, and metadata
        """
        
        logger.info(f"Starting RAG extraction for {source_document}")
        
        # Step 1: Generate embedding for current document
        try:
            query_vector = self.openai_manager.generate_embeddings(document_text)
            logger.info(f"Generated query embedding: dimension={len(query_vector)}")
        except Exception as e:
            logger.error(f"Failed to generate query embedding: {e}")
            # Fallback to non-RAG extraction
            return self._extract_without_rag(document_text, source_document, document_type)
        
        # Step 2: Search for similar documents in vector store
        similar_docs = []
        if self.use_rag:
            try:
                similar_docs = self._search_similar_documents(
                    provider=provider,
                    query_vector=query_vector,
                    top_k=self.top_k
                )
                logger.info(f"Found {len(similar_docs)} similar documents")
                
                # Filter by similarity threshold
                similar_docs = [
                    doc for doc in similar_docs
                    if doc.get('@search.score', 0) >= self.similarity_threshold
                ]
                logger.info(f"After filtering: {len(similar_docs)} documents above threshold {self.similarity_threshold}")
                
            except Exception as e:
                logger.warning(f"Vector search failed: {e}, falling back to non-RAG")
                similar_docs = []
        
        # Step 3: Build prompt (with or without RAG context)
        if similar_docs:
            user_prompt = self.prompt_builder.build_extraction_prompt_with_rag(
                document_text=document_text,
                similar_documents=similar_docs,
                top_k=min(len(similar_docs), self.top_k)
            )
            extraction_method = "RAG-enhanced"
        else:
            user_prompt = self.prompt_builder.build_extraction_prompt_without_rag(
                document_text=document_text
            )
            extraction_method = "Standard"
        
        # Step 4: Build system prompt
        system_prompt = self.prompt_builder.build_system_prompt()
        
        # Optimize for document type if provided
        if document_type:
            system_prompt = PromptOptimizer.optimize_for_document_type(
                system_prompt,
                document_type=document_type
            )
        
        logger.info(f"Using {extraction_method} extraction")
        
        # Step 5: Call GPT for extraction
        try:
            result = self._call_extraction_api(system_prompt, user_prompt, source_document)
            
            # Add metadata
            result['extraction_method'] = extraction_method
            result['similar_docs_count'] = len(similar_docs)
            result['similarity_scores'] = [
                doc.get('@search.score', 0) for doc in similar_docs
            ]
            
            return result
            
        except Exception as e:
            logger.error(f"Extraction failed: {e}")
            return {
                'success': False,
                'extracted_fields': {},
                'error': str(e),
                'extraction_method': extraction_method
            }
    
    def _search_similar_documents(
        self,
        provider: str,
        query_vector: List[float],
        top_k: int
    ) -> List[Dict[str, Any]]:
        """
        Search for similar documents using vector similarity
        
        Args:
            provider: Provider name (determines index)
            query_vector: Query embedding vector
            top_k: Number of similar documents to retrieve
            
        Returns:
            List of similar documents with scores
        """
        
        # Sanitize provider name for index
        index_name = provider.lower().replace(' ', '-').replace('_', '-')
        index_name = ''.join(c for c in index_name if c.isalnum() or c == '-')
        index_name = index_name.strip('-')
        
        logger.info(f"Searching index '{index_name}' for similar documents")
        
        try:
            # Create search client for this provider's index
            search_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            # Perform vector similarity search
            results = search_client.search(
                search_text=None,  # Pure vector search
                vector_queries=[{
                    "kind": "vector",
                    "vector": query_vector,
                    "fields": "content_vector",
                    "k": top_k
                }],
                select=["id", "content", "document_name", "provider", "extracted_fields", "page_count"],
                top=top_k
            )
            
            # Convert to list and add scores
            similar_documents = []
            for result in results:
                doc = dict(result)
                similar_documents.append(doc)
            
            logger.info(f"Retrieved {len(similar_documents)} similar documents")
            
            return similar_documents
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}", exc_info=True)
            return []
    
    def _extract_without_rag(
        self,
        document_text: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """Fallback extraction without RAG"""
        
        logger.info("Performing extraction without RAG context")
        
        system_prompt = self.prompt_builder.build_system_prompt()
        
        if document_type:
            system_prompt = PromptOptimizer.optimize_for_document_type(
                system_prompt,
                document_type=document_type
            )
        
        user_prompt = self.prompt_builder.build_extraction_prompt_without_rag(document_text)
        
        result = self._call_extraction_api(system_prompt, user_prompt, source_document)
        result['extraction_method'] = "Standard (no RAG)"
        result['similar_docs_count'] = 0
        
        return result
    
    def _call_extraction_api(
        self,
        system_prompt: str,
        user_prompt: str,
        source_document: str
    ) -> Dict[str, Any]:
        """Call OpenAI API for extraction"""
        
        # Use the existing openai_manager's extract_fields method
        # but we need to call it directly with custom prompts
        
        try:
            import json
            
            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                max_tokens=2000
            )
            
            # Track tokens
            self.openai_manager.total_tokens += response.usage.total_tokens
            self.openai_manager.prompt_tokens += response.usage.prompt_tokens
            self.openai_manager.completion_tokens += response.usage.completion_tokens
            
            content = response.choices[0].message.content.strip()
            
            # Remove markdown code blocks
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()
            
            # Parse JSON
            data = json.loads(content)
            
            # Normalize data format
            normalized_data = {}
            for field_name in data:
                field_value = data[field_name]
                
                if isinstance(field_value, dict) and 'value' in field_value:
                    normalized_data[field_name] = field_value
                    normalized_data[field_name]['source_document'] = source_document
                else:
                    normalized_data[field_name] = {
                        'value': str(field_value),
                        'confidence': 0.5,
                        'source_document': source_document
                    }
            
            logger.info(f"Successfully extracted {len(normalized_data)} fields")
            
            return {
                'success': True,
                'extracted_fields': normalized_data,
                'raw_response': content
            }
            
        except Exception as e:
            logger.error(f"API call failed: {e}", exc_info=True)
            return {
                'success': False,
                'extracted_fields': {},
                'raw_response': '',
                'error': str(e)
            }
    
    def validate_extraction(
        self,
        document_text: str,
        initial_extraction: Dict[str, Any],
        similar_documents: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Perform validation/refinement pass using similar documents
        
        This is an optional second pass to improve accuracy
        """
        
        if not similar_documents:
            logger.info("No similar documents for validation, skipping")
            return initial_extraction
        
        logger.info(f"Validating extraction using {len(similar_documents)} similar documents")
        
        validation_prompt = self.prompt_builder.build_validation_prompt(
            document_text=document_text,
            initial_extraction=initial_extraction,
            similar_documents=similar_documents
        )
        
        system_prompt = "You are validating extracted document fields for accuracy."
        
        try:
            result = self._call_extraction_api(
                system_prompt=system_prompt,
                user_prompt=validation_prompt,
                source_document="validation"
            )
            
            if result['success']:
                logger.info("Validation pass completed successfully")
                return result['extracted_fields']
            else:
                logger.warning("Validation failed, using initial extraction")
                return initial_extraction
                
        except Exception as e:
            logger.error(f"Validation error: {e}")
            return initial_extraction


def test_rag_extractor():
    """Test function for RAG extractor"""
    
    # This would be called in your main pipeline
    # Example:
    
    from helper import ConfigManager, AzureOpenAIManager
    
    cfg = ConfigManager('config.json')
    
    openai_cfg = cfg.get("AzureOpenAI")
    embedding_cfg = cfg.get("AzureEmbedding")
    search_cfg = cfg.get("AzureAISearch")
    fields = cfg.get("fields")
    
    # Initialize OpenAI manager
    openai_manager = AzureOpenAIManager(
        gpt_endpoint=openai_cfg['endpoint'],
        gpt_api_key=openai_cfg['api_key'],
        gpt_api_version=openai_cfg['api_version'],
        gpt_deployment=openai_cfg['deployment_name'],
        embedding_endpoint=embedding_cfg['endpoint'],
        embedding_api_key=embedding_cfg['api_key'],
        embedding_api_version=embedding_cfg['api_version'],
        embedding_deployment=embedding_cfg['deployment_name'],
        embedding_dimension=embedding_cfg['dimension']
    )
    
    # Initialize RAG extractor
    rag_extractor = RAGExtractor(
        search_endpoint=search_cfg['endpoint'],
        search_api_key=search_cfg['api_key'],
        openai_manager=openai_manager,
        fields=fields,
        top_k=3,
        similarity_threshold=0.7,
        use_rag=True
    )
    
    # Test document
    test_text = """
    PASSPORT
    Name: JOHN DOE
    Passport No: AB123456
    DOB: 15 JAN 1985
    Nationality: USA
    """
    
    # Extract with RAG
    result = rag_extractor.extract_with_rag(
        document_text=test_text,
        provider="test_provider",
        source_document="test.pdf",
        document_type="passport"
    )
    
    print("Extraction Result:")
    print(json.dumps(result, indent=2))


if __name__ == "__main__":
    import json
    test_rag_extractor()
