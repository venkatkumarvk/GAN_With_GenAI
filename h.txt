def process_azure_pdf_files(config, api_type, azure_folder, logger):
    """
    Process PDF files from Azure Blob Storage with archiving support and SQL logging.
    
    Parameters:
    - config: Configuration dictionary
    - api_type: 'batch' or 'general'
    - azure_folder: Folder path in Azure Blob Storage
    - logger: Logger instance
    """
    # Initialize helpers with archive container
    archive_container = config["azure_storage"].get("input_archive_container")
    logger.info(f"Initializing Azure Storage Helper with containers:")
    logger.info(f"  Input: {config['azure_storage']['input_container']}")
    logger.info(f"  Output: {config['azure_storage']['output_container']}")
    logger.info(f"  Archive: {archive_container}")
    
    storage_helper = AzureStorageHelper(
        config["azure_storage"]["connection_string"],
        config["azure_storage"]["input_container"],
        config["azure_storage"]["output_container"],
        archive_container,
        logger
    )
    
    pdf_processor = PDFProcessor(config, logger)
    
    logger.info(f"Initializing Azure OpenAI Client with {api_type} API")
    ai_client = AzureOpenAIClient(config, logger)
    
    # List PDF blobs in the specified folder
    logger.info(f"Listing PDF files in Azure folder: {azure_folder}")
    pdf_blobs = storage_helper.list_blobs_in_folder(azure_folder)
    
    if not pdf_blobs:
        logger.warning(f"No PDF files found in folder: {azure_folder}")
        return
    
    logger.info(f"Found {len(pdf_blobs)} PDF files to process")
    
    # Track processed and unprocessed files for archiving
    processed_files = []
    unprocessed_files = []
    
    # Track document IDs for archive path updates
    file_document_ids = {}  # blob_name -> document_id mapping
    file_target_paths = {}  # blob_name -> target_path mapping
    
    # Process each PDF
    for i, blob_name in enumerate(pdf_blobs):
        file_processed_successfully = False
        document_id = None  # Initialize document_id for this file
        filename = blob_name.split('/')[-1]
        
        # Initialize paths for SQL logging
        source_path = blob_name
        target_path = ""  # Will be updated when we know the target
        archive_path = ""  # Will be updated during archiving
        
        try:
            logger.info(f"Processing file {i+1}/{len(pdf_blobs)}: {blob_name}")
            
            # **SQL LOGGING - BEGIN: Log the start of processing**
            document_id = log_file_status_begin(
                filename=filename,
                source_path=source_path,
                target_path=target_path,
                archive_path=archive_path,
                status_desc="Starting PDF processing",
                config=config
            )
            
            # Store document_id for later archive updates
            if document_id:
                file_document_ids[blob_name] = document_id
                logger.info(f"SQL Log: Started processing log with ID {document_id}")
            else:
                logger.warning(f"SQL Log: Failed to create initial log entry for {filename}")
            
            # Download blob to memory
            logger.debug(f"Downloading blob: {blob_name}")
            blob_content = storage_helper.download_blob_to_memory(blob_name)
            
            if blob_content is None:
                logger.error(f"Could not download blob: {blob_name}")
                unprocessed_files.append(blob_name)
                
                # **SQL LOGGING - UPDATE: Failed to download**
                if document_id:
                    log_file_status_update(
                        document_id=document_id,
                        filename=filename,
                        source_path=source_path,
                        target_path=target_path,
                        archive_path=archive_path,
                        status="FAILED",
                        status_desc="Failed to download blob from Azure storage",
                        config=config
                    )
                continue
            
            # Extract pages as base64 strings
            logger.info(f"Extracting pages from {filename}")
            pages = pdf_processor.extract_pdf_pages(blob_content)
            
            if not pages:
                logger.warning(f"No pages extracted from {filename}")
                unprocessed_files.append(blob_name)
                
                # **SQL LOGGING - UPDATE: No pages extracted**
                if document_id:
                    log_file_status_update(
                        document_id=document_id,
                        filename=filename,
                        source_path=source_path,
                        target_path=target_path,
                        archive_path=archive_path,
                        status="FAILED",
                        status_desc="No pages could be extracted from PDF",
                        config=config
                    )
                continue
            
            logger.info(f"Extracted {len(pages)} pages from {filename}")
            
            # **SQL LOGGING - UPDATE: Successfully extracted pages**
            if document_id:
                log_file_status_update(
                    document_id=document_id,
                    filename=filename,
                    source_path=source_path,
                    target_path=target_path,
                    archive_path=archive_path,
                    status="PROCESSING",
                    status_desc=f"Successfully extracted {len(pages)} pages, starting AI processing",
                    config=config
                )
            
            # Prepare batches for processing
            batch_size = config["processing"]["batch_size"]
            
            all_results = []
            batch_processing_successful = True
            
            for batch_start in range(0, len(pages), batch_size):
                batch_end = min(batch_start + batch_size, len(pages))
                batch_pages = pages[batch_start:batch_end]
                
                # Split into page numbers and base64 strings
                page_nums = [p[0] for p in batch_pages]
                base64_strings = [p[1] for p in batch_pages]
                
                # Create prompts
                prompts = [pdf_processor.create_extraction_prompt() for _ in range(len(batch_pages))]
                
                logger.info(f"Processing batch of {len(batch_pages)} pages (pages {batch_start+1}-{batch_end})")
                
                # Process batch using specified API type
                try:
                    if api_type == "batch":
                        logger.debug("Using batch API for processing")
                        raw_results = ai_client.process_batch(base64_strings, prompts)
                    else:
                        logger.debug("Using general API for processing")
                        raw_results = ai_client.process_general(base64_strings, prompts)
                    
                    # Process the results
                    logger.debug("Processing batch results")
                    processed_results = pdf_processor.process_batch_results(raw_results, page_nums)
                    all_results.extend(processed_results)
                    
                    logger.info(f"Processed batch {batch_start+1}-{batch_end}")
                except Exception as batch_error:
                    logger.error(f"Error processing batch: {str(batch_error)}")
                    batch_processing_successful = False
                    
                    # **SQL LOGGING - UPDATE: Batch processing failed**
                    if document_id:
                        log_file_status_update(
                            document_id=document_id,
                            filename=filename,
                            source_path=source_path,
                            target_path=target_path,
                            archive_path=archive_path,
                            status="FAILED",
                            status_desc=f"AI processing failed at batch {batch_start+1}-{batch_end}: {str(batch_error)}",
                            config=config
                        )
                    break
            
            # Check if batch processing was successful
            if not batch_processing_successful:
                unprocessed_files.append(blob_name)
                continue
            
            # Log classification results
            for page_num, category, _ in all_results:
                logger.info(f"Page {page_num+1} classified as: {category}")
            
            # **SQL LOGGING - UPDATE: AI processing completed**
            if document_id:
                log_file_status_update(
                    document_id=document_id,
                    filename=filename,
                    source_path=source_path,
                    target_path=target_path,
                    archive_path=archive_path,
                    status="PROCESSING",
                    status_desc=f"AI processing completed for {len(all_results)} pages, creating output files",
                    config=config
                )
            
            # Create CSV and determine confidence level
            logger.info("Creating CSV from extraction results")
            csv_content, invoice_number, total_amount = pdf_processor.create_csv_for_results(
                all_results, filename
            )
            
            if csv_content:
                # Determine confidence level for folder structure
                is_high_confidence = pdf_processor.has_high_confidence(all_results)
                
                # Determine folder path based on confidence
                if is_high_confidence:
                    folder_path = config["azure_storage"]["high_confidence_folder"]
                    logger.info(f"{filename} has HIGH confidence (≥{config['processing']['confidence_threshold']}%)")
                else:
                    folder_path = config["azure_storage"]["low_confidence_folder"]
                    logger.info(f"{filename} has LOW confidence (<{config['processing']['confidence_threshold']}%)")
                
                # Prepare filenames for upload
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                base_filename = os.path.splitext(filename)[0]
                
                # Upload CSV to blob storage
                csv_blob_name = f"{folder_path}{base_filename}_{invoice_number}_{total_amount}_{timestamp}.csv"
                logger.info(f"Uploading CSV to {csv_blob_name}")
                csv_success, csv_url = storage_helper.upload_to_storage(
                    csv_blob_name,
                    csv_content,
                    "text/csv"
                )
                
                # Upload original PDF to appropriate folder
                source_folder = "source_documents/" + folder_path
                source_blob_name = f"{source_folder}{filename}"
                logger.info(f"Uploading source PDF to {source_blob_name}")
                source_success, source_url = storage_helper.upload_to_storage(
                    source_blob_name,
                    blob_content,
                    "application/pdf"
                )
                
                logger.info(f"CSV upload: {'Success' if csv_success else 'Failed'}")
                logger.info(f"Source PDF upload: {'Success' if source_success else 'Failed'}")
                
                # Update target_path with the actual output location
                target_path = f"CSV: {csv_blob_name}, PDF: {source_blob_name}"
                
                # Store target path for archive updates
                file_target_paths[blob_name] = target_path
                
                if csv_success and source_success:
                    file_processed_successfully = True
                    processed_files.append(blob_name)
                    
                    # **SQL LOGGING - UPDATE: Successfully completed**
                    if document_id:
                        confidence_level = "HIGH" if is_high_confidence else "LOW"
                        log_file_status_update(
                            document_id=document_id,
                            filename=filename,
                            source_path=source_path,
                            target_path=target_path,
                            archive_path=archive_path,
                            status="COMPLETED",
                            status_desc=f"Processing completed successfully. Confidence: {confidence_level}. CSV: {csv_blob_name}",
                            config=config
                        )
                    
                    if csv_success:
                        logger.info(f"CSV URL: {csv_url}")
                    if source_success:
                        logger.info(f"Source PDF URL: {source_url}")
                else:
                    unprocessed_files.append(blob_name)
                    
                    # **SQL LOGGING - UPDATE: Upload failed**
                    if document_id:
                        log_file_status_update(
                            document_id=document_id,
                            filename=filename,
                            source_path=source_path,
                            target_path=target_path,
                            archive_path=archive_path,
                            status="FAILED",
                            status_desc=f"File upload failed - CSV: {'Success' if csv_success else 'Failed'}, PDF: {'Success' if source_success else 'Failed'}",
                            config=config
                        )
            else:
                logger.warning(f"No extractable content found in {filename}")
                unprocessed_files.append(blob_name)
                
                # **SQL LOGGING - UPDATE: No extractable content**
                if document_id:
                    log_file_status_update(
                        document_id=document_id,
                        filename=filename,
                        source_path=source_path,
                        target_path=target_path,
                        archive_path=archive_path,
                        status="FAILED",
                        status_desc="No extractable content found in PDF after processing",
                        config=config
                    )
        
        except Exception as e:
            logger.error(f"Error processing {blob_name}: {str(e)}", exc_info=True)
            unprocessed_files.append(blob_name)
            
            # **SQL LOGGING - UPDATE: Unexpected error**
            if document_id:
                log_file_status_update(
                    document_id=document_id,
                    filename=filename,
                    source_path=source_path,
                    target_path=target_path,
                    archive_path=archive_path,
                    status="FAILED",
                    status_desc=f"Unexpected error during processing: {str(e)}",
                    config=config
                )
    
    # Handle archiving based on configuration
    blob_input_move_on = config.get("archive", {}).get("blob_input_move_on", False)
    
    if blob_input_move_on:
        logger.info("Starting archiving process...")
        logger.info(f"Files to archive - Processed: {len(processed_files)}, Unprocessed: {len(unprocessed_files)}")
        
        if processed_files or unprocessed_files:
            archive_config = config.get("archive", {})
            success, archive_url = storage_helper.move_files_to_archive(
                processed_files, 
                unprocessed_files, 
                archive_config
            )
            
            if success:
                logger.info(f"Successfully archived all files to: {archive_url}")
                
                # **SQL LOGGING - UPDATE: Update archive paths for all files**
                logger.info("Updating SQL logs with archive paths...")
                
                # Update processed files with archive paths
                for blob_name in processed_files:
                    if blob_name in file_document_ids:
                        document_id = file_document_ids[blob_name]
                        filename = blob_name.split('/')[-1]
                        
                        # Construct archive path (adjust this based on your actual archive structure)
                        archive_timestamp = datetime.now().strftime("%Y%m%d")
                        archive_path = f"archive/processed/{archive_timestamp}/{filename}"
                        
                        # Get actual target_path for this file
                        current_target_path = file_target_paths.get(blob_name, "Processing completed successfully")
                        
                        log_file_status_update(
                            document_id=document_id,
                            filename=filename,
                            source_path=blob_name,
                            target_path=current_target_path,
                            archive_path=archive_path,
                            status="COMPLETED",
                            status_desc=f"Processing completed and archived successfully to {archive_path}",
                            config=config
                        )
                        logger.info(f"Updated archive path for {filename}: {archive_path}")
                
                # Update unprocessed files with archive paths  
                for blob_name in unprocessed_files:
                    if blob_name in file_document_ids:
                        document_id = file_document_ids[blob_name]
                        filename = blob_name.split('/')[-1]
                        
                        # Construct archive path for failed files
                        archive_timestamp = datetime.now().strftime("%Y%m%d")
                        archive_path = f"archive/failed/{archive_timestamp}/{filename}"
                        
                        log_file_status_update(
                            document_id=document_id,
                            filename=filename,
                            source_path=blob_name,
                            target_path="",
                            archive_path=archive_path,
                            status="FAILED",
                            status_desc=f"Processing failed but archived to {archive_path}",
                            config=config
                        )
                        logger.info(f"Updated archive path for failed file {filename}: {archive_path}")
                
            else:
                logger.error("Failed to archive files")
                
                # **SQL LOGGING - UPDATE: Archive failed**
                for blob_name in (processed_files + unprocessed_files):
                    if blob_name in file_document_ids:
                        document_id = file_document_ids[blob_name]
                        filename = blob_name.split('/')[-1]
                        current_status = "COMPLETED" if blob_name in processed_files else "FAILED"
                        
                        log_file_status_update(
                            document_id=document_id,
                            filename=filename,
                            source_path=blob_name,
                            target_path="",
                            archive_path="",
                            status=current_status,
                            status_desc=f"Processing {current_status.lower()} but archiving failed",
                            config=config
                        )
        else:
            logger.info("No files to archive")
    else:
        logger.info("Archiving is disabled (blob_input_move_on = False)")
    
    # Summary
    logger.info("Processing complete!")
    logger.info(f"Summary:")
    logger.info(f"  Total files processed: {len(pdf_blobs)}")
    logger.info(f"  Successfully processed: {len(processed_files)}")
    logger.info(f"  Failed to process: {len(unprocessed_files)}")



---
                  # logger_helpers.py
import pyodbc
import logging

def log_file_status_begin(filename, source_path, target_path, archive_path, status_desc, config):
    """
    Calls the proc to insert BEGIN log and returns the documentProcessorKey
    
    Parameters:
    - filename: Name of the file being processed
    - source_path: Original location of the file
    - target_path: Where the processed file will be stored (usually empty initially)
    - archive_path: Where the file will be archived (usually empty initially)
    - status_desc: Description of the current status
    - config: Configuration dictionary containing SQL server settings
    
    Returns:
    - document_id: The documentProcessorKey for subsequent updates, or None if failed
    """
    try:
        sql_conf = config.get("sql_server", {})
        if not sql_conf or "connection_string" not in sql_conf:
            print(f"[SQL BEGIN ERROR] Missing SQL server configuration")
            return None
            
        conn_str = sql_conf["connection_string"]
        conn = pyodbc.connect(conn_str)
        cur = conn.cursor()
        
        params = [
            None,  # @documentProcessorKey = NULL → INSERT
            filename,
            source_path,
            target_path,
            archive_path,
            'BEGIN',
            status_desc
        ]
        
        result = cur.execute("{CALL dbo.uspXUpdateDocumentProcessor (?, ?, ?, ?, ?, ?, ?)}", params).fetchone()
        key = result[0] if result else None
        
        cur.close()
        conn.close()
        
        return key
        
    except pyodbc.Error as db_error:
        print(f"[SQL BEGIN DB ERROR] {db_error}")
        return None
    except Exception as e:
        print(f"[SQL BEGIN ERROR] {e}")
        return None

def log_file_status_update(document_id, filename, source_path, target_path, archive_path, status, status_desc, config):
    """
    Calls the proc to update the same log row using the key
    
    Parameters:
    - document_id: The documentProcessorKey returned from log_file_status_begin
    - filename: Name of the file being processed
    - source_path: Original location of the file
    - target_path: Where the processed file is stored
    - archive_path: Where the file is archived
    - status: Current status (PROCESSING, COMPLETED, FAILED)
    - status_desc: Description of the current status
    - config: Configuration dictionary containing SQL server settings
    
    Returns:
    - bool: True if successful, False if failed
    """
    try:
        # Validate document_id
        if document_id is None:
            print(f"[SQL UPDATE ERROR] document_id is None, cannot update log")
            return False
            
        sql_conf = config.get("sql_server", {})
        if not sql_conf or "connection_string" not in sql_conf:
            print(f"[SQL UPDATE ERROR] Missing SQL server configuration")
            return False
            
        conn_str = sql_conf["connection_string"]
        conn = pyodbc.connect(conn_str)
        cur = conn.cursor()
        
        update_params = [
            document_id,  # @documentProcessorKey = actual key
            filename,
            source_path,
            target_path,
            archive_path,
            status,
            status_desc
        ]
        
        cur.execute("{CALL dbo.uspXUpdateDocumentProcessor (?, ?, ?, ?, ?, ?, ?)}", update_params)
        conn.commit()
        
        cur.close()
        conn.close()
        
        return True
        
    except pyodbc.Error as db_error:
        print(f"[SQL UPDATE DB ERROR] {db_error}")
        return False
    except Exception as e:
        print(f"[SQL UPDATE ERROR] {e}")
        return False

def log_batch_file_status(file_status_list, config):
    """
    Batch update multiple files at once (useful for archive updates)
    
    Parameters:
    - file_status_list: List of dictionaries with file status info
      [{"document_id": 123, "filename": "file.pdf", "source_path": "...", 
        "target_path": "...", "archive_path": "...", "status": "COMPLETED", 
        "status_desc": "..."}]
    - config: Configuration dictionary
    
    Returns:
    - dict: {"success_count": int, "failed_count": int, "errors": []}
    """
    success_count = 0
    failed_count = 0
    errors = []
    
    for file_status in file_status_list:
        try:
            success = log_file_status_update(
                document_id=file_status["document_id"],
                filename=file_status["filename"],
                source_path=file_status["source_path"],
                target_path=file_status["target_path"],
                archive_path=file_status["archive_path"],
                status=file_status["status"],
                status_desc=file_status["status_desc"],
                config=config
            )
            
            if success:
                success_count += 1
            else:
                failed_count += 1
                errors.append(f"Failed to update {file_status['filename']}")
                
        except Exception as e:
            failed_count += 1
            errors.append(f"Error updating {file_status.get('filename', 'unknown')}: {str(e)}")
    
    return {
        "success_count": success_count,
        "failed_count": failed_count,
        "errors": errors
    }

def test_sql_connection(config):
    """
    Test the SQL connection to ensure it works
    
    Parameters:
    - config: Configuration dictionary
    
    Returns:
    - bool: True if connection successful, False otherwise
    """
    try:
        sql_conf = config.get("sql_server", {})
        if not sql_conf or "connection_string" not in sql_conf:
            print("[SQL TEST ERROR] Missing SQL server configuration")
            return False
            
        conn_str = sql_conf["connection_string"]
        conn = pyodbc.connect(conn_str)
        
        # Test with a simple query
        cur = conn.cursor()
        cur.execute("SELECT 1")
        result = cur.fetchone()
        
        cur.close()
        conn.close()
        
        print("[SQL TEST SUCCESS] Database connection successful")
        return True
        
    except Exception as e:
        print(f"[SQL TEST ERROR] {e}")
        return False

# Optional: Add logging integration
def setup_sql_logger(config):
    """
    Setup a proper logger for SQL operations
    
    Parameters:
    - config: Configuration dictionary
    
    Returns:
    - logging.Logger: Configured logger instance
    """
    logger = logging.getLogger('sql_logger')
    logger.setLevel(logging.INFO)
    
    # Create console handler if no handlers exist
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
    return logger
