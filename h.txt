import os
import json
import base64
import logging
import hashlib
from datetime import datetime
from io import BytesIO
from typing import List, Dict, Any, Tuple

import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential

from azure.storage.blob import BlobServiceClient, ContentSettings
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchableField, SearchField, SearchFieldDataType,  # ADDED SearchField
    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,
    SemanticConfiguration, SemanticField, SemanticPrioritizedFields, SemanticSearch
)
from openai import AzureOpenAI

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)


class ConfigManager:
    def __init__(self, config_path: str = 'config.json'):
        self.config_path = config_path
        self.config = self.load_config()

    def load_config(self) -> Dict[str, Any]:
        with open(self.config_path, 'r') as f:
            cfg = json.load(f)
        return cfg

    def get(self, section: str, key: str = None) -> Any:
        if key:
            return self.config.get(section, {}).get(key)
        return self.config.get(section)


class AzureBlobManager:
    def __init__(self, connection_string: str, input_container: str, output_container: str):
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self.input_container = input_container
        self.output_container = output_container
        self.supported_extensions = {'.pdf', '.doc', '.docx', '.png', '.jpg', '.jpeg', '.tiff', '.tif'}

    def get_providers(self) -> List[str]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs()
        providers = set()
        for blob in blobs:
            parts = blob.name.split('/')
            if parts[0]:
                providers.add(parts[0])
        return sorted(list(providers))

    def get_provider_files(self, provider: str) -> List[Dict[str, str]]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs(name_starts_with=f"{provider}/")
        files = []
        for blob in blobs:
            ext = os.path.splitext(blob.name)[1].lower()
            if ext in self.supported_extensions:
                files.append({
                    'name': blob.name,
                    'filename': os.path.basename(blob.name),
                    'provider': provider,
                    'size': blob.size,
                    'extension': ext
                })
        return files

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def download_blob_as_base64(self, blob_name: str) -> str:
        blob_client = self.blob_service_client.get_blob_client(self.input_container, blob_name)
        data = blob_client.download_blob().readall()
        return base64.b64encode(data).decode('utf-8')

    def upload_to_blob(self, data, blob_path: str, content_type: str = 'text/plain'):
        """Upload data to blob storage"""
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        content_settings = ContentSettings(content_type=content_type)
        if isinstance(data, str):
            data = data.encode('utf-8')
        blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
        logger.info(f"Uploaded to blob: {blob_path}")

    def upload_dataframe_as_csv(self, df: pd.DataFrame, blob_path: str):
        csv_buffer = BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_data = csv_buffer.getvalue()
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        content_settings = ContentSettings(content_type='text/csv')
        blob_client.upload_blob(csv_data, overwrite=True, content_settings=content_settings)
        logger.info(f"Uploaded CSV to blob: {blob_path}")


class DocumentIntelligenceManager:
    def __init__(self, endpoint: str, key: str):
        self.client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def analyze_document(self, base64_data: str, file_extension: str) -> Dict[str, Any]:
        try:
            # Handle .doc and .docx files - convert to PDF first
            if file_extension.lower() in ['.doc', '.docx']:
                logger.info(f"Converting {file_extension} to PDF for OCR processing")
                try:
                    from docx2pdf import convert as docx_to_pdf_convert
                    import tempfile
                    
                    # Decode base64 to bytes
                    document_bytes = base64.b64decode(base64_data)
                    
                    # Create temporary files
                    with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as temp_word:
                        temp_word.write(document_bytes)
                        temp_word_path = temp_word.name
                    
                    temp_pdf_path = temp_word_path.replace(file_extension, '.pdf')
                    
                    # Convert to PDF
                    docx_to_pdf_convert(temp_word_path, temp_pdf_path)
                    
                    # Read PDF and convert back to base64
                    with open(temp_pdf_path, 'rb') as pdf_file:
                        pdf_bytes = pdf_file.read()
                        base64_data = base64.b64encode(pdf_bytes).decode('utf-8')
                    
                    # Clean up temp files
                    os.remove(temp_word_path)
                    os.remove(temp_pdf_path)
                    
                    # Process as PDF
                    file_extension = '.pdf'
                    logger.info(f"Successfully converted to PDF")
                    
                except ImportError:
                    logger.warning("docx2pdf not installed, trying alternative method with python-docx")
                    # Alternative: Extract text directly from Word document
                    try:
                        from docx import Document
                        import io
                        
                        document_bytes = base64.b64decode(base64_data)
                        doc = Document(io.BytesIO(document_bytes))
                        
                        text = ''
                        page_count = 1  # Word doesn't have pages in same way, estimate
                        
                        for para in doc.paragraphs:
                            text += para.text + "\n"
                        
                        # Also extract text from tables
                        for table in doc.tables:
                            for row in table.rows:
                                for cell in row.cells:
                                    text += cell.text + " "
                                text += "\n"
                        
                        logger.info(f"Extracted text from Word document: {len(text)} characters")
                        
                        return {
                            'success': True,
                            'text': text.strip(),
                            'page_count': max(1, len(text) // 3000)  # Estimate pages
                        }
                    except Exception as e:
                        logger.error(f"Failed to extract text from Word document: {e}")
                        return {
                            'success': False,
                            'text': '',
                            'page_count': 0,
                            'error': f'Word document processing failed: {str(e)}'
                        }
            
            # Standard processing for PDF and images
            content_type_map = {
                '.pdf': 'application/pdf',
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.tiff': 'image/tiff',
                '.tif': 'image/tiff',
                '.bmp': 'image/bmp'
            }
            content_type = content_type_map.get(file_extension.lower(), 'application/pdf')
            document_bytes = base64.b64decode(base64_data)
            
            logger.info(f"Starting OCR for {file_extension}, size: {len(document_bytes)} bytes")
            
            poller = self.client.begin_analyze_document(
                model_id="prebuilt-read",
                analyze_request=document_bytes,
                content_type=content_type
            )
            
            result = poller.result()
            text = ''
            page_count = 0
            
            if hasattr(result, 'pages') and result.pages:
                page_count = len(result.pages)
                for page in result.pages:
                    if hasattr(page, 'lines') and page.lines:
                        for line in page.lines:
                            if hasattr(line, 'content'):
                                text += line.content + "\n"
            
            logger.info(f"OCR completed: {page_count} pages, {len(text)} characters")
            
            return {
                'success': True,
                'text': text.strip(),
                'page_count': page_count
            }
            
        except Exception as e:
            logger.error(f"OCR failed: {e}", exc_info=True)
            return {
                'success': False,
                'text': '',
                'page_count': 0,
                'error': str(e)
            }


class AzureOpenAIManager:
    """Manages both GPT extraction and embeddings with separate clients"""
    
    def __init__(self, gpt_endpoint: str, gpt_api_key: str, gpt_api_version: str, gpt_deployment: str,
                 embedding_endpoint: str, embedding_api_key: str, embedding_api_version: str, 
                 embedding_deployment: str, embedding_dimension: int = 3072):
        
        # GPT-4o client for field extraction
        self.gpt_client = AzureOpenAI(
            azure_endpoint=gpt_endpoint,
            api_key=gpt_api_key,
            api_version=gpt_api_version
        )
        self.gpt_deployment = gpt_deployment
        
        # Separate embedding client (important for separate deployment)
        self.embedding_client = AzureOpenAI(
            azure_endpoint=embedding_endpoint,
            api_key=embedding_api_key,
            api_version=embedding_api_version
        )
        self.embedding_deployment = embedding_deployment
        self.embedding_dimension = embedding_dimension
        
        # Token tracking
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def extract_fields(self, text: str, fields: List[str], source_document: str) -> Dict[str, Any]:
        try:
            system_prompt = f"""You are a document extraction expert. Extract the following fields: {', '.join(fields)}.

Return ONLY a JSON object with this structure:
{{
    "field_name": {{"value": "extracted_value", "confidence": 0.95}},
    ...
}}

Be precise with confidence scores."""
            
            user_prompt = f"Document text:\n\n{text[:8000]}"
            
            response = self.gpt_client.chat.completions.create(
                model=self.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                max_tokens=2000
            )
            
            self.total_tokens += response.usage.total_tokens
            self.prompt_tokens += response.usage.prompt_tokens
            self.completion_tokens += response.usage.completion_tokens

            content = response.choices[0].message.content.strip()
            
            # Remove markdown code blocks
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()
            
            data = json.loads(content)
            
            # Normalize data format
            normalized_data = {}
            for field_name in data:
                field_value = data[field_name]
                
                if isinstance(field_value, dict) and 'value' in field_value:
                    normalized_data[field_name] = field_value
                    normalized_data[field_name]['source_document'] = source_document
                else:
                    # Wrap direct values
                    normalized_data[field_name] = {
                        'value': str(field_value),
                        'confidence': 0.5,
                        'source_document': source_document
                    }
            
            logger.info(f"Extracted {len(normalized_data)} fields successfully")
            
            return {
                'success': True,
                'extracted_fields': normalized_data,
                'raw_response': content
            }
            
        except Exception as e:
            logger.error(f"Field extraction failed: {e}", exc_info=True)
            return {
                'success': False,
                'extracted_fields': {},
                'raw_response': '',
                'error': str(e)
            }

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def generate_embeddings(self, text: str) -> List[float]:
        """Generate embeddings using separate embedding deployment"""
        try:
            # Validate input
            if not text or not isinstance(text, str):
                logger.warning("Invalid text for embeddings, using placeholder")
                text = "No content available"
            
            text = str(text).strip()
            
            if len(text) < 3:
                logger.warning("Text too short, using placeholder")
                text = "No content available"
            
            # Truncate if needed
            if len(text) > 30000:
                text = text[:30000]
                logger.info("Text truncated for embedding")
            
            # Call embedding endpoint (separate from GPT)
            response = self.embedding_client.embeddings.create(
                model=self.embedding_deployment,
                input=text
            )
            
            embeddings = response.data[0].embedding
            logger.info(f"Generated embeddings: dimension={len(embeddings)}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}", exc_info=True)
            # Return zero vector as fallback
            logger.warning(f"Returning zero vector of dimension {self.embedding_dimension}")
            return [0.0] * self.embedding_dimension

    def get_token_usage(self) -> Dict[str, int]:
        return {
            'total_tokens': self.total_tokens,
            'prompt_tokens': self.prompt_tokens,
            'completion_tokens': self.completion_tokens
        }

    def calculate_cost(self, costs_config: Dict[str, float]) -> Dict[str, float]:
        input_cost = (self.prompt_tokens / 1000) * costs_config.get('gpt4o_input_per_1k', 0)
        output_cost = (self.completion_tokens / 1000) * costs_config.get('gpt4o_output_per_1k', 0)
        total_cost = input_cost + output_cost
        return {
            'input_cost': input_cost,
            'output_cost': output_cost,
            'total_cost': total_cost
        }


class AzureAISearchManager:
    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.credential = AzureKeyCredential(api_key)
        self.index_client = SearchIndexClient(endpoint=endpoint, credential=self.credential)

    def get_index_name(self, provider: str, timestamp: str = None) -> str:
        """
        Sanitize provider name for Azure AI Search index
        
        If timestamp provided: Returns providername_timestamp (e.g., "anand_20240211_143022")
        If no timestamp: Returns providername only (e.g., "anand")
        
        Args:
            provider: Provider name
            timestamp: Optional timestamp string (YYYYMMDD_HHMMSS)
        
        Returns:
            Sanitized index name
        """
        # Sanitize provider name
        sanitized = provider.lower().replace(' ', '-').replace('_', '-')
        sanitized = ''.join(c for c in sanitized if c.isalnum() or c == '-')
        sanitized = sanitized.strip('-')
        
        # Add timestamp if provided (for unique index per run)
        if timestamp:
            # Replace underscores with hyphens for index name
            clean_timestamp = timestamp.replace('_', '')  # Remove underscores: 20240211143022
            index_name = f"{sanitized}-{clean_timestamp}"
        else:
            index_name = sanitized
        
        return index_name

    def create_index(self, provider: str, embedding_dimension: int = 3072, timestamp: str = None):
        """
        Create search index with vector field and all required fields
        
        Args:
            provider: Provider name
            embedding_dimension: Vector dimension (default 3072)
            timestamp: Optional timestamp for unique index name (YYYYMMDD_HHMMSS)
        """
        index_name = self.get_index_name(provider, timestamp)
        
        logger.info(f"Creating index '{index_name}' with vector dimension {embedding_dimension}")
        
        # Define fields including vector field using SearchField
        fields = [
            SimpleField(name="id", type=SearchFieldDataType.String, key=True),
            SearchableField(name="provider_id", type=SearchFieldDataType.String, filterable=True, sortable=True),
            SearchableField(name="provider", type=SearchFieldDataType.String, filterable=True),
            SearchableField(name="document_name", type=SearchFieldDataType.String, filterable=True),
            SimpleField(name="document_type", type=SearchFieldDataType.String, filterable=True),
            SimpleField(name="file_extension", type=SearchFieldDataType.String, filterable=True),
            SearchableField(name="content", type=SearchFieldDataType.String),
            SimpleField(name="page_count", type=SearchFieldDataType.Int32, filterable=True),
            SimpleField(name="total_documents", type=SearchFieldDataType.Int32, filterable=True),
            SimpleField(name="extraction_datetime", type=SearchFieldDataType.String, sortable=True),
            SearchableField(name="extracted_fields", type=SearchFieldDataType.String),
            SimpleField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
            # MULTIMODAL FIELDS
            SearchableField(name="visual_description", type=SearchFieldDataType.String),
            SimpleField(name="is_multimodal", type=SearchFieldDataType.Boolean, filterable=True),
            SimpleField(name="modality", type=SearchFieldDataType.String, filterable=True),
            # CRITICAL: Use SearchField for vector field
            SearchField(
                name="content_vector",
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                searchable=True,
                vector_search_dimensions=embedding_dimension,
                vector_search_profile_name="vector-profile"
            )
        ]
        
        # Vector search configuration
        vector_search = VectorSearch(
            algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
            profiles=[VectorSearchProfile(name="vector-profile", algorithm_configuration_name="hnsw-config")]
        )
        
        # Create index
        index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
        self.index_client.create_or_update_index(index)
        
        logger.info(f"Index '{index_name}' created successfully with multimodal fields")
        return index_name

    def upload_document(
        self, 
        index_name: str,
        doc_id: str,
        content: str,
        document_name: str,
        provider: str,
        content_vector: List[float],
        extracted_fields: Any = None,
        page_count: int = 1
    ):
        """
        Upload a single document to search index
        
        Args:
            index_name: Index name to upload to
            doc_id: Unique document ID
            content: Document text content
            document_name: Original document filename
            provider: Provider name
            content_vector: Embedding vector
            extracted_fields: Extracted fields (dict or JSON string)
            page_count: Number of pages
        """
        # Convert extracted_fields to string if dict
        if isinstance(extracted_fields, dict):
            extracted_fields_str = json.dumps(extracted_fields)
        else:
            extracted_fields_str = str(extracted_fields) if extracted_fields else '{}'
        
        # Create document
        document = {
            'id': doc_id,
            'content': content,
            'content_vector': content_vector,
            'document_name': document_name,
            'provider': provider,
            'extracted_fields': extracted_fields_str,
            'page_count': page_count
        }
        
        try:
            client = SearchClient(
                endpoint=self.endpoint,
                index_name=index_name,
                credential=self.credential
            )
            
            result = client.upload_documents(documents=[document])
            
            if result and result[0].succeeded:
                logger.info(f"Uploaded document {doc_id} to index {index_name}")
            else:
                error_msg = result[0].error_message if result else "Unknown error"
                logger.error(f"Failed to upload document {doc_id}: {error_msg}")
                
        except Exception as e:
            logger.error(f"Document upload failed for {doc_id}: {e}")
            raise
    
    def upload_documents(self, provider: str, documents: List[Dict[str, Any]], timestamp: str = None):
        """
        Upload documents to search index
        
        Args:
            provider: Provider name
            documents: List of documents to upload
            timestamp: Optional timestamp to match index name
        """
        if not documents:
            logger.warning("No documents to upload")
            return
        
        index_name = self.get_index_name(provider, timestamp)
        
        logger.info(f"Uploading {len(documents)} documents to index '{index_name}'")
        
        try:
            client = SearchClient(
                endpoint=self.endpoint,
                index_name=index_name,
                credential=self.credential
            )
            
            result = client.upload_documents(documents=documents)
            
            # Check results
            success_count = sum(1 for r in result if r.succeeded)
            logger.info(f"Uploaded {success_count}/{len(documents)} documents successfully")
            
            if success_count < len(documents):
                failed = [r for r in result if not r.succeeded]
                for fail in failed:
                    logger.error(f"Failed to upload document: {fail.key} - {fail.error_message}")
            
        except Exception as e:
            logger.error(f"Document upload failed: {e}", exc_info=True)
            raise
