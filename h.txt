import os
from openai import AzureOpenAI
import json
import base64
import time
from pathlib import Path
import logging
from datetime import datetime, timedelta

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("azure_batch_processing.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Set your Azure OpenAI credentials
api_key = os.environ.get("AZURE_OPENAI_KEY")
if not api_key:
    raise ValueError("AZURE_OPENAI_KEY environment variable is not set")

azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
if not azure_endpoint:
    raise ValueError("AZURE_OPENAI_ENDPOINT environment variable is not set")

deployment_name = "your-gpt-4o-vision-batch-deployment-name"  # IMPORTANT: Use your gpt-4o Global-Batch deployment name

# Input/output directories
INPUT_DIR = "input"
OUTPUT_DIR = "output"

# Ensure output directory exists
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Initialize Azure OpenAI client
try:
    client = AzureOpenAI(
        api_version="2024-02-15-preview",  # Or the latest API version supporting gpt-4o vision batch
        api_key=api_key,
        azure_endpoint=azure_endpoint
    )
except Exception as e:
    logger.error(f"Failed to initialize Azure OpenAI client: {e}")
    raise

def pdf_to_base64(pdf_path):
    """Converts a PDF file to its Base64 encoded string."""
    try:
        with open(pdf_path, "rb") as pdf_file:
            encoded_string = base64.b64encode(pdf_file.read()).decode("utf-8")
        return encoded_string
    except Exception as e:
        logger.error(f"Error encoding PDF {pdf_path}: {e}")
        return None

def prepare_batch_input():
    """Prepare batch input data from PDFs in the input directory."""
    inputs_data = []
    processed_files = []
    
    # Check if input directory exists
    if not os.path.exists(INPUT_DIR):
        logger.error(f"Input directory '{INPUT_DIR}' does not exist")
        return inputs_data, processed_files
    
    # Get all PDF files in the input directory
    pdf_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith('.pdf')]
    
    if not pdf_files:
        logger.warning(f"No PDF files found in {INPUT_DIR}")
        return inputs_data, processed_files
    
    logger.info(f"Found {len(pdf_files)} PDF files to process")
    
    for i, pdf_file in enumerate(pdf_files):
        pdf_path = os.path.join(INPUT_DIR, pdf_file)
        processed_files.append(pdf_path)
        
        # Encode PDF to base64
        base64_pdf = pdf_to_base64(pdf_path)
        
        if base64_pdf:
            inputs_data.append(
                {
                    "custom_id": f"pdf_{i+1}_{pdf_file}",
                    "body": {
                        "messages": [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "image_url", "image_url": {"url": f"data:application/pdf;base64,{base64_pdf}"}},
                                    {"type": "text", "text": "Please analyze this PDF document and extract the main points, key data, and provide a detailed summary."}
                                ]
                            }
                        ],
                        "model": deployment_name
                    }
                }
            )
            logger.info(f"Added {pdf_file} to batch input")
        else:
            logger.warning(f"Skipping {pdf_file} due to encoding error")
    
    return inputs_data, processed_files

def main():
    try:
        # Start timing
        start_time = datetime.now()
        logger.info(f"Batch processing started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. Prepare your input data with Base64 encoded PDFs
        logger.info("Preparing batch input data...")
        inputs_data, processed_files = prepare_batch_input()
        
        if not inputs_data:
            logger.error("No valid PDF data to process. Exiting.")
            return
        
        # 2. Create the .jsonl file
        input_file_path = os.path.join(OUTPUT_DIR, "gpt4o_vision_batch_input_base64.jsonl")
        try:
            with open(input_file_path, "w") as f:
                for item in inputs_data:
                    f.write(json.dumps(item) + "\n")
            logger.info(f"Created input file at {input_file_path}")
        except Exception as e:
            logger.error(f"Failed to create input file: {e}")
            return
        
        # 3. Upload the input file to Azure OpenAI
        try:
            with open(input_file_path, "rb") as f:
                upload_response = client.files.create(file=f, purpose="batch")
            input_file_id = upload_response.id
            logger.info(f"Uploaded file ID: {input_file_id}")
        except Exception as e:
            logger.error(f"Failed to upload input file: {e}")
            return
        
        # 4. Submit the batch job (using /chat/completions for gpt-4o)
        try:
            batch_response = client.batches.create(
                input_file_id=input_file_id,
                endpoint="/chat/completions",  # gpt-4o uses the chat completions endpoint
                completion_window="24h"
            )
            batch_id = batch_response.id
            logger.info(f"Batch job ID: {batch_id}")
        except Exception as e:
            logger.error(f"Failed to submit batch job: {e}")
            return
        
        # 5. Monitor the batch job status (polling)
        status = "running"
        retry_count = 0
        max_retries = 5
        
        logger.info("Monitoring batch job status...")
        while status not in ("completed", "failed", "canceled"):
            try:
                time.sleep(60)  # Check every minute
                retrieved_batch = client.batches.retrieve(batch_id)
                status = retrieved_batch.status
                logger.info(f"Batch job status: {status}")
                
                # Print progress information if available
                if hasattr(retrieved_batch, 'usage') and retrieved_batch.usage:
                    logger.info(f"Progress: {retrieved_batch.usage}")
                
                if retrieved_batch.error:
                    logger.error(f"Batch job error: {retrieved_batch.error}")
                    break
                
                retry_count = 0  # Reset retry count on successful check
            except Exception as e:
                retry_count += 1
                logger.warning(f"Error checking status (attempt {retry_count}/{max_retries}): {e}")
                if retry_count >= max_retries:
                    logger.error("Max retries reached. Stopping status monitoring.")
                    status = "error"
                    break
        
        # 6. Retrieve and save the results
        if status == "completed" and batch_response.output_file_id:
            try:
                logger.info(f"Downloading results from file ID: {batch_response.output_file_id}")
                output_file = client.files.content(batch_response.output_file_id)
                
                # Save raw results
                raw_results_path = os.path.join(OUTPUT_DIR, f"batch_results_{batch_id}.jsonl")
                with open(raw_results_path, "w", encoding="utf-8") as f:
                    f.write(output_file.text)
                logger.info(f"Raw results saved to {raw_results_path}")
                
                # Process individual results
                results = output_file.text.strip().split('\n')
                logger.info(f"Processing {len(results)} result items")
                
                for result_json in results:
                    result = json.loads(result_json)
                    custom_id = result.get("custom_id", "unknown")
                    
                    # Extract PDF filename from custom_id (if format is pdf_N_filename.pdf)
                    pdf_filename = custom_id.split("_", 2)[2] if len(custom_id.split("_")) > 2 else f"{custom_id}.pdf"
                    
                    # Save the content to a separate file
                    if "body" in result and "choices" in result["body"]:
                        content = result["body"]["choices"][0]["message"]["content"]
                        output_path = os.path.join(OUTPUT_DIR, f"{os.path.splitext(pdf_filename)[0]}_analysis.txt")
                        
                        with open(output_path, "w", encoding="utf-8") as f:
                            f.write(content)
                        logger.info(f"Saved analysis for {pdf_filename} to {output_path}")
            except Exception as e:
                logger.error(f"Error retrieving or processing results: {e}")
        elif status in ("failed", "canceled"):
            logger.error("Batch job failed or was canceled.")
        else:
            logger.error(f"Unexpected status: {status}")
        
        # Calculate and log the total duration
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info(f"Batch processing completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"Total processing time: {duration}")
        
        # Format duration in a more readable way
        days = duration.days
        hours, remainder = divmod(duration.seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        duration_str = f"{days} days, {hours} hours, {minutes} minutes, {seconds} seconds" if days else f"{hours} hours, {minutes} minutes, {seconds} seconds"
        
        # Log the summary
        logger.info("=== PROCESSING SUMMARY ===")
        logger.info(f"Files processed: {len(processed_files)}")
        logger.info(f"Total duration: {duration_str}")
        logger.info("=========================")
        
        # Save summary to a file
        summary_path = os.path.join(OUTPUT_DIR, f"batch_summary_{batch_id}.txt")
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("=== AZURE GPT-4o VISION BATCH PROCESSING SUMMARY ===\n")
            f.write(f"Batch ID: {batch_id}\n")
            f.write(f"Started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Completed: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Duration: {duration_str}\n")
            f.write(f"Files processed: {len(processed_files)}\n")
            f.write(f"Status: {status}\n")
        
        logger.info(f"Processing summary saved to {summary_path}")
        
    except Exception as e:
        logger.error(f"Unexpected error in main process: {e}", exc_info=True)

if __name__ == "__main__":
    main()
