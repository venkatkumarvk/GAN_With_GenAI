# logger_helpers.py

import pandas as pd
import pyodbc
from pathlib import Path

LOG_CSV_PATH = Path("logs/process_log.csv")

def log_file_status(filename, source_path, target_path, archive_path, status, status_desc="", config=None):
    log_entry = {
        "FileName": filename,
        "SourceFilePath": source_path,
        "TargetFilePath": target_path,
        "ArchiveFilePath": archive_path,
        "Status": status,
        "StatusDesc": status_desc
    }

    log_config = config.get("logging", {}) if config else {}
    store_csv = log_config.get("store_csv", True)
    store_sql = log_config.get("store_sql", False)

    # --- CSV logging ---
    if store_csv:
        try:
            row_df = pd.DataFrame([log_entry])
            LOG_CSV_PATH.parent.mkdir(parents=True, exist_ok=True)
            if not LOG_CSV_PATH.exists():
                row_df.to_csv(LOG_CSV_PATH, index=False)
            else:
                row_df.to_csv(LOG_CSV_PATH, mode='a', header=False, index=False)
        except Exception as e:
            print(f"[CSV LOGGING ERROR] {e}")

    # --- SQL Server logging ---
    if store_sql:
        try:
            sql_conf = config.get("sql_server", {})
            conn_str = sql_conf["connection_string"]
            table_name = sql_conf["table_name"]

            conn = pyodbc.connect(conn_str)
            cursor = conn.cursor()
            cursor.execute(f"""
                INSERT INTO {table_name}
                (FileName, SourceFilePath, TargetFilePath, ArchiveFilePath, Status, StatusDesc)
                VALUES (?, ?, ?, ?, ?, ?)
            """, filename, source_path, target_path, archive_path, status, status_desc)
            conn.commit()
            cursor.close()
            conn.close()
        except Exception as e:
            print(f"[SQL LOGGING ERROR] {e}")


----------------------

                  log_summary = []  # Collect logs per file
log_entry = {
    "filename": blob_name.split("/")[-1],
    "source_path": blob_name,
    "target_path": "N/A",
    "archive_path": "N/A",
    "status": "",
    "status_desc": ""
}

log_entry["target_path"] = csv_blob_name
log_entry["status"] = "COMPLETE"
log_entry["status_desc"] = "Successfully processed and uploaded"

log_entry["status"] = "FAILURE"
log_entry["status_desc"] = "Reason for failure"

log_summary.append(log_entry)


for entry in log_summary:
    entry["archive_path"] = archive_url if success else "N/A"
    log_file_status(
        filename=entry["filename"],
        source_path=entry["source_path"],
        target_path=entry["target_path"],
        archive_path=entry["archive_path"],
        status=entry["status"],
        status_desc=entry["status_desc"],
        config=config
    )
