"""
TEXT RAG - Text-Only Document Extraction
=========================================

OCR text is PRIMARY source.
Uses Azure AI Search vector search to find similar past documents.
Passes them as few-shot examples to GPT-4o for extraction.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import json
import logging
from typing import List, Dict, Any, Optional

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class TextRAGExtractor:
    """
    Text-Only RAG Extractor

    Flow:
        Document â†’ OCR (primary) â†’ Embed â†’ Search Similar â†’ GPT-4o Extract

    Mode: Fast, cost-effective, 92-95% accuracy
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(f"TextRAGExtractor ready | top_k={top_k} | threshold={similarity_threshold}")
        print(f"    âœ“ RAG Mode : TEXT-ONLY")
        print(f"    âœ“ Vision   : Disabled")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PUBLIC: Extract with RAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def extract_with_rag(
        self,
        document_text: str,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Text RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, embeddings
            }
        """
        logger.info(f"Text RAG extraction | doc={source_document} | provider={provider}")

        # Step 1 â€” Generate text embedding (OCR is primary)
        embedding = self.openai_manager.generate_embeddings(document_text)
        logger.info("  âœ“ Text embedding generated")

        # Step 2 â€” Search similar documents
        similar_docs = self._search_similar(embedding, provider, index_name)

        # Step 3 â€” Build prompt
        if similar_docs:
            print(f"    âœ“ RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  âœ“ {len(similar_docs)} similar docs retrieved")
            # Use prompt_builder to build RAG prompt
            user_prompt = self.prompt_builder.build_rag_user_prompt(
                document_text=document_text,
                visual_description="",
                similar_examples=similar_docs
            )
        else:
            print(f"    âš  RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  âš  No similar docs â€” standard extraction")
            # Use prompt_builder to build standard prompt
            user_prompt = self.prompt_builder.build_rag_user_prompt(
                document_text=document_text,
                visual_description="",
                similar_examples=None
            )

        # Step 4 â€” GPT-4o extraction
        extracted_fields, system_prompt = self._call_gpt(user_prompt, document_type)
        
        # Step 5 â€” Store in Azure AI Search for future RAG
        confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        self._store_in_index(
            index_name=index_name,
            document_name=source_document,
            provider=provider,
            document_text=document_text,
            text_embedding=embedding,  # Fixed: was text_emb
            extracted_fields=extracted_fields,
            avg_confidence=avg_confidence
        )

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':        False,
            'visual_description': '',
            'system_prompt':     system_prompt,  # Include system prompt
            'user_prompt':       user_prompt,     # Include user prompt
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding   # combined = text in text-only mode
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        document_type: Optional[str] = None,
        source_document: str = '',
        provider: str = '',
        index_name: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG â€” used for first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            document_type:   Document type hint
            source_document: Source filename
            provider:        Provider name (for storage)
            index_name:      Index name (for storage)

        Returns:
            Same shape as extract_with_rag
        """
        logger.info(f"Text extraction (no RAG) | doc={source_document}")

        embedding   = self.openai_manager.generate_embeddings(document_text)
        # Use prompt_builder to build prompt
        user_prompt = self.prompt_builder.build_rag_user_prompt(
            document_text=document_text,
            visual_description="",
            similar_examples=None
        )
        extracted_fields, system_prompt = self._call_gpt(user_prompt, document_type)
        
        # Store in index for future RAG (if provider/index provided)
        if provider and index_name:
            confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            self._store_in_index(
                index_name=index_name,
                document_name=source_document,
                provider=provider,
                document_text=document_text,
                text_embedding=embedding,
                extracted_fields=extracted_fields,
                avg_confidence=avg_confidence
            )

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          False,
            'similar_docs_count': 0,
            'has_vision':        False,
            'visual_description': '',
            'system_prompt':     system_prompt,  # Include system prompt
            'user_prompt':       user_prompt,     # Include user prompt
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding
            }
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PRIVATE HELPERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _store_in_index(
        self,
        index_name: str,
        document_name: str,
        provider: str,
        document_text: str,
        text_embedding: List[float],
        extracted_fields: Dict[str, Any],
        avg_confidence: float
    ):
        """
        Store document in Azure AI Search index for future RAG
        """
        try:
            from azure.search.documents.indexes import SearchIndexClient
            from azure.search.documents.indexes.models import (
                SearchIndex, SearchField, SearchFieldDataType,
                VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile
            )
            
            # Create index if not exists
            index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=self.search_credential
            )
            
            try:
                index_client.get_index(index_name)
                logger.info(f"  Index '{index_name}' exists")
            except:
                logger.info(f"  Creating index '{index_name}'...")
                
                fields = [
                    SearchField(name="id", type=SearchFieldDataType.String, key=True, filterable=True),
                    SearchField(name="document_name", type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SearchField(name="provider", type=SearchFieldDataType.String, filterable=True),
                    SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="extracted_fields", type=SearchFieldDataType.String),
                    SearchField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
                    SearchField(
                        name="content_vector",
                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        vector_search_dimensions=3072,
                        vector_search_profile_name="text-profile"
                    )
                ]
                
                vector_search = VectorSearch(
                    algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
                    profiles=[VectorSearchProfile(name="text-profile", algorithm_configuration_name="hnsw-config")]
                )
                
                index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
                index_client.create_index(index)
                logger.info(f"  âœ“ Index '{index_name}' created")
            
            # Upload document
            doc_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            # Generate document ID for Azure AI Search
            # LONG-TERM SOLUTION: Use hash of original filename
            # This way:
            # 1. Original filename preserved in metadata
            # 2. Document ID is always valid (hash is alphanumeric)
            # 3. No future errors from new special characters
            # 4. Consistent and deterministic
            
            import hashlib
            
            # Create hash from provider + document name (deterministic)
            unique_string = f"{provider}_{document_name}"
            hash_value = hashlib.sha256(unique_string.encode()).hexdigest()[:32]  # 32 chars
            
            # Document ID: hash only (always valid!)
            doc_id = f"{provider}_{hash_value}"
            
            # Store original filename separately in document metadata
            # This is returned in search results and stored in JSON
            
            document = {
                "id": doc_id,
                "document_name": document_name,
                "provider": provider,
                "content": document_text[:50000],
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": avg_confidence,
                "content_vector": text_embedding
            }
            
            doc_client.upload_documents([document])
            logger.info(f"  âœ“ Stored in index: {doc_id}")
            
        except Exception as e:
            logger.error(f"Failed to store in index: {e}", exc_info=True)

    def _search_similar(
        self,
        embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search for similar documents"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=["document_name", "content", "extracted_fields", "avg_confidence"]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _build_rag_prompt(
        self,
        document_text: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build user prompt with RAG few-shot examples"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass
            parts.append("")

        parts.append("â”€" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")
        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:8000])
        parts.append("")
        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(self, document_text: str) -> str:
        """Build standard prompt without RAG context"""
        return (
            f"Extract the following fields from this document:\n\n"
            f"OCR TEXT (Primary Source):\n{document_text[:8000]}\n\n"
            f"Fields to extract: {', '.join(self.fields)}\n\n"
            "Return ONLY a JSON object with field values and confidence scores."
        )

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> tuple[Dict[str, Any], str]:
        """Call GPT-4o and return parsed extraction result + system prompt"""
        try:
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            # Track tokens
            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content), system_prompt

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}, ""

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}


---

multimodal

"""
MULTIMODAL RAG - Vision + Text Document Extraction
====================================================

OCR text is PRIMARY source.
Vision (GPT-4 Vision) is SUPPLEMENTARY enhancement.
Combined embedding (text + visual) improves RAG similarity search.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import base64
import json
import logging
from typing import Dict, Any, List, Optional, Tuple

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class MultimodalRAGExtractor:
    """
    Multimodal RAG Extractor â€” Text + Vision

    Flow:
        Document â†’ OCR (primary)       â†’ Text Embed   â”€â”
                 â†’ Vision (supplement) â†’ Visual Embed â”€â”¤ â†’ Combined â†’ Search â†’ GPT-4o
                                                       â”€â”˜

    Mode: Higher accuracy (95-97%), 100% coverage (handles image-only docs)
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        text_weight: float = 0.7,
        visual_weight: float = 0.3
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
            text_weight:          Weight for text embedding in combined vector (default 0.7)
            visual_weight:        Weight for visual embedding in combined vector (default 0.3)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold
        self.text_weight          = text_weight
        self.visual_weight        = visual_weight

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(
            f"MultimodalRAGExtractor ready | "
            f"top_k={top_k} | text_w={text_weight} | visual_w={visual_weight}"
        )
        print(f"    âœ“ RAG Mode : MULTIMODAL (Text + Vision)")
        print(f"    âœ“ Vision   : Enabled")
        print(f"    âœ“ Weights  : text={text_weight} | visual={visual_weight}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PUBLIC: Extract with RAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def extract_with_rag(
        self,
        document_text: str,
        image_bytes: bytes,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Multimodal RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            image_bytes:     Raw image bytes for vision analysis
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, has_vision,
              visual_description, embeddings
            }
        """
        logger.info(f"Multimodal RAG | doc={source_document} | provider={provider}")

        # Step 1 â€” Vision analysis (supplementary)
        visual_description = ''
        if image_bytes:
            print(f"    ðŸ” Vision analysis (supplementary)...")
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']
                print(f"    âœ“ Vision       : {len(visual_description)} chars")
                logger.info(f"  âœ“ Vision complete ({len(visual_description)} chars)")
            else:
                print(f"    âš  Vision failed â€” OCR only (primary)")
                logger.warning(f"  âš  Vision failed: {result.get('error')}")
        else:
            print(f"    âš  No image bytes â€” OCR only (primary)")

        # Step 2 â€” Generate embeddings
        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )
        emb_mode = "text + visual" if visual_description else "text only (vision unavailable)"
        print(f"    âœ“ Embeddings   : {emb_mode}")
        logger.info(f"  âœ“ Embeddings generated: {emb_mode}")

        # Step 3 â€” Search similar documents
        similar_docs = self._search_similar(combined_emb, provider, index_name)

        # Step 4 â€” Build prompt
        if similar_docs:
            print(f"    âœ“ RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  âœ“ {len(similar_docs)} similar docs retrieved")
            user_prompt = self.prompt_builder.build_rag_user_prompt(document_text=document_text, visual_description=visual_description, similar_examples=similar_docs)
        else:
            print(f"    âš  RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  âš  No similar docs â€” standard extraction")
            user_prompt = self.prompt_builder.build_rag_user_prompt(document_text=document_text, visual_description=visual_description, similar_examples=None)

        # Step 5 â€” GPT-4o extraction
        extracted_fields, system_prompt = self._call_gpt(user_prompt, document_type)
        
        # Step 6 â€” Store in Azure AI Search for future RAG
        confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        self._store_in_index(
            index_name=index_name,
            document_name=source_document,
            provider=provider,
            document_text=document_text,
            visual_description=visual_description,
            combined_embedding=combined_emb,
            extracted_fields=extracted_fields,
            avg_confidence=avg_confidence
        )

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'system_prompt':     system_prompt,  # Include system prompt
            'user_prompt':       user_prompt,     # Include user prompt
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        image_bytes: Optional[bytes] = None,
        document_type: Optional[str] = None,
        source_document: str = '',
        provider: str = '',
        index_name: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG â€” first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            image_bytes:     Image bytes for vision (optional)
            document_type:   Document type hint
            source_document: Source filename
            provider:        Provider name (for storage)
            index_name:      Index name (for storage)
        """
        logger.info(f"Multimodal extraction (no RAG) | doc={source_document}")

        visual_description = ''
        if image_bytes:
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']

        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )

        user_prompt = self.prompt_builder.build_rag_user_prompt(document_text=document_text, visual_description=visual_description, similar_examples=None)
        extracted_fields, system_prompt = self._call_gpt(user_prompt, document_type)
        
        # Store in index for future RAG (if provider/index provided)
        if provider and index_name:
            confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            self._store_in_index(
                index_name=index_name,
                document_name=source_document,
                provider=provider,
                document_text=document_text,
                visual_description=visual_description,
                combined_embedding=combined_emb,
                extracted_fields=extracted_fields,
                avg_confidence=avg_confidence
            )

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           False,
            'similar_docs_count': 0,
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'system_prompt':     system_prompt,  # Include system prompt
            'user_prompt':       user_prompt,     # Include user prompt
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PRIVATE HELPERS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _analyze_vision(
        self,
        image_bytes: bytes,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """Analyze document image using GPT-4 Vision"""
        try:
            b64 = base64.b64encode(image_bytes).decode('utf-8')

            type_hints = {
                'passport': "\n\nFocus on: MRZ zone, country emblem, biographical page layout.",
                'license':  "\n\nFocus on: License number position, photo, endorsements.",
                'id_card':  "\n\nFocus on: ID number placement, official seals, front/back layout."
            }
            doc_hint = type_hints.get(document_type, '') if document_type else ''

            prompt = (
                "Analyze this identity document image and describe:\n"
                "1. Document type and layout\n"
                "2. Security features (holograms, watermarks, seals)\n"
                "3. Document condition and image quality\n"
                "4. Photo position and quality\n"
                "5. Any notable visual elements or anomalies\n\n"
                "Keep description concise (max 400 chars) â€” it supplements OCR text."
                + doc_hint
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url":    f"data:image/jpeg;base64,{b64}",
                                "detail": "high"
                            }
                        }
                    ]
                }],
                max_tokens=500,
                temperature=0.0
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return {
                'success':            True,
                'visual_description': response.choices[0].message.content
            }

        except Exception as e:
            logger.error(f"Vision analysis failed: {e}", exc_info=True)
            return {'success': False, 'visual_description': '', 'error': str(e)}

    def _generate_embeddings(
        self,
        document_text: str,
        visual_description: str
    ) -> Tuple[List[float], Optional[List[float]], List[float]]:
        """
        Generate text, visual, and combined embeddings

        Returns:
            (text_embedding, visual_embedding_or_None, combined_embedding)
        """
        text_emb = self.openai_manager.generate_embeddings(document_text)

        if visual_description:
            visual_emb   = self.openai_manager.generate_embeddings(visual_description)
            combined_emb = [
                self.text_weight * text_emb[i] + self.visual_weight * visual_emb[i]
                for i in range(len(text_emb))
            ]
            return text_emb, visual_emb, combined_emb
        else:
            return text_emb, None, text_emb   # fallback: combined = text

    def _search_similar(
        self,
        combined_embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search using combined embedding"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": combined_embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=[
                    "document_name", "content",
                    "extracted_fields", "visual_description", "avg_confidence"
                ]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _store_in_index(
        self,
        index_name: str,
        document_name: str,
        provider: str,
        document_text: str,
        visual_description: str,
        combined_embedding: List[float],
        extracted_fields: Dict[str, Any],
        avg_confidence: float
    ):
        """
        Store document in Azure AI Search index for future RAG
        
        Creates index if doesn't exist, then uploads document
        """
        try:
            from azure.search.documents.indexes import SearchIndexClient
            from azure.search.documents.indexes.models import (
                SearchIndex, SearchField, SearchFieldDataType,
                VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile
            )
            
            # Create index if not exists
            index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=self.search_credential
            )
            
            try:
                index_client.get_index(index_name)
                logger.info(f"  Index '{index_name}' exists")
            except:
                logger.info(f"  Creating index '{index_name}'...")
                
                fields = [
                    SearchField(name="id", type=SearchFieldDataType.String, key=True, filterable=True),
                    SearchField(name="document_name", type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SearchField(name="provider", type=SearchFieldDataType.String, filterable=True),
                    SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="visual_description", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="extracted_fields", type=SearchFieldDataType.String),
                    SearchField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
                    SearchField(
                        name="content_vector",
                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        vector_search_dimensions=3072,
                        vector_search_profile_name="multimodal-profile"
                    )
                ]
                
                vector_search = VectorSearch(
                    algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
                    profiles=[VectorSearchProfile(name="multimodal-profile", algorithm_configuration_name="hnsw-config")]
                )
                
                index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
                index_client.create_index(index)
                logger.info(f"  âœ“ Index '{index_name}' created")
            
            # Upload document
            doc_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            # Generate document ID for Azure AI Search
            # LONG-TERM SOLUTION: Use hash of original filename
            # This way:
            # 1. Original filename preserved in metadata
            # 2. Document ID is always valid (hash is alphanumeric)
            # 3. No future errors from new special characters
            # 4. Consistent and deterministic
            
            import hashlib
            
            # Create hash from provider + document name (deterministic)
            unique_string = f"{provider}_{document_name}"
            hash_value = hashlib.sha256(unique_string.encode()).hexdigest()[:32]  # 32 chars
            
            # Document ID: hash only (always valid!)
            doc_id = f"{provider}_{hash_value}"
            
            # Store original filename separately in document metadata
            # This is returned in search results and stored in JSON
            
            document = {
                "id": doc_id,
                "document_name": document_name,
                "provider": provider,
                "content": document_text[:50000],  # Limit size
                "visual_description": visual_description,
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": avg_confidence,
                "content_vector": combined_embedding
            }
            
            doc_client.upload_documents([document])
            logger.info(f"  âœ“ Stored in index: {doc_id}")
            
        except Exception as e:
            logger.error(f"Failed to store in index: {e}", exc_info=True)

    def _build_rag_prompt(
        self,
        document_text: str,
        visual_description: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build prompt with RAG few-shot examples + visual context"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass

            visual_ctx = doc.get('visual_description', '')
            if visual_ctx:
                parts.append(f"Visual   : {visual_ctx[:150]}")

            parts.append("")

        parts.append("â”€" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")

        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:7000])
        parts.append("")

        if visual_description:
            parts.append("VISUAL ANALYSIS (Supplementary):")
            parts.append(visual_description)
            parts.append("")

        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(
        self,
        document_text: str,
        visual_description: str
    ) -> str:
        """Standard prompt (no RAG) with optional visual context"""
        parts = [
            "Extract the following fields from this document:",
            "",
            "OCR TEXT (Primary Source):",
            document_text[:7000],
            ""
        ]
        if visual_description:
            parts += ["VISUAL ANALYSIS (Supplementary):", visual_description, ""]

        parts += [
            f"Fields to extract: {', '.join(self.fields)}",
            "Return ONLY a JSON object with field values and confidence scores."
        ]
        return "\n".join(parts)

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> tuple[Dict[str, Any], str]:
        """Call GPT-4o and return parsed extraction + system prompt"""
        try:
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )
            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content), system_prompt

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}, ""

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}


-----

prompt_builder.pu


"""
PROMPT BUILDER - SYSTEM FILE
=============================

DO NOT EDIT THIS FILE!

This file contains system functions that build prompts automatically.
To add fields, edit prompt.py instead.
"""

import json
from typing import List, Dict, Any, Optional
from prompt import FIELD_LIBRARY, SYSTEM_PROMPT_CONFIG, DOCUMENT_TYPE_HINTS


class ExtractionPromptBuilder:
    """Builds prompts dynamically from configurations in prompt.py"""
    
    def __init__(self, fields: List[str]):
        """
        Initialize with list of fields to extract
        
        Args:
            fields: List of field names (must exist in FIELD_LIBRARY in prompt.py)
        """
        self.fields = fields
        self._validate_fields()
    
    def _validate_fields(self):
        """Validate that all fields have configurations"""
        missing = [f for f in self.fields if f not in FIELD_LIBRARY]
        if missing:
            print(f"âš ï¸  Warning: Missing configurations for fields: {', '.join(missing)}")
            print(f"   Add them to FIELD_LIBRARY in prompt.py")
    
    def build_system_prompt(
        self, 
        document_type: Optional[str] = None,
        ocr_quality: str = "high"
    ) -> str:
        """
        Build system prompt dynamically
        
        Args:
            document_type: Type of document (passport, license, id_card)
            ocr_quality: OCR quality level (low, medium, high)
        
        Returns:
            Complete system prompt string
        """
        parts = []
        
        # Role
        parts.append(SYSTEM_PROMPT_CONFIG['role'])
        parts.append("")
        
        # Task
        field_list = ", ".join(self.fields)
        parts.append(f"Your task is to extract the following fields from documents:")
        parts.append(field_list)
        parts.append("")
        
        # Extraction rules
        parts.append("EXTRACTION RULES:")
        for idx, rule in enumerate(SYSTEM_PROMPT_CONFIG['extraction_rules'], 1):
            parts.append(f"{idx}. {rule}")
        parts.append("")
        
        # Confidence guide
        parts.append("CONFIDENCE SCORING GUIDE:")
        for range_str, description in SYSTEM_PROMPT_CONFIG['confidence_guide'].items():
            parts.append(f"- {range_str}: {description}")
        parts.append("")
        
        # Document type hints
        if document_type and document_type in DOCUMENT_TYPE_HINTS:
            hints = DOCUMENT_TYPE_HINTS[document_type]
            parts.append(hints['title'])
            parts.append("Special considerations:")
            for consideration in hints['special_considerations']:
                parts.append(f"- {consideration}")
            parts.append("")
        
        # OCR quality adjustments
        if ocr_quality == "low":
            parts.append("OCR QUALITY NOTE: This text may contain OCR errors.")
            parts.append("- Be more conservative with confidence scores")
            parts.append("- Look for context clues to verify values")
            parts.append("- Consider common OCR mistakes (0/O, 1/I, 5/S, 8/B)")
            parts.append("")
        
        # Field details
        parts.append("DETAILED FIELD INFORMATION:")
        parts.append("")
        for field in self.fields:
            if field in FIELD_LIBRARY:
                info = FIELD_LIBRARY[field]
                parts.append(f"**{field}**:")
                parts.append(f"  Description: {info['description']}")
                parts.append(f"  Examples: {', '.join(info['examples'])}")
                parts.append(f"  Format: {info['format']}")
                
                if info.get('extraction_hints'):
                    parts.append("  Extraction Hints:")
                    for hint in info['extraction_hints']:
                        parts.append(f"    - {hint}")
                
                parts.append("")
        
        # Output format
        parts.append("OUTPUT FORMAT:")
        parts.append(SYSTEM_PROMPT_CONFIG['output_format'])
        parts.append("")
        parts.append(SYSTEM_PROMPT_CONFIG['restrictions'])
        
        return "\n".join(parts)
    
    def build_rag_user_prompt(
        self,
        document_text: str,
        visual_description: str = "",
        similar_examples: List[Dict[str, Any]] = None
    ) -> str:
        """
        Build user prompt with RAG examples
        
        Args:
            document_text: OCR text from current document
            visual_description: GPT-4o vision analysis (if multimodal)
            similar_examples: List of similar past extractions
        
        Returns:
            User prompt with examples
        """
        
        prompt_parts = []
        
        # Add similar examples if provided
        if similar_examples:
            prompt_parts.append(
                f"Here are {len(similar_examples)} similar documents with their extractions "
                f"as reference examples:\n\n"
            )
            
            for idx, example in enumerate(similar_examples, 1):
                similarity = example.get('@search.score', 0.0)
                doc_name = example.get('document_name', f'Example {idx}')
                extracted = example.get('extracted_fields', '{}')
                
                prompt_parts.append(f"EXAMPLE {idx} (similarity: {similarity:.2f})\n")
                prompt_parts.append(f"Document: {doc_name}\n")
                prompt_parts.append(f"Extraction:\n{extracted}\n\n")
            
            prompt_parts.append("="*60 + "\n\n")
        
        # Current document to extract
        prompt_parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:\n\n")
        
        # OCR text (PRIMARY source)
        prompt_parts.append("OCR TEXT (Primary Source):\n")
        prompt_parts.append(document_text[:15000])  # Limit to 15000 chars (increased for better extraction)
        prompt_parts.append("\n\n")
        
        # Visual description (SUPPLEMENTARY, if multimodal)
        if visual_description:
            prompt_parts.append("VISUAL ANALYSIS (Supplementary):\n")
            prompt_parts.append(visual_description)
            prompt_parts.append("\n\n")
        
        # Remind of output format
        prompt_parts.append(f"Extract these fields: {', '.join(self.fields)}\n")
        prompt_parts.append("Return ONLY the JSON object with field values and confidence scores.\n")
        
        return "".join(prompt_parts)
    
    def get_field_info(self, field_name: str) -> Dict[str, Any]:
        """Get configuration for a specific field"""
        return FIELD_LIBRARY.get(field_name, {})
    
    @staticmethod
    def get_available_fields() -> List[str]:
        """Get list of all available fields"""
        return list(FIELD_LIBRARY.keys())
