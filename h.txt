#document.py

import os
import io
import json
import base64
import shutil
import logging
import traceback
import fitz  # PyMuPDF
from PIL import Image
from datetime import datetime

from helper import (
    load_config, 
    setup_logging
)
from llm import DocumentClassifier

def convert_to_pdf(input_file):
    """
    Convert various file types to PDF
    """
    try:
        # Get file extension
        _, ext = os.path.splitext(input_file)
        ext = ext.lower()

        # PDF files are already in the right format
        if ext == '.pdf':
            return input_file

        # Image files conversion
        if ext in ['.jpg', '.jpeg', '.png']:
            img = Image.open(input_file)
            
            # Convert to RGB if needed
            if img.mode != 'RGB':
                img = img.convert('RGB')
            
            # Resize if very large
            max_size = (2000, 2000)
            img.thumbnail(max_size, Image.LANCZOS)
            
            pdf_path = input_file.replace(ext, '.pdf')
            img.save(pdf_path, 'PDF', resolution=100.0)
            
            return pdf_path

        # For other file types like .doc, .docx, use PyMuPDF
        try:
            doc = fitz.open(input_file)
            pdf_path = input_file.replace(ext, '.pdf')
            doc.save(pdf_path)
            doc.close()
            return pdf_path
        except Exception as conversion_error:
            logging.error(f"Conversion error for {input_file}: {conversion_error}")
            return None

    except Exception as e:
        logging.error(f"PDF conversion error for {input_file}: {e}")
        return None

def extract_page_image(file_path, page_number):
    """
    Extract image for a specific page from various document types
    """
    try:
        # PDF handling
        if file_path.lower().endswith('.pdf'):
            doc = fitz.open(file_path)
            
            # Validate page number
            if page_number < 0 or page_number >= len(doc):
                logging.error(f"Invalid page number {page_number} for {file_path}")
                return _create_blank_image()
            
            page = doc.load_page(page_number)
            pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))
            
            # Validate pixmap
            if not pix or pix.width <= 0 or pix.height <= 0:
                logging.error(f"Invalid page {page_number} in {file_path}")
                return _create_blank_image()
            
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            doc.close()
        
        # Image handling
        elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):
            img = Image.open(file_path)
        
        else:
            logging.error(f"Unsupported file type: {file_path}")
            return _create_blank_image()
        
        # Resize and convert to bytes
        img = img.resize((800, 600), Image.LANCZOS)
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        return buf.getvalue()
    
    except Exception as e:
        logging.error(f"Error extracting page {page_number} from {file_path}: {e}")
        logging.error(traceback.format_exc())
        return _create_blank_image()

def _create_blank_image():
    """
    Create a blank white image for error cases
    """
    img = Image.new('RGB', (800, 600), color='white')
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()

def process_documents(
    source='local', 
    config_path='config.json', 
    output_dir='./output', 
    input_dir='./input', 
    confidence_threshold=0.6
):
    """
    Comprehensive document processing pipeline
    """
    try:
        # Load configuration
        config = load_config(config_path)
        
        # Setup logging
        setup_logging(config)
        
        # Prepare output directories
        source_dir = os.path.join(output_dir, 'source')
        classified_dir = os.path.join(output_dir, 'classified')
        unclassified_dir = os.path.join(output_dir, 'unclassified')
        unprocessed_dir = os.path.join(output_dir, 'unprocessed')
        
        # Create directories
        for directory in [source_dir, classified_dir, unclassified_dir, unprocessed_dir]:
            os.makedirs(directory, exist_ok=True)
        
        # Initialize document classifier
        classifier = DocumentClassifier(config)
        
        # Reference directory
        reference_dir = config['paths']['reference_dir']
        
        # Track processed and unprocessed files
        processed_files = []
        unprocessed_files = []

        # Process each document
        for fname in os.listdir(input_dir):
            try:
                fpath = os.path.join(input_dir, fname)
                
                # Skip directories and hidden files
                if not os.path.isfile(fpath) or fname.startswith('.'):
                    continue

                # Determine file type
                file_type = os.path.splitext(fname)[1].lower()
                
                # Process only supported file types
                if file_type not in ['.pdf', '.jpg', '.jpeg', '.png']:
                    # Move unprocessable files to unprocessed folder
                    unprocessed_path = os.path.join(unprocessed_dir, fname)
                    shutil.copy2(fpath, unprocessed_path)
                    logging.warning(f"Unsupported file type, moved to unprocessed: {fname}")
                    unprocessed_files.append(fpath)
                    continue

                # Convert to PDF
                pdf_path = convert_to_pdf(fpath)
                if not pdf_path:
                    raise ValueError(f"Failed to convert {fname} to PDF")

                # Copy source PDF
                shutil.copy2(pdf_path, os.path.join(source_dir, os.path.basename(pdf_path)))

                # Open PDF
                doc = fitz.open(pdf_path)
                total_pages = len(doc)

                # Track page classifications
                classified_pages = []
                unclassified_pages = []

                # Process each page
                for page_num in range(total_pages):
                    try:
                        # Extract page image
                        page_image = extract_page_image(pdf_path, page_num)
                        
                        # Classify page
                        classification = classifier.classify_document(page_image, reference_dir)
                        
                        # Check classification confidence
                        if (classification['confidence'] >= confidence_threshold 
                            and classification['main_category'] != 'unknown'):
                            classified_pages.append({
                                'page': page_num,
                                'category': classification['main_category'],
                                'subcategory': classification['subcategory'],
                                'confidence': classification['confidence']
                            })
                        else:
                            unclassified_pages.append(page_num)

                    except Exception as page_error:
                        logging.error(f"Error processing page {page_num} in {fname}: {page_error}")
                        unclassified_pages.append(page_num)

                # Close original document
                doc.close()

                # Process classified pages
                if classified_pages:
                    # Group by subcategory
                    subcategory_groups = {}
                    for page in classified_pages:
                        key = (page['category'], page['subcategory'])
                        if key not in subcategory_groups:
                            subcategory_groups[key] = []
                        subcategory_groups[key].append(page['page'])

                    # Create PDFs for each subcategory
                    for (main_cat, sub_cat), pages in subcategory_groups.items():
                        # Convert page numbers to 1-based for filename
                        page_nums_str = '_'.join(str(p+1) for p in pages)
                        
                        # Open original PDF
                        doc = fitz.open(pdf_path)
                        classified_doc = fitz.open()
                        
                        # Add classified pages
                        for page_num in pages:
                            classified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                        
                        # Save classified PDF
                        output_filename = f"{os.path.splitext(fname)[0]}_classified_{page_nums_str}.pdf"
                        category_path = os.path.join(classified_dir, main_cat, sub_cat)
                        os.makedirs(category_path, exist_ok=True)
                        output_path = os.path.join(category_path, output_filename)
                        
                        classified_doc.save(output_path)
                        classified_doc.close()
                        doc.close()

                # Process unclassified pages
                if unclassified_pages:
                    # Convert page numbers to 1-based for filename
                    page_nums_str = '_'.join(str(p+1) for p in unclassified_pages)
                    
                    # Open original PDF
                    doc = fitz.open(pdf_path)
                    unclassified_doc = fitz.open()
                    
                    # Add unclassified pages
                    for page_num in unclassified_pages:
                        unclassified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                    
                    # Save unclassified PDF
                    output_filename = f"{os.path.splitext(fname)[0]}_unclassified_{page_nums_str}.pdf"
                    output_path = os.path.join(unclassified_dir, output_filename)
                    
                    unclassified_doc.save(output_path)
                    unclassified_doc.close()
                    doc.close()

                # Add to processed files
                processed_files.append(pdf_path)

            except Exception as doc_error:
                logging.error(f"Error processing document {fname}: {doc_error}")
                logging.error(traceback.format_exc())
                
                # Move source document to unprocessed folder
                unprocessed_path = os.path.join(unprocessed_dir, fname)
                try:
                    shutil.copy2(fpath, unprocessed_path)
                    logging.info(f"Moved unprocessed document to: {unprocessed_path}")
                except Exception as move_error:
                    logging.error(f"Could not move unprocessed document {fname}: {move_error}")
                
                unprocessed_files.append(fpath)

        # Log processing summary
        logging.info("Processing Summary:")
        logging.info(f"Processed Files: {processed_files}")
        logging.info(f"Unprocessed Files: {unprocessed_files}")

        return processed_files, unprocessed_files

    except Exception as critical_error:
        logging.critical(f"Critical processing error: {critical_error}")
        logging.critical(traceback.format_exc())
        print(f"Document Processing Failed: {critical_error}")
        return [], []


-----


helper.py

import os
import io
import json
import logging
import shutil
import zipfile
from datetime import datetime, timedelta
from azure.storage.blob import BlobServiceClient

def load_config(config_path='config.json'):
    """
    Load configuration from JSON file
    """
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"Error loading configuration: {e}")
        raise

def setup_logging(config):
    """
    Configure logging based on configuration
    """
    # Ensure output directory exists
    output_dir = config['paths']['output_dir']
    log_dir = os.path.join(output_dir, 'logs')
    os.makedirs(log_dir, exist_ok=True)

    # Create timestamped log filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f'document_classification_{timestamp}.log')

    # Configure logging
    logging.basicConfig(
        level=getattr(logging, config['logging']['level'].upper()),
        format='%(asctime)s - %(levelname)s: %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='a'),
            logging.StreamHandler()
        ]
    )

    # Log rotation and cleanup
    _cleanup_old_logs(log_dir, config['logging'])

    return log_file

def _cleanup_old_logs(log_dir, log_config):
    """
    Clean up old log files based on configuration
    """
    try:
        # Get all log files
        log_files = [
            os.path.join(log_dir, f) for f in os.listdir(log_dir) 
            if f.startswith('document_classification_') and f.endswith('.log')
        ]

        # Sort logs by creation time
        log_files.sort(key=os.path.getctime, reverse=True)

        # Remove excess log files
        if len(log_files) > log_config.get('max_log_files', 10):
            for log_file in log_files[log_config.get('max_log_files', 10):]:
                os.remove(log_file)

        # Remove logs older than specified retention days
        retention_days = log_config.get('log_retention_days', 30)
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        for log_file in log_files:
            file_created = datetime.fromtimestamp(os.path.getctime(log_file))
            if file_created < cutoff_date:
                os.remove(log_file)

    except Exception as e:
        logging.warning(f"Error during log cleanup: {e}")

class AzureStorageManager:
    def __init__(self, config):
        """
        Initialize Azure Storage client
        """
        self.config = config
        self.blob_service_client = BlobServiceClient.from_connection_string(
            config['azure_storage']['connection_string']
        )

    def list_blobs(self, container_name):
        """
        List all blobs in a container
        """
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            return [blob.name for blob in container_client.list_blobs()]
        except Exception as e:
            logging.error(f"Error listing blobs in {container_name}: {e}")
            return []

    def download_blob(self, container_name, blob_name, local_path):
        """
        Download a blob to a local file
        """
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container_name, 
                blob=blob_name
            )
            with open(local_path, "wb") as file:
                blob_data = blob_client.download_blob()
                blob_data.readinto(file)
            return True
        except Exception as e:
            logging.error(f"Error downloading {blob_name}: {e}")
            return False

    def upload_blob(self, container_name, blob_name, local_path):
        """
        Upload a local file to blob storage
        """
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container_name, 
                blob=blob_name
            )
            with open(local_path, "rb") as data:
                blob_client.upload_blob(data, overwrite=True)
            return True
        except Exception as e:
            logging.error(f"Error uploading {blob_name}: {e}")
            return False

    def prepare_reference_documents(self):
        """
        Download reference documents from Azure Blob Storage
        """
        try:
            # Get reference container name
            reference_container = self.config['azure_storage']['reference_container']
            
            # Create local reference directory
            reference_dir = self.config['paths']['reference_dir']
            os.makedirs(reference_dir, exist_ok=True)
            
            # List blobs in reference container
            reference_blobs = self.list_blobs(reference_container)
            
            # Download and organize reference documents
            for blob_name in reference_blobs:
                # Determine category and subcategory from blob path
                parts = blob_name.split('/')
                
                # Ensure we have at least main category and subcategory
                if len(parts) < 2:
                    logging.warning(f"Skipping blob {blob_name}: Incorrect path structure")
                    continue
                
                main_category = parts[-2]
                subcategory = parts[-1]
                
                # Create local category and subcategory directories
                local_category_path = os.path.join(reference_dir, main_category)
                local_subcategory_path = os.path.join(local_category_path, subcategory)
                
                os.makedirs(local_subcategory_path, exist_ok=True)
                
                # Download blob
                local_file_path = os.path.join(local_subcategory_path, os.path.basename(blob_name))
                
                if self.download_blob(reference_container, blob_name, local_file_path):
                    logging.info(f"Downloaded reference document: {blob_name}")
                else:
                    logging.warning(f"Failed to download reference document: {blob_name}")
            
            logging.info("Reference document preparation completed")
            return True
        
        except Exception as e:
            logging.error(f"Error preparing reference documents: {e}")
            return False

    def create_archive(self, processed_files, unprocessed_files):
        """
        Create a ZIP archive of processed and unprocessed files
        """
        try:
            # Create a BytesIO object to store the ZIP
            archive_buffer = io.BytesIO()
            
            with zipfile.ZipFile(archive_buffer, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Add processed files
                for file_path in processed_files:
                    filename = os.path.basename(file_path)
                    zipf.write(file_path, arcname=f"processed/{filename}")
                
                # Add unprocessed files
                for file_path in unprocessed_files:
                    filename = os.path.basename(file_path)
                    zipf.write(file_path, arcname=f"unprocessed/{filename}")
            
            # Prepare archive name
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = f"document_archive_{timestamp}.zip"
            
            # Upload archive to storage
            archive_buffer.seek(0)
            archive_container = self.config['azure_storage']['archive_container']
            
            blob_client = self.blob_service_client.get_blob_client(
                container=archive_container, 
                blob=archive_name
            )
            blob_client.upload_blob(archive_buffer.getvalue())
            
            return archive_name
        
        except Exception as e:
            logging.error(f"Error creating archive: {e}")
            return None

    def delete_multiple_blobs(self, blob_names, container_name):
        """
        Delete multiple blobs from a container
        """
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            
            for blob_name in blob_names:
                blob_client = container_client.get_blob_client(blob_name)
                blob_client.delete_blob()
                logging.info(f"Deleted blob: {blob_name}")
            
            return True
        except Exception as e:
            logging.error(f"Error deleting blobs: {e}")
            return False

------

main.py

import os
import argparse
import logging
import shutil
from document_process import process_documents
from helper import load_config, setup_logging, AzureStorageManager

def parse_arguments():
    """
    Parse command-line arguments for document processing
    """
    parser = argparse.ArgumentParser(description='Document Classification Pipeline')
    parser.add_argument(
        '--source', 
        choices=['local', 'azure'], 
        default='local',
        help='Source of documents: local filesystem or Azure Blob Storage'
    )
    parser.add_argument(
        '--input', 
        default='./input',
        help='Input folder path or Azure Blob Storage container name'
    )
    parser.add_argument(
        '--output', 
        default='./output', 
        help='Path to output directory'
    )
    parser.add_argument(
        '--config', 
        default='config.json', 
        help='Path to configuration file'
    )
    parser.add_argument(
        '--confidence', 
        type=float, 
        default=0.6, 
        help='Confidence threshold for classification'
    )
    parser.add_argument(
        '--no-archive', 
        action='store_true',
        help='Disable archiving of processed files'
    )
    return parser.parse_args()

def main():
    """
    Main entry point for document processing
    """
    # Parse arguments
    args = parse_arguments()
    
    try:
        # Load configuration
        config = load_config(args.config)
        
        # Setup logging
        setup_logging(config)
        
        # Determine archiving setting
        archive_enabled = config.get('archiving', {}).get('enabled', True)
        if args.no_archive:
            archive_enabled = False
        
        # Prepare output directory
        os.makedirs(args.output, exist_ok=True)
        
        # Process based on source
        if args.source == 'local':
            # Process local documents
            processed_files, unprocessed_files = process_documents(
                source='local',
                config_path=args.config,
                output_dir=args.output,
                input_dir=args.input,
                confidence_threshold=args.confidence
            )
        
        elif args.source == 'azure':
            # Initialize Azure Storage Manager
            storage_manager = AzureStorageManager(config)
            
            # Prepare reference documents
            storage_manager.prepare_reference_documents()
            
            # Create local input directory
            local_input_dir = config['paths']['input_dir']
            os.makedirs(local_input_dir, exist_ok=True)
            
            # Download blobs from input container
            input_container = config['azure_storage']['input_container']
            blobs = storage_manager.list_blobs(input_container)
            
            # Download each blob
            downloaded_files = []
            for blob_name in blobs:
                local_path = os.path.join(local_input_dir, os.path.basename(blob_name))
                if storage_manager.download_blob(input_container, blob_name, local_path):
                    downloaded_files.append(local_path)
            
            # Process downloaded documents
            processed_files, unprocessed_files = process_documents(
                source='azure',
                config_path=args.config,
                output_dir=args.output,
                input_dir=local_input_dir,
                confidence_threshold=args.confidence
            )
            
            # Archive if enabled
            if archive_enabled:
                # Create archive with processed and unprocessed files
                archive_name = storage_manager.create_archive(
                    processed_files, 
                    unprocessed_files
                )
                
                if archive_name:
                    logging.info(f"Created archive: {archive_name}")
                
                # Optionally clean input container
                if config.get('archiving', {}).get('clean_input_container', True):
                    # Delete processed blobs
                    processed_blob_names = [
                        os.path.basename(f) for f in processed_files + unprocessed_files
                    ]
                    storage_manager.delete_multiple_blobs(processed_blob_names, input_container)
        
        logging.info("Document processing completed successfully")
    
    except Exception as e:
        logging.critical(f"Critical error in document processing: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()


------

llm.py


import os
import json
import logging
import base64
from openai import AzureOpenAI

class DocumentClassifier:
    def __init__(self, config):
        """
        Initialize Azure OpenAI client for document classification
        """
        self.config = config
        try:
            azure_cfg = config['azure_openai']
            self.client = AzureOpenAI(
                api_key=azure_cfg['api_key'],
                api_version=azure_cfg['api_version'],
                azure_endpoint=azure_cfg['endpoint']
            )
            
            # Store configuration for API call
            self.deployment = azure_cfg['deployment_name']
            self.model_params = {
                'max_tokens': azure_cfg.get('max_tokens', 300),
                'temperature': azure_cfg.get('temperature', 0.2),
                'top_p': azure_cfg.get('top_p', 0.95)
            }
        except Exception as e:
            logging.error(f"Failed to initialize Azure OpenAI client: {e}")
            raise

    def classify_document(self, document_image, reference_dir):
        """
        Comprehensive document classification using references
        """
        try:
            # Preprocess image
            base64_image = self._preprocess_image(document_image)
            
            if not base64_image:
                logging.error("Image preprocessing failed")
                return self._default_classification()

            # Prepare reference details
            reference_details = self._prepare_reference_details(reference_dir)

            # Get available categories from config
            categories = self.config['categories']
            
            # Create comprehensive classification prompt
            prompt = self._create_classification_prompt(
                categories, 
                reference_details
            )

            # Make API call
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {"role": "system", "content": prompt},
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Analyze and classify this document page using the comprehensive guidelines."
                            },
                            {
                                "type": "image",
                                "image_base64": base64_image
                            }
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                **self.model_params
            )

            # Parse response
            result = json.loads(response.choices[0].message.content)

            # Validate and process classification
            classification = self._process_classification(
                result, 
                categories, 
                reference_details
            )

            # Log classification details
            logging.info(f"Classification Result: {json.dumps(classification, indent=2)}")

            return classification

        except Exception as e:
            logging.error(f"Document classification error: {e}")
            return self._default_classification()

    def _prepare_reference_details(self, reference_dir):
        """
        Generate detailed reference document metadata
        """
        reference_details = {}
        
        # Iterate through main categories
        for main_category in os.listdir(reference_dir):
            main_path = os.path.join(reference_dir, main_category)
            if not os.path.isdir(main_path):
                continue
            
            reference_details[main_category] = {}
            
            # Iterate through subcategories
            for subcategory in os.listdir(main_path):
                subcat_path = os.path.join(main_path, subcategory)
                if not os.path.isdir(subcat_path):
                    continue
                
                # Analyze reference documents
                reference_docs = [
                    f for f in os.listdir(subcat_path) 
                    if os.path.isfile(os.path.join(subcat_path, f))
                ]
                
                # Collect detailed metadata
                reference_details[main_category][subcategory] = {
                    'document_count': len(reference_docs),
                    'document_types': list(set(os.path.splitext(doc)[1] for doc in reference_docs)),
                    'document_names': reference_docs
                }
        
        return reference_details

    def _create_classification_prompt(self, categories, reference_details):
        """
        Create a detailed, context-rich classification prompt
        """
        return f"""
        Advanced Document Classification AI System

        COMPREHENSIVE CLASSIFICATION FRAMEWORK:
        - Perform multi-dimensional document analysis
        - Leverage extensive reference document insights
        - Provide precise, confidence-driven categorization

        REFERENCE DOCUMENT LANDSCAPE:
        {json.dumps(reference_details, indent=2)}

        CANONICAL DOCUMENT CATEGORIES:
        {json.dumps(categories, indent=2)}

        CLASSIFICATION PROTOCOL:
        1. Visual Structural Analysis
            - Examine document layout topology
            - Identify characteristic visual markers
            - Cross-reference with reference document patterns

        2. Content Semantic Evaluation
            - Decode implicit and explicit document signals
            - Match against reference document semantic profiles
            - Assess information density and structural coherence

        3. Confidence Calibration
            - Quantify matching precision
            - Factor in reference document statistical insights
            - Dynamically adjust confidence score

        RESPONSE ARCHITECTURE:
        {{
            "main_category": "Primary document domain",
            "subcategory": "Specialized document type",
            "confidence_score": "Precision metric [0.0-1.0]",
            "reasoning": "Comprehensive classification rationale"
        }}

        CRITICAL CLASSIFICATION CONSTRAINTS:
        - Maintain taxonomic integrity
        - Prioritize precision over broad categorization
        - Provide transparent, data-driven reasoning
        """

    def _process_classification(self, result, categories, reference_details):
        """
        Process and validate classification result
        """
        # Extract classification components
        main_category = result.get('main_category', 'unknown').lower()
        subcategory = result.get('subcategory', 'unknown').lower()
        confidence = float(result.get('confidence_score', 0.0))
        reasoning = result.get('reasoning', 'No detailed reasoning')

        # Validate main category
        if main_category not in categories:
            main_category = 'unknown'
            subcategory = 'unknown'
            confidence = 0.0

        # Validate subcategory
        if main_category != 'unknown':
            valid_subcategories = categories.get(main_category, [])
            if subcategory not in valid_subcategories:
                subcategory = 'unknown'
                confidence *= 0.5

        # Reference document impact
        reference_count = reference_details.get(main_category, {}).get(subcategory, {}).get('document_count', 0)
        if reference_count > 0:
            confidence *= 1.2  # Boost for strong reference matching

        # Confidence capping
        confidence = min(confidence, 1.0)

        return {
            'main_category': main_category,
            'subcategory': subcategory,
            'confidence': confidence,
            'reasoning': reasoning,
            'reference_document_count': reference_count
        }

    def _default_classification(self):
        """
        Provide a default classification when processing fails
        """
        return {
            'main_category': 'unknown',
            'subcategory': 'unknown',
            'confidence': 0.0,
            'reasoning': 'Classification process encountered an error',
            'reference_document_count': 0
        }

    def _preprocess_image(self, document_image):
        """
        Preprocess document image for classification
        """
        try:
            from PIL import Image
            import io

            # Open image from bytes
            img = Image.open(io.BytesIO(document_image))
            
            # Convert to RGB if needed
            if img.mode != 'RGB':
                img = img.convert('RGB')
            
            # Resize image
            img = img.resize((800, 600), Image.LANCZOS)
            
            # Compress image
            buffer = io.BytesIO()
            img.save(buffer, format="PNG", optimize=True, quality=85)
            
            # Convert to base64
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        except Exception as e:
            logging.error(f"Image preprocessing error: {e}")
            return None


-----
config.json

{
    "azure_openai": {
        "api_key": "YOUR_AZURE_OPENAI_KEY",
        "endpoint": "https://your-resource-name.openai.azure.com/",
        "deployment_name": "your-gpt-4o-deployment-name",
        "api_version": "2024-02-15-preview",
        "api_type": "azure",
        "model_name": "gpt-4o",
        "max_tokens": 300,
        "temperature": 0.2,
        "top_p": 0.95
    },
    "azure_storage": {
        "connection_string": "YOUR_AZURE_STORAGE_CONNECTION_STRING",
        "input_container": "documentclassificationinput",
        "output_container": "documentclassificationoutput",
        "reference_container": "documentclassificationreference",
        "archive_container": "documentclassificationarchive"
    },
    "paths": {
        "input_dir": "./input",
        "output_dir": "./output",
        "reference_dir": "./reference"
    },
    "categories": {
        "medical": [
            "insurance_claim", 
            "prescription", 
            "medical_report"
        ],
        "financial": [
            "invoice", 
            "bank_statement", 
            "tax_document"
        ],
        "legal": [
            "contract", 
            "agreement", 
            "legal_notice"
        ]
    },
    "classification": {
        "confidence_threshold": 0.6
    },
    "archiving": {
        "enabled": true,
        "clean_input_container": true,
        "archive_name_format": "document_archive_{timestamp}.zip",
        "processed_folder": "processed/",
        "unprocessed_folder": "unprocessed/"
    },
    "logging": {
        "level": "INFO",
        "max_log_files": 10,
        "log_retention_days": 30
    },
    "token_pricing": {
        "gpt-4o": {
            "input": {
                "price_per_million": 2.50,
                "description": "Input token pricing"
            },
            "output": {
                "price_per_million": 10.00,
                "description": "Output token pricing"
            },
            "cached_input": {
                "price_per_million": 1.25,
                "description": "Cached input token pricing"
            }
        }
    }
}


------

def parse_arguments():
    """
    Parse command-line arguments for document processing
    """
    parser = argparse.ArgumentParser(description='Document Classification Pipeline')
    parser.add_argument(
        '--source', 
        choices=['local', 'azure'], 
        default='local',
        help='Source of documents: local filesystem or Azure Blob Storage'
    )
    parser.add_argument(
        '--inputfolder', 
        default='./input',
        help='Input folder path or Azure Blob Storage container/folder'
    )
    # ... rest of existing arguments ...
    return parser.parse_args()

def main():
    # Parse arguments
    args = parse_arguments()
    
    try:
        # Load configuration
        config = load_config(args.config)
        
        # Setup logging
        setup_logging(config)
        
        # Prepare output directory
        os.makedirs(args.output, exist_ok=True)
        
        # Process based on source
        if args.source == 'local':
            # Process local documents
            processed_files, unprocessed_files = process_documents(
                source='local',
                config_path=args.config,
                output_dir=args.output,
                input_dir=args.inputfolder,
                confidence_threshold=args.confidence
            )
        
        elif args.source == 'azure':
            # Initialize Azure Storage Manager
            storage_manager = AzureStorageManager(config)
            
            # Prepare reference documents
            storage_manager.prepare_reference_documents()
            
            # Create local input directory
            local_input_dir = config['paths']['input_dir']
            os.makedirs(local_input_dir, exist_ok=True)
            
            # Parse input folder (container/folder)
            input_parts = args.inputfolder.split('/')
            if len(input_parts) < 2:
                raise ValueError("Input folder must be in format 'container/folder'")
            
            input_container = input_parts[0]
            input_folder = '/'.join(input_parts[1:])
            
            # List blobs in the specific input folder
            blobs = storage_manager.list_blobs(input_container, prefix=input_folder)
            
            # Download each blob
            downloaded_files = []
            for blob_name in blobs:
                local_path = os.path.join(local_input_dir, os.path.basename(blob_name))
                if storage_manager.download_blob(input_container, blob_name, local_path):
                    downloaded_files.append(local_path)
            
            # Process downloaded documents
            processed_files, unprocessed_files = process_documents(
                source='azure',
                config_path=args.config,
                output_dir=args.output,
                input_dir=local_input_dir,
                confidence_threshold=args.confidence
            )
            
            # Archive if enabled
            archive_enabled = config.get('archiving', {}).get('enabled', True)
            if archive_enabled:
                # Create archive with processed and unprocessed files
                archive_name = storage_manager.create_archive(
                    processed_files, 
                    unprocessed_files
                )
                
                if archive_name:
                    logging.info(f"Created archive: {archive_name}")
                
                # Clean input container
                if config.get('archiving', {}).get('clean_input_container', True):
                    # Delete all blobs from input folder
                    storage_manager.delete_multiple_blobs(blobs, input_container)
                    logging.info(f"Cleaned input folder: {args.inputfolder}")
        
        logging.info("Document processing completed successfully")
    
    except Exception as e:
        logging.critical(f"Critical error in document processing: {e}")
        import traceback
        traceback.print_exc()
        raise

# Update in helper.py
def list_blobs(self, container_name, prefix=None):
    """
    List blobs in a container, optionally filtered by prefix
    """
    try:
        container_client = self.blob_service_client.get_container_client(container_name)
        
        # If prefix is provided, list blobs with that prefix
        if prefix:
            blobs = [blob.name for blob in container_client.list_blobs(name_starts_with=prefix)]
        else:
            blobs = [blob.name for blob in container_client.list_blobs()]
        
        return blobs
    except Exception as e:
        logging.error(f"Error listing blobs in {container_name}: {e}")
        return []


----

def sanitize_filename(filename):
    """
    Sanitize filename to remove or replace special characters
    """
    import re
    
    # Remove or replace problematic characters
    sanitized = re.sub(r'[^\w\-_\.]', '_', filename)
    
    # Ensure filename is not empty
    if not sanitized:
        sanitized = 'unnamed_document'
    
    # Truncate very long filenames
    if len(sanitized) > 255:
        sanitized = sanitized[:255]
    
    return sanitized

def process_documents(
    source='local', 
    config_path='config.json', 
    output_dir='./output', 
    input_dir='./input', 
    confidence_threshold=0.6
):
    # ... existing code ...
    
    # When creating output filenames, use sanitize_filename
    output_filename = f"{sanitize_filename(os.path.splitext(fname)[0])}_classified_{page_nums_str}.pdf"
    output_filename = f"{sanitize_filename(os.path.splitext(fname)[0])}_unclassified_{page_nums_str}.pdf"

-----

def delete_multiple_blobs(self, blob_names, container_name):
    """
    Delete multiple blobs from a container
    """
    try:
        container_client = self.blob_service_client.get_container_client(container_name)
        
        # If no blobs provided, attempt to delete all blobs in the container
        if not blob_names:
            blob_names = [blob.name for blob in container_client.list_blobs()]
        
        deleted_count = 0
        total_blobs = len(blob_names)
        
        for blob_name in blob_names:
            try:
                blob_client = container_client.get_blob_client(blob_name)
                blob_client.delete_blob()
                deleted_count += 1
                logging.info(f"Deleted blob: {blob_name}")
            except Exception as blob_error:
                logging.warning(f"Could not delete blob {blob_name}: {blob_error}")
        
        logging.info(f"Successfully deleted {deleted_count}/{total_blobs} blobs from container {container_name}")
        
        return deleted_count == total_blobs
    except Exception as e:
        logging.error(f"Error deleting blobs from container {container_name}: {e}")
        return False
