import os
import json
import time
import tempfile
from io import BytesIO
from datetime import datetime
from openai import AzureOpenAI

class AzureOpenAIClient:
    def __init__(self, config):
        self.api_key = config["azure_openai"]["api_key"]
        self.api_version = config["azure_openai"]["api_version"]
        self.endpoint = config["azure_openai"]["azure_endpoint"]
        self.deployment_name = config["azure_openai"]["deployment_name"]
        self.batch_size = config["processing"]["batch_size"]
        self.timeout = config["processing"]["timeout_seconds"]
        
        self.client = AzureOpenAI(
            api_key=self.api_key,
            api_version=self.api_version,
            azure_endpoint=self.endpoint
        )
    
    def prepare_batch_jsonl(self, image_base64_strings, prompts):
        """
        Prepares a JSONL file for batch processing.
        
        Parameters:
        - image_base64_strings: List of base64-encoded image strings
        - prompts: List of corresponding prompts
        
        Returns:
        - BytesIO object containing the JSONL content
        """
        jsonl_file = BytesIO()
        
        for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
            # Create the request object with proper data URL format
            request = {
                "custom_id": f"request-{i+1}",
                "method": "POST",
                "url": "/chat/completions",
                "body": {
                    "model": self.deployment_name,
                    "messages": [
                        {
                            "role": "system",
                            "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
                        },
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_img}"
                                    }
                                }
                            ]
                        }
                    ],
                    "max_tokens": 2000
                }
            }
            
            # Write the JSON line to the file
            jsonl_file.write((json.dumps(request) + "\n").encode('utf-8'))
        
        # Reset the file pointer to the beginning
        jsonl_file.seek(0)
        return jsonl_file
    
    def process_batch(self, image_base64_strings, prompts):
        """
        Process images in a batch using the Azure OpenAI batch API.
        
        Parameters:
        - image_base64_strings: List of base64-encoded image strings
        - prompts: List of corresponding prompts
        
        Returns:
        - List of raw API responses
        """
        # We need to create a temporary JSONL file as the API might have issues with direct BytesIO
        tmp_jsonl_path = None
        
        try:
            # Prepare the JSONL batch file
            jsonl_file = self.prepare_batch_jsonl(image_base64_strings, prompts)
            
            # Create a temporary file for the JSONL content
            with tempfile.NamedTemporaryFile(suffix='.jsonl', delete=False) as tmp_jsonl:
                tmp_jsonl.write(jsonl_file.getvalue())
                tmp_jsonl_path = tmp_jsonl.name
            
            # Upload with explicit file open
            print("Uploading batch file to Azure...")
            with open(tmp_jsonl_path, 'rb') as f:
                file = self.client.files.create(
                    file=f,
                    purpose="batch"
                )
            
            # We can delete the temp file immediately after upload
            if tmp_jsonl_path:
                os.unlink(tmp_jsonl_path)
                tmp_jsonl_path = None
            
            file_id = file.id
            print(f"File uploaded (ID: {file_id}). Creating batch job...")
            
            # Submit batch job
            batch_response = self.client.batches.create(
                input_file_id=file_id,
                endpoint="/chat/completions",
                completion_window="24h"
            )
            
            batch_id = batch_response.id
            print(f"Batch job created (ID: {batch_id}). Waiting for processing...")
            
            # Track batch job status
            status = "validating"
            start_time = time.time()
            
            while status not in ("completed", "failed", "canceled"):
                time.sleep(10)  # Check every 10 seconds for faster feedback
                
                # Check for timeout
                if time.time() - start_time > self.timeout:
                    print(f"Timeout after {self.timeout} seconds. Canceling batch job.")
                    try:
                        self.client.batches.cancel(batch_id)
                    except:
                        pass
                    raise TimeoutError(f"Batch processing timed out after {self.timeout} seconds")
                
                batch_response = self.client.batches.retrieve(batch_id)
                status = batch_response.status
                status_message = f"{datetime.now()} Batch Id: {batch_id}, Status: {status}"
                print(status_message)
            
            if batch_response.status == "failed":
                error_message = "Batch processing failed:"
                if hasattr(batch_response, 'errors') and hasattr(batch_response.errors, 'data'):
                    for error in batch_response.errors.data:
                        error_message += f"\nError code {error.code} Message {error.message}"
                else:
                    error_message += " Unknown error occurred"
                print(error_message)
                raise Exception(error_message)
            
            # Retrieve results
            output_file_id = batch_response.output_file_id
            
            if not output_file_id:
                output_file_id = batch_response.error_file_id
                if not output_file_id:
                    print("No output or error file was produced by the batch job.")
                    raise Exception("No output file produced")
            
            print(f"Batch completed. Retrieving results...")
            file_response = self.client.files.content(output_file_id)
            raw_responses = file_response.text.strip().split('\n')
            
            return raw_responses
        
        except Exception as e:
            print(f"Error during batch processing: {str(e)}")
            raise
        finally:
            # Ensure temporary file is deleted if it exists
            if tmp_jsonl_path and os.path.exists(tmp_jsonl_path):
                try:
                    os.unlink(tmp_jsonl_path)
                except:
                    pass
    
    def process_general(self, image_base64_strings, prompts):
        """
        Process images using the general (non-batch) API.
        
        Parameters:
        - image_base64_strings: List of base64-encoded image strings
        - prompts: List of corresponding prompts
        
        Returns:
        - List of API responses
        """
        results = []
        
        for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
            try:
                print(f"Processing image {i+1}/{len(image_base64_strings)}")
                
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are an AI assistant that classifies documents and extracts information from invoices when appropriate."
                        },
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_img}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=2000,
                    temperature=0.7
                )
                
                if hasattr(response, 'choices') and len(response.choices) > 0:
                    content = response.choices[0].message.content
                    results.append(json.dumps({
                        "custom_id": f"request-{i+1}",
                        "response": {
                            "body": {
                                "choices": [
                                    {
                                        "message": {
                                            "content": content
                                        }
                                    }
                                ]
                            }
                        }
                    }))
                else:
                    results.append(json.dumps({
                        "custom_id": f"request-{i+1}",
                        "error": "No response content"
                    }))
            
            except Exception as e:
                print(f"Error processing image {i+1}: {str(e)}")
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "error": str(e)
                }))
        
        return results
