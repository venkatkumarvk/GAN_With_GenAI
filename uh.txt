"""
MAIN PIPELINE - Document Extraction with Multimodal RAG
========================================================
Entry point. Run: python main.py
"""

import json
from datetime import datetime
from typing import List, Dict, Any

from helper import AzureBlobManager, AzureOpenAIManager, DocumentIntelligenceManager
from unified_rag import create_rag_extractor
from logging_config import (
    setup_logging, log_config, log_provider_start, log_document,
    log_ocr, log_vision, log_rag, log_extraction, log_error,
    log_provider_summary, log_outputs, log_complete
)


def load_config(path: str = "config.json") -> Dict[str, Any]:
    with open(path, 'r') as f:
        return json.load(f)


def process_provider(
    provider: str,
    documents: List[str],
    blob_mgr,
    doc_intel_mgr,
    openai_mgr,
    rag_extractor,
    config: Dict[str, Any]
) -> Dict[str, Any]:
    
    # Create index_name FIRST (before any usage)
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    index_name = f"{provider.lower()}-{timestamp}"
    
    print(f"\n{'='*60}\n  PROVIDER: {provider} ({len(documents)} docs)\n{'='*60}")
    log_provider_start(provider, len(documents), index_name)
    
    fields = config['fields']
    mode = config['rag']['mode']
    
    results = []
    total_cost = 0.0
    
    for idx, doc_name in enumerate(documents, 1):
        print(f"\n [{idx}/{len(documents)}] {doc_name}")
        log_document(idx, len(documents), doc_name)
        
        try:
            doc_bytes = blob_mgr.download_blob(doc_name)
            
            # Get file extension
            import os
            file_extension = os.path.splitext(doc_name)[1]  # e.g., '.pdf', '.jpg'
            
            # Convert to base64 for Document Intelligence
            import base64
            doc_base64 = base64.b64encode(doc_bytes).decode('utf-8')
            
            # OCR (PRIMARY)
            print(f"   ğŸ” OCR...")
            log_ocr(0, 0)  # Start OCR logging
            ocr_result = doc_intel_mgr.analyze_document(doc_base64, file_extension)
            doc_text = ocr_result.get('content', '')
            
            if len(doc_text) < 50:
                print(f"   âš ï¸  Insufficient OCR - skip")
                log_error(doc_name, "Insufficient OCR text")
                continue
            
            print(f"   âœ“ OCR: {len(doc_text)} chars")
            log_ocr(len(doc_text), ocr_result.get('page_count', 1))
            
            # RAG Extraction
            if idx == 1:
                print(f"   ğŸ“„ First doc - no RAG")
                if mode == 'multimodal':
                    result = rag_extractor.extract_without_rag(doc_text, doc_bytes, None, doc_name, provider, index_name)
                else:
                    result = rag_extractor.extract_without_rag(doc_text, None, doc_name, provider, index_name)
            else:
                print(f"   ğŸ“‹ RAG extraction...")
                if mode == 'multimodal':
                    result = rag_extractor.extract_with_rag(doc_text, doc_bytes, provider, index_name, doc_name)
                else:
                    result = rag_extractor.extract_with_rag(doc_text, provider, index_name, doc_name)
            
            extracted = result.get('extracted_fields', {})
            avg_conf = sum([f.get('confidence',0) for f in extracted.values() if isinstance(f,dict)]) / len(extracted) if extracted else 0
            
            print(f"   âœ“ Confidence: {avg_conf:.2f} | RAG: {result.get('used_rag')} | Vision: {result.get('has_vision')}")
            log_rag(result.get('used_rag', False), result.get('similar_docs_count', 0), mode, result.get('has_vision', False))
            log_extraction(extracted, avg_conf)
            
            results.append({
                'document_name': doc_name,
                'extracted_fields': extracted,
                'avg_confidence': avg_conf,
                'used_rag': result.get('used_rag', False),
                'has_vision': result.get('has_vision', False)
            })
            
            total_cost += 0.07 if mode == 'multimodal' else 0.05
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
            log_error(doc_name, str(e))
    
    print(f"\n  âœ“ Processed: {len(results)}/{len(documents)} | Cost: ${total_cost:.2f}")
    log_provider_summary(provider, len(results), len(documents), total_cost)
    
    return {
        'provider': provider,
        'results': results,
        'total_cost': total_cost,
        'index_name': index_name
    }


def save_results(provider_results: Dict, blob_mgr, config: Dict):
    provider = provider_results['provider']
    results = provider_results['results']
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    fields = config['fields']
    confidence_threshold = config.get('confidence_threshold', 0.90)
    
    # CSV - one row
    csv_lines = [','.join(['provider','doc_count'] + fields + ['avg_confidence'])]
    agg = {'provider': provider, 'doc_count': len(results)}
    
    for field in fields:
        for r in results:
            if field in r['extracted_fields']:
                fd = r['extracted_fields'][field]
                agg[field] = fd.get('value','') if isinstance(fd,dict) else str(fd)
                break
        if field not in agg:
            agg[field] = ''
    
    avg = sum([r['avg_confidence'] for r in results]) / len(results) if results else 0
    agg['avg_confidence'] = f"{avg:.2f}"
    
    csv_lines.append(','.join([agg.get('provider',''), str(agg.get('doc_count',0))] + [agg.get(f,'') for f in fields] + [agg.get('avg_confidence','0.00')]))
    
    # Determine folder: highconfidence or lowconfidence
    folder = "highconfidence" if avg >= confidence_threshold else "lowconfidence"
    
    # CSV
    csv_file = f"{folder}/{provider}_{timestamp}.csv"
    blob_mgr.upload_blob(csv_file, '\n'.join(csv_lines).encode('utf-8'))
    print(f"  âœ“ CSV: {csv_file}")
    
    # JSON - detailed
    json_file = f"{folder}/{provider}_{timestamp}_detailed.json"
    blob_mgr.upload_blob(json_file, json.dumps(provider_results, indent=2).encode('utf-8'))
    print(f"  âœ“ JSON: {json_file}")
    
    # Costs
    cost_file = f"{folder}/{provider}_{timestamp}_costs.json"
    cost_data = {
        'provider': provider,
        'total_cost_usd': provider_results['total_cost'],
        'documents': len(results),
        'cost_per_doc': provider_results['total_cost']/len(results) if results else 0,
        'avg_confidence': avg,
        'confidence_category': folder
    }
    blob_mgr.upload_blob(cost_file, json.dumps(cost_data, indent=2).encode('utf-8'))
    print(f"  âœ“ Costs: {cost_file}")
    print(f"  ğŸ“Š Confidence: {avg:.2f} â†’ Saved to '{folder}/' folder")
    log_outputs(csv_file, json_file, cost_file)


def main():
    # Setup logging FIRST
    log_file = setup_logging(log_dir="logs", log_level="INFO")
    
    print("\n" + "="*60)
    print("  MULTIMODAL RAG â€” DOCUMENT EXTRACTION")
    print("  Healthcare / HIPAA Â· Azure BAA")
    print("="*60)
    
    config = load_config()
    log_config(config)
    print(f"\nâœ“ Config loaded | Fields: {', '.join(config['fields'])}")
    
    # Get RAG mode safely
    rag_mode = config.get('rag', {}).get('mode', 'text')
    print(f"âœ“ RAG Mode: {rag_mode.upper()}")
    
    # Initialize managers
    blob_mgr = AzureBlobManager(
        config['AzureBlob']['connection_string'],
        config['AzureBlob']['inputcontainer'],
        config['AzureBlob']['outputcontainer']
    )
    print(f"âœ“ Blob connected")
    
    openai_mgr = AzureOpenAIManager(
        config['AzureOpenAI']['endpoint'],
        config['AzureOpenAI']['api_key'],
        config['AzureOpenAI']['api_version'],
        config['AzureOpenAI']['deployment_name'],
        config['AzureEmbedding']['endpoint'],
        config['AzureEmbedding']['api_key'],
        config['AzureEmbedding'].get('api_version', config['AzureOpenAI']['api_version']),  # Use same version if not specified
        config['AzureEmbedding']['deployment_name'],
        config['AzureEmbedding'].get('dimension', 3072)  # Default to 3072
    )
    print(f"âœ“ OpenAI connected")
    
    doc_intel_mgr = DocumentIntelligenceManager(
        config['DocumentIntelligence']['endpoint'],
        config['DocumentIntelligence']['key']
    )
    print(f"âœ“ Doc Intelligence connected")
    
    rag_extractor = create_rag_extractor(
        config['rag'],
        config['AzureAISearch']['endpoint'],
        config['AzureAISearch']['api_key'],
        openai_mgr,
        config['fields']
    )
    print(f"âœ“ RAG Extractor ready")
    
    # Get documents
    all_docs = blob_mgr.list_blobs()
    print(f"\nâœ“ Found {len(all_docs)} documents")
    
    if not all_docs:
        print("\nâš ï¸  No documents. Upload to input container and retry.")
        return
    
    # Group by provider
    providers = {}
    for doc in all_docs:
        prov = doc.split('_')[0] if '_' in doc else 'default'
        if prov not in providers:
            providers[prov] = []
        providers[prov].append(doc)
    
    print(f"âœ“ Grouped into {len(providers)} providers")
    
    # Process each
    for prov, docs in providers.items():
        prov_results = process_provider(prov, docs, blob_mgr, doc_intel_mgr, openai_mgr, rag_extractor, config)
        save_results(prov_results, blob_mgr, config)
    
    print("\n" + "="*60)
    print("  COMPLETE âœ“")
    print("="*60 + "\n")
    log_complete(len(providers), len(all_docs))


if __name__ == "__main__":
    main()

-----------
helper.py
import os
import json
import base64
import logging
import hashlib
from datetime import datetime
from io import BytesIO
from typing import List, Dict, Any, Tuple

import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential

from azure.storage.blob import BlobServiceClient, ContentSettings
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchableField, SearchField, SearchFieldDataType,  # ADDED SearchField
    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,
    SemanticConfiguration, SemanticField, SemanticPrioritizedFields, SemanticSearch
)
from openai import AzureOpenAI

# Import standalone OCR module
from documentintelligence_ocr import DocumentIntelligenceOCR

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)


class ConfigManager:
    def __init__(self, config_path: str = 'config.json'):
        self.config_path = config_path
        self.config = self.load_config()

    def load_config(self) -> Dict[str, Any]:
        with open(self.config_path, 'r') as f:
            cfg = json.load(f)
        return cfg

    def get(self, section: str, key: str = None) -> Any:
        if key:
            return self.config.get(section, {}).get(key)
        return self.config.get(section)


class AzureBlobManager:
    def __init__(self, connection_string: str, input_container: str, output_container: str):
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self.input_container = input_container
        self.output_container = output_container
        self.supported_extensions = {'.pdf', '.doc', '.docx', '.png', '.jpg', '.jpeg', '.tiff', '.tif'}

    def get_providers(self) -> List[str]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs()
        providers = set()
        for blob in blobs:
            parts = blob.name.split('/')
            if parts[0]:
                providers.add(parts[0])
        return sorted(list(providers))
    
    def list_blobs(self) -> List[str]:
        """List all blobs in input container (for main.py compatibility)"""
        container_client = self.blob_service_client.get_container_client(self.input_container)
        return [blob.name for blob in container_client.list_blobs()]
    
    def download_blob(self, blob_name: str) -> bytes:
        """Download blob as bytes (for main.py compatibility)"""
        blob_client = self.blob_service_client.get_blob_client(self.input_container, blob_name)
        return blob_client.download_blob().readall()
    
    def upload_blob(self, blob_name: str, data: bytes, overwrite: bool = True):
        """Upload blob to output container (for main.py compatibility)"""
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_name)
        blob_client.upload_blob(data, overwrite=overwrite)

    def get_provider_files(self, provider: str) -> List[Dict[str, str]]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs(name_starts_with=f"{provider}/")
        files = []
        for blob in blobs:
            ext = os.path.splitext(blob.name)[1].lower()
            if ext in self.supported_extensions:
                files.append({
                    'name': blob.name,
                    'filename': os.path.basename(blob.name),
                    'provider': provider,
                    'size': blob.size,
                    'extension': ext
                })
        return files

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def download_blob_as_base64(self, blob_name: str) -> str:
        blob_client = self.blob_service_client.get_blob_client(self.input_container, blob_name)
        data = blob_client.download_blob().readall()
        return base64.b64encode(data).decode('utf-8')

    def upload_to_blob(self, data, blob_path: str, content_type: str = 'text/plain'):
        """Upload data to blob storage"""
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        content_settings = ContentSettings(content_type=content_type)
        if isinstance(data, str):
            data = data.encode('utf-8')
        blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
        logger.info(f"Uploaded to blob: {blob_path}")

    def upload_dataframe_as_csv(self, df: pd.DataFrame, blob_path: str):
        csv_buffer = BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_data = csv_buffer.getvalue()
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        content_settings = ContentSettings(content_type='text/csv')
        blob_client.upload_blob(csv_data, overwrite=True, content_settings=content_settings)
class DocumentIntelligenceManager:
    """
    Document Intelligence Manager - Wrapper around standalone OCR module
    
    This class wraps documentintelligence_ocr.py for compatibility with main.py
    """
    
    def __init__(self, endpoint: str, key: str):
        """
        Initialize Document Intelligence Manager
        
        Args:
            endpoint: Azure Document Intelligence endpoint
            key: Azure Document Intelligence API key
        """
        self.endpoint = endpoint
        self.key = key
        
        # Initialize standalone OCR module
        self.ocr = DocumentIntelligenceOCR(endpoint=endpoint, api_key=key)
        
        logger.info(f"DocumentIntelligenceManager initialized (using standalone OCR module)")
    
    def analyze_document(self, base64_data: str, file_extension: str) -> Dict[str, Any]:
        """
        Analyze document and extract text
        
        Args:
            base64_data: Base64-encoded document bytes
            file_extension: File extension (e.g., '.pdf', '.jpg', '.docx')
        
        Returns:
            Dictionary with OCR results
        """
        # Call standalone OCR module
        result = self.ocr.extract_text(base64_data, file_extension)
        return result
            }


class AzureOpenAIManager:
    """Manages both GPT extraction and embeddings with separate clients"""
    
    def __init__(self, gpt_endpoint: str, gpt_api_key: str, gpt_api_version: str, gpt_deployment: str,
                 embedding_endpoint: str, embedding_api_key: str, embedding_api_version: str, 
                 embedding_deployment: str, embedding_dimension: int = 3072):
        
        # GPT-4o client for field extraction
        self.gpt_client = AzureOpenAI(
            azure_endpoint=gpt_endpoint,
            api_key=gpt_api_key,
            api_version=gpt_api_version
        )
        self.gpt_deployment = gpt_deployment
        
        # Separate embedding client (important for separate deployment)
        self.embedding_client = AzureOpenAI(
            azure_endpoint=embedding_endpoint,
            api_key=embedding_api_key,
            api_version=embedding_api_version
        )
        self.embedding_deployment = embedding_deployment
        self.embedding_dimension = embedding_dimension
        
        # Token tracking
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def extract_fields(self, text: str, fields: List[str], source_document: str) -> Dict[str, Any]:
        try:
            system_prompt = f"""You are a document extraction expert. Extract the following fields: {', '.join(fields)}.

Return ONLY a JSON object with this structure:
{{
    "field_name": {{"value": "extracted_value", "confidence": 0.95}},
    ...
}}

Be precise with confidence scores."""
            
            user_prompt = f"Document text:\n\n{text[:8000]}"
            
            response = self.gpt_client.chat.completions.create(
                model=self.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                max_tokens=2000
            )
            
            self.total_tokens += response.usage.total_tokens
            self.prompt_tokens += response.usage.prompt_tokens
            self.completion_tokens += response.usage.completion_tokens

            content = response.choices[0].message.content.strip()
            
            # Remove markdown code blocks
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()
            
            data = json.loads(content)
            
            # Normalize data format
            normalized_data = {}
            for field_name in data:
                field_value = data[field_name]
                
                if isinstance(field_value, dict) and 'value' in field_value:
                    normalized_data[field_name] = field_value
                    normalized_data[field_name]['source_document'] = source_document
                else:
                    # Wrap direct values
                    normalized_data[field_name] = {
                        'value': str(field_value),
                        'confidence': 0.5,
                        'source_document': source_document
                    }
            
            logger.info(f"Extracted {len(normalized_data)} fields successfully")
            
            return {
                'success': True,
                'extracted_fields': normalized_data,
                'raw_response': content
            }
            
        except Exception as e:
            logger.error(f"Field extraction failed: {e}", exc_info=True)
            return {
                'success': False,
                'extracted_fields': {},
                'raw_response': '',
                'error': str(e)
            }

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def generate_embeddings(self, text: str) -> List[float]:
        """Generate embeddings using separate embedding deployment"""
        try:
            # Validate input
            if not text or not isinstance(text, str):
                logger.warning("Invalid text for embeddings, using placeholder")
                text = "No content available"
            
            text = str(text).strip()
            
            if len(text) < 3:
                logger.warning("Text too short, using placeholder")
                text = "No content available"
            
            # Truncate if needed
            if len(text) > 30000:
                text = text[:30000]
                logger.info("Text truncated for embedding")
            
            # Call embedding endpoint (separate from GPT)
            response = self.embedding_client.embeddings.create(
                model=self.embedding_deployment,
                input=text
            )
            
            embeddings = response.data[0].embedding
            logger.info(f"Generated embeddings: dimension={len(embeddings)}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}", exc_info=True)
            # Return zero vector as fallback
            logger.warning(f"Returning zero vector of dimension {self.embedding_dimension}")
            return [0.0] * self.embedding_dimension

    def get_token_usage(self) -> Dict[str, int]:
        return {
            'total_tokens': self.total_tokens,
            'prompt_tokens': self.prompt_tokens,
            'completion_tokens': self.completion_tokens
        }

    def calculate_cost(self, costs_config: Dict[str, float]) -> Dict[str, float]:
        input_cost = (self.prompt_tokens / 1000) * costs_config.get('gpt4o_input_per_1k', 0)
        output_cost = (self.completion_tokens / 1000) * costs_config.get('gpt4o_output_per_1k', 0)
        total_cost = input_cost + output_cost
        return {
            'input_cost': input_cost,
            'output_cost': output_cost,
            'total_cost': total_cost
        }


class AzureAISearchManager:
    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.credential = AzureKeyCredential(api_key)
        self.index_client = SearchIndexClient(endpoint=endpoint, credential=self.credential)

    def get_index_name(self, provider: str, timestamp: str = None) -> str:
        """
        Sanitize provider name for Azure AI Search index
        
        If timestamp provided: Returns providername_timestamp (e.g., "anand_20240211_143022")
        If no timestamp: Returns providername only (e.g., "anand")
        
        Args:
            provider: Provider name
            timestamp: Optional timestamp string (YYYYMMDD_HHMMSS)
        
        Returns:
            Sanitized index name
        """
        # Sanitize provider name
        sanitized = provider.lower().replace(' ', '-').replace('_', '-')
        sanitized = ''.join(c for c in sanitized if c.isalnum() or c == '-')
        sanitized = sanitized.strip('-')
        
        # Add timestamp if provided (for unique index per run)
        if timestamp:
            # Replace underscores with hyphens for index name
            clean_timestamp = timestamp.replace('_', '')  # Remove underscores: 20240211143022
            index_name = f"{sanitized}-{clean_timestamp}"
        else:
            index_name = sanitized
        
        return index_name

    def create_index(self, provider: str, embedding_dimension: int = 3072, timestamp: str = None):
        """
        Create search index with vector field and all required fields
        
        Args:
            provider: Provider name
            embedding_dimension: Vector dimension (default 3072)
            timestamp: Optional timestamp for unique index name (YYYYMMDD_HHMMSS)
        """
        index_name = self.get_index_name(provider, timestamp)
        
        logger.info(f"Creating index '{index_name}' with vector dimension {embedding_dimension}")
        
        # Define fields including vector field using SearchField
        fields = [
            SimpleField(name="id", type=SearchFieldDataType.String, key=True),
            SearchableField(name="provider_id", type=SearchFieldDataType.String, filterable=True, sortable=True),
            SearchableField(name="provider", type=SearchFieldDataType.String, filterable=True),
            SearchableField(name="document_name", type=SearchFieldDataType.String, filterable=True),
            SimpleField(name="document_type", type=SearchFieldDataType.String, filterable=True),
            SimpleField(name="file_extension", type=SearchFieldDataType.String, filterable=True),
            SearchableField(name="content", type=SearchFieldDataType.String),
            SimpleField(name="page_count", type=SearchFieldDataType.Int32, filterable=True),
            SimpleField(name="total_documents", type=SearchFieldDataType.Int32, filterable=True),
            SimpleField(name="extraction_datetime", type=SearchFieldDataType.String, sortable=True),
            SearchableField(name="extracted_fields", type=SearchFieldDataType.String),
            SimpleField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
            # MULTIMODAL FIELDS
            SearchableField(name="visual_description", type=SearchFieldDataType.String),
            SimpleField(name="is_multimodal", type=SearchFieldDataType.Boolean, filterable=True),
            SimpleField(name="modality", type=SearchFieldDataType.String, filterable=True),
            # CRITICAL: Use SearchField for vector field
            SearchField(
                name="content_vector",
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                searchable=True,
                vector_search_dimensions=embedding_dimension,
                vector_search_profile_name="vector-profile"
            )
        ]
        
        # Vector search configuration
        vector_search = VectorSearch(
            algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
            profiles=[VectorSearchProfile(name="vector-profile", algorithm_configuration_name="hnsw-config")]
        )
        
        # Create index
        index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
        self.index_client.create_or_update_index(index)
        
        logger.info(f"Index '{index_name}' created successfully with multimodal fields")
        return index_name

    def upload_documents(self, provider: str, documents: List[Dict[str, Any]], timestamp: str = None):
        """
        Upload documents to search index
        
        Args:
            provider: Provider name
            documents: List of documents to upload
            timestamp: Optional timestamp to match index name
        """
        if not documents:
            logger.warning("No documents to upload")
            return
        
        index_name = self.get_index_name(provider, timestamp)
        
        logger.info(f"Uploading {len(documents)} documents to index '{index_name}'")
        
        try:
            client = SearchClient(
                endpoint=self.endpoint,
                index_name=index_name,
                credential=self.credential
            )
            
            result = client.upload_documents(documents=documents)
            
            # Check results
            success_count = sum(1 for r in result if r.succeeded)
            logger.info(f"Uploaded {success_count}/{len(documents)} documents successfully")
            
            if success_count < len(documents):
                failed = [r for r in result if not r.succeeded]
                for fail in failed:
                    logger.error(f"Failed to upload document: {fail.key} - {fail.error_message}")
            
        except Exception as e:
            logger.error(f"Document upload failed: {e}", exc_info=True)
            raise


-----

documentintelligence


"""
DOCUMENT INTELLIGENCE OCR - Standalone OCR Module
==================================================

Complete Azure Document Intelligence OCR implementation.
Supports: PDF, Images (JPG, PNG, TIFF, BMP), Word documents (.doc, .docx)

Features:
- Automatic Word â†’ PDF conversion
- Retry logic with exponential backoff
- Multi-page document support
- Base64 input handling
- Page count tracking
- Comprehensive error handling

Usage:
    from document_intelligence_ocr import DocumentIntelligenceOCR
    
    ocr = DocumentIntelligenceOCR(endpoint, api_key)
    result = ocr.extract_text(base64_data, file_extension)
    
    text = result['content']
    pages = result['page_count']
"""

import base64
import logging
import os
import tempfile
from typing import Dict, Any, Optional
from tenacity import retry, stop_after_attempt, wait_exponential

from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest

logger = logging.getLogger(__name__)


class DocumentIntelligenceOCR:
    """
    Azure Document Intelligence OCR Manager
    
    Extracts text from documents using Azure's prebuilt-read model.
    Handles multiple file formats with automatic conversion.
    """
    
    def __init__(self, endpoint: str, api_key: str):
        """
        Initialize Document Intelligence OCR client
        
        Args:
            endpoint: Azure Document Intelligence endpoint URL
            api_key: Azure Document Intelligence API key
        """
        self.endpoint = endpoint
        self.api_key = api_key
        
        self.client = DocumentIntelligenceClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(api_key)
        )
        
        logger.info(f"DocumentIntelligenceOCR initialized | endpoint={endpoint}")
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def extract_text(
        self,
        base64_data: str,
        file_extension: str,
        convert_word: bool = True
    ) -> Dict[str, Any]:
        """
        Extract text from document using Azure Document Intelligence
        
        Args:
            base64_data: Base64-encoded document bytes
            file_extension: File extension (e.g., '.pdf', '.jpg', '.docx')
            convert_word: If True, auto-convert Word docs to PDF (default: True)
        
        Returns:
            Dictionary with:
            {
                'success': True/False,
                'content': 'Extracted text...',
                'page_count': 3,
                'error': 'Error message if failed'
            }
        
        Raises:
            Exception: If OCR fails after 3 retry attempts
        """
        try:
            file_ext = file_extension.lower()
            
            # Handle Word documents - convert to PDF first
            if file_ext in ['.doc', '.docx'] and convert_word:
                logger.info(f"Converting {file_ext} to PDF for OCR processing")
                result = self._convert_word_to_pdf(base64_data, file_ext)
                
                if result['success']:
                    base64_data = result['pdf_base64']
                    file_ext = '.pdf'
                    logger.info("Word document converted to PDF successfully")
                else:
                    # Fallback: Try direct text extraction from Word
                    logger.warning("PDF conversion failed, attempting direct Word text extraction")
                    return self._extract_text_from_word(base64_data, file_ext)
            
            # Validate file type
            supported_types = ['.pdf', '.jpg', '.jpeg', '.png', '.tiff', '.tif', '.bmp']
            if file_ext not in supported_types:
                error_msg = f"Unsupported file type: {file_ext}. Supported: {supported_types}"
                logger.error(error_msg)
                return {
                    'success': False,
                    'content': '',
                    'page_count': 0,
                    'error': error_msg
                }
            
            # Perform OCR using Azure Document Intelligence
            logger.info(f"Starting OCR analysis for {file_ext} file")
            
            # Create analyze request
            analyze_request = AnalyzeDocumentRequest(base64_source=base64_data)
            
            # Call Document Intelligence (prebuilt-read model)
            poller = self.client.begin_analyze_document(
                model_id="prebuilt-read",
                analyze_request=analyze_request
            )
            
            # Wait for result
            result = poller.result()
            
            # Extract text content
            content = result.content if hasattr(result, 'content') else ''
            
            # Get page count
            page_count = len(result.pages) if hasattr(result, 'pages') else 0
            
            # Log success
            logger.info(
                f"OCR completed successfully | "
                f"chars={len(content)} | pages={page_count}"
            )
            
            return {
                'success': True,
                'content': content,
                'page_count': page_count,
                'error': None
            }
            
        except Exception as e:
            error_msg = f"OCR extraction failed: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return {
                'success': False,
                'content': '',
                'page_count': 0,
                'error': error_msg
            }
    
    def _convert_word_to_pdf(
        self,
        base64_data: str,
        file_extension: str
    ) -> Dict[str, Any]:
        """
        Convert Word document to PDF
        
        Args:
            base64_data: Base64-encoded Word document
            file_extension: '.doc' or '.docx'
        
        Returns:
            Dictionary with:
            {
                'success': True/False,
                'pdf_base64': 'base64-encoded PDF',
                'error': 'Error message if failed'
            }
        """
        try:
            # Try using docx2pdf (Windows/Mac)
            try:
                from docx2pdf import convert as docx_to_pdf_convert
                
                # Decode base64 to bytes
                document_bytes = base64.b64decode(base64_data)
                
                # Create temporary files
                with tempfile.NamedTemporaryFile(
                    suffix=file_extension,
                    delete=False
                ) as temp_word:
                    temp_word.write(document_bytes)
                    temp_word_path = temp_word.name
                
                temp_pdf_path = temp_word_path.replace(file_extension, '.pdf')
                
                # Convert to PDF
                docx_to_pdf_convert(temp_word_path, temp_pdf_path)
                
                # Read PDF and encode to base64
                with open(temp_pdf_path, 'rb') as pdf_file:
                    pdf_bytes = pdf_file.read()
                    pdf_base64 = base64.b64encode(pdf_bytes).decode('utf-8')
                
                # Clean up temp files
                os.remove(temp_word_path)
                os.remove(temp_pdf_path)
                
                logger.info("Word to PDF conversion successful (docx2pdf)")
                
                return {
                    'success': True,
                    'pdf_base64': pdf_base64,
                    'error': None
                }
                
            except ImportError:
                logger.warning("docx2pdf not installed, trying alternative method")
                
                # Alternative: Use LibreOffice (Linux)
                try:
                    import subprocess
                    
                    document_bytes = base64.b64decode(base64_data)
                    
                    with tempfile.NamedTemporaryFile(
                        suffix=file_extension,
                        delete=False
                    ) as temp_word:
                        temp_word.write(document_bytes)
                        temp_word_path = temp_word.name
                    
                    temp_dir = os.path.dirname(temp_word_path)
                    
                    # Convert using LibreOffice
                    subprocess.run(
                        [
                            'libreoffice',
                            '--headless',
                            '--convert-to',
                            'pdf',
                            '--outdir',
                            temp_dir,
                            temp_word_path
                        ],
                        check=True,
                        capture_output=True
                    )
                    
                    # Read converted PDF
                    pdf_path = temp_word_path.replace(file_extension, '.pdf')
                    
                    with open(pdf_path, 'rb') as pdf_file:
                        pdf_bytes = pdf_file.read()
                        pdf_base64 = base64.b64encode(pdf_bytes).decode('utf-8')
                    
                    # Clean up
                    os.remove(temp_word_path)
                    os.remove(pdf_path)
                    
                    logger.info("Word to PDF conversion successful (LibreOffice)")
                    
                    return {
                        'success': True,
                        'pdf_base64': pdf_base64,
                        'error': None
                    }
                    
                except Exception as e:
                    error_msg = f"LibreOffice conversion failed: {str(e)}"
                    logger.error(error_msg)
                    return {
                        'success': False,
                        'pdf_base64': '',
                        'error': error_msg
                    }
                    
        except Exception as e:
            error_msg = f"Word to PDF conversion failed: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return {
                'success': False,
                'pdf_base64': '',
                'error': error_msg
            }
    
    def _extract_text_from_word(
        self,
        base64_data: str,
        file_extension: str
    ) -> Dict[str, Any]:
        """
        Fallback: Extract text directly from Word document
        
        Args:
            base64_data: Base64-encoded Word document
            file_extension: '.doc' or '.docx'
        
        Returns:
            Dictionary with extracted text
        """
        try:
            from docx import Document
            import io
            
            logger.info("Attempting direct text extraction from Word document")
            
            # Decode base64
            document_bytes = base64.b64decode(base64_data)
            
            # Load Word document
            doc = Document(io.BytesIO(document_bytes))
            
            # Extract all text from paragraphs
            text_parts = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_parts.append(paragraph.text)
            
            # Extract text from tables
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            text_parts.append(cell.text)
            
            content = '\n'.join(text_parts)
            
            logger.info(
                f"Direct Word text extraction successful | "
                f"chars={len(content)}"
            )
            
            return {
                'success': True,
                'content': content,
                'page_count': 1,  # Approximate
                'error': None
            }
            
        except ImportError:
            error_msg = "python-docx not installed. Cannot extract text from Word."
            logger.error(error_msg)
            return {
                'success': False,
                'content': '',
                'page_count': 0,
                'error': error_msg
            }
            
        except Exception as e:
            error_msg = f"Direct Word text extraction failed: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return {
                'success': False,
                'content': '',
                'page_count': 0,
                'error': error_msg
            }
    
    def extract_from_file(self, file_path: str) -> Dict[str, Any]:
        """
        Extract text from a file on disk
        
        Args:
            file_path: Path to the file
        
        Returns:
            Dictionary with extraction results
        """
        try:
            # Read file
            with open(file_path, 'rb') as f:
                file_bytes = f.read()
            
            # Encode to base64
            base64_data = base64.b64encode(file_bytes).decode('utf-8')
            
            # Get extension
            file_extension = os.path.splitext(file_path)[1]
            
            # Extract text
            return self.extract_text(base64_data, file_extension)
            
        except Exception as e:
            error_msg = f"Failed to read file {file_path}: {str(e)}"
            logger.error(error_msg)
            return {
                'success': False,
                'content': '',
                'page_count': 0,
                'error': error_msg
            }
    
    def extract_from_bytes(
        self,
        file_bytes: bytes,
        file_extension: str
    ) -> Dict[str, Any]:
        """
        Extract text from bytes
        
        Args:
            file_bytes: Document bytes
            file_extension: File extension (e.g., '.pdf')
        
        Returns:
            Dictionary with extraction results
        """
        try:
            # Encode to base64
            base64_data = base64.b64encode(file_bytes).decode('utf-8')
            
            # Extract text
            return self.extract_text(base64_data, file_extension)
            
        except Exception as e:
            error_msg = f"Failed to process bytes: {str(e)}"
            logger.error(error_msg)
            return {
                'success': False,
                'content': '',
                'page_count': 0,
                'error': error_msg
            }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USAGE EXAMPLES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
# Example 1: Extract from base64 string
from document_intelligence_ocr import DocumentIntelligenceOCR

ocr = DocumentIntelligenceOCR(
    endpoint="https://YOUR_ENDPOINT.cognitiveservices.azure.com/",
    api_key="YOUR_API_KEY"
)

# Base64-encoded PDF
result = ocr.extract_text(base64_pdf_data, '.pdf')

if result['success']:
    print(f"Extracted text: {result['content'][:200]}...")
    print(f"Pages: {result['page_count']}")
else:
    print(f"Error: {result['error']}")


# Example 2: Extract from file path
result = ocr.extract_from_file('/path/to/document.pdf')
print(result['content'])


# Example 3: Extract from bytes
with open('document.jpg', 'rb') as f:
    file_bytes = f.read()

result = ocr.extract_from_bytes(file_bytes, '.jpg')
print(result['content'])


# Example 4: Word document (auto-converts to PDF)
result = ocr.extract_text(base64_docx_data, '.docx')
# Automatically converts .docx â†’ PDF â†’ OCR


# Example 5: Error handling
result = ocr.extract_text(invalid_data, '.xyz')
if not result['success']:
    print(f"OCR failed: {result['error']}")
"""
