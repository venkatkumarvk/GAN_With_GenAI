
"""
TEXT RAG - Text-Only Document Extraction
=========================================

OCR text is PRIMARY source.
Uses Azure AI Search vector search to find similar past documents.
Passes them as few-shot examples to GPT-4o for extraction.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import json
import logging
from typing import List, Dict, Any, Optional

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class TextRAGExtractor:
    """
    Text-Only RAG Extractor

    Flow:
        Document ‚Üí OCR (primary) ‚Üí Embed ‚Üí Search Similar ‚Üí GPT-4o Extract

    Mode: Fast, cost-effective, 92-95% accuracy
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(f"TextRAGExtractor ready | top_k={top_k} | threshold={similarity_threshold}")
        print(f"    ‚úì RAG Mode : TEXT-ONLY")
        print(f"    ‚úì Vision   : Disabled")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # PUBLIC: Extract with RAG
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def extract_with_rag(
        self,
        document_text: str,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Text RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, embeddings
            }
        """
        logger.info(f"Text RAG extraction | doc={source_document} | provider={provider}")

        # Step 1 ‚Äî Generate text embedding (OCR is primary)
        embedding = self.openai_manager.generate_embeddings(document_text)
        logger.info("  ‚úì Text embedding generated")

        # Step 2 ‚Äî Search similar documents
        similar_docs = self._search_similar(embedding, provider, index_name)

        # Step 3 ‚Äî Build prompt
        if similar_docs:
            print(f"    ‚úì RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  ‚úì {len(similar_docs)} similar docs retrieved")
            user_prompt = self._build_rag_prompt(document_text, similar_docs)
        else:
            print(f"    ‚ö† RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  ‚ö† No similar docs ‚Äî standard extraction")
            user_prompt = self._build_standard_prompt(document_text)

        # Step 4 ‚Äî GPT-4o extraction
        extracted_fields = self._call_gpt(user_prompt, document_type)
        
        # Step 5 ‚Äî Store in Azure AI Search for future RAG
        confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        self._store_in_index(
            index_name=index_name,
            document_name=source_document,
            provider=provider,
            document_text=document_text,
            text_embedding=embedding,  # Fixed: was text_emb
            extracted_fields=extracted_fields,
            avg_confidence=avg_confidence
        )

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':        False,
            'visual_description': '',
            'prompt':            user_prompt,  # Include the prompt used
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding   # combined = text in text-only mode
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        document_type: Optional[str] = None,
        source_document: str = '',
        provider: str = '',
        index_name: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG ‚Äî used for first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            document_type:   Document type hint
            source_document: Source filename
            provider:        Provider name (for storage)
            index_name:      Index name (for storage)

        Returns:
            Same shape as extract_with_rag
        """
        logger.info(f"Text extraction (no RAG) | doc={source_document}")

        embedding   = self.openai_manager.generate_embeddings(document_text)
        user_prompt = self._build_standard_prompt(document_text)
        extracted_fields = self._call_gpt(user_prompt, document_type)
        
        # Store in index for future RAG (if provider/index provided)
        if provider and index_name:
            confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            self._store_in_index(
                index_name=index_name,
                document_name=source_document,
                provider=provider,
                document_text=document_text,
                text_embedding=embedding,
                extracted_fields=extracted_fields,
                avg_confidence=avg_confidence
            )

        return {
            'success':           True,
            'extracted_fields':  extracted_fields,
            'source_document':   source_document,
            'mode':              'text',
            'used_rag':          False,
            'similar_docs_count': 0,
            'has_vision':        False,
            'visual_description': '',
            'prompt':            user_prompt,  # Include the prompt used
            'embeddings': {
                'text':     embedding,
                'visual':   None,
                'combined': embedding
            }
        }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # PRIVATE HELPERS
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _store_in_index(
        self,
        index_name: str,
        document_name: str,
        provider: str,
        document_text: str,
        text_embedding: List[float],
        extracted_fields: Dict[str, Any],
        avg_confidence: float
    ):
        """
        Store document in Azure AI Search index for future RAG
        """
        try:
            from azure.search.documents.indexes import SearchIndexClient
            from azure.search.documents.indexes.models import (
                SearchIndex, SearchField, SearchFieldDataType,
                VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile
            )
            
            # Create index if not exists
            index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=self.search_credential
            )
            
            try:
                index_client.get_index(index_name)
                logger.info(f"  Index '{index_name}' exists")
            except:
                logger.info(f"  Creating index '{index_name}'...")
                
                fields = [
                    SearchField(name="id", type=SearchFieldDataType.String, key=True, filterable=True),
                    SearchField(name="document_name", type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SearchField(name="provider", type=SearchFieldDataType.String, filterable=True),
                    SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="extracted_fields", type=SearchFieldDataType.String),
                    SearchField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
                    SearchField(
                        name="content_vector",
                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        vector_search_dimensions=3072,
                        vector_search_profile_name="text-profile"
                    )
                ]
                
                vector_search = VectorSearch(
                    algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
                    profiles=[VectorSearchProfile(name="text-profile", algorithm_configuration_name="hnsw-config")]
                )
                
                index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
                index_client.create_index(index)
                logger.info(f"  ‚úì Index '{index_name}' created")
            
            # Upload document
            doc_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            # Generate document ID for Azure AI Search
            # LONG-TERM SOLUTION: Use hash of original filename
            # This way:
            # 1. Original filename preserved in metadata
            # 2. Document ID is always valid (hash is alphanumeric)
            # 3. No future errors from new special characters
            # 4. Consistent and deterministic
            
            import hashlib
            
            # Create hash from provider + document name (deterministic)
            unique_string = f"{provider}_{document_name}"
            hash_value = hashlib.sha256(unique_string.encode()).hexdigest()[:32]  # 32 chars
            
            # Document ID: hash only (always valid!)
            doc_id = f"{provider}_{hash_value}"
            
            # Store original filename separately in document metadata
            # This is returned in search results and stored in JSON
            
            document = {
                "id": doc_id,
                "document_name": document_name,
                "provider": provider,
                "content": document_text[:50000],
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": avg_confidence,
                "content_vector": text_embedding
            }
            
            doc_client.upload_documents([document])
            logger.info(f"  ‚úì Stored in index: {doc_id}")
            
        except Exception as e:
            logger.error(f"Failed to store in index: {e}", exc_info=True)

    def _search_similar(
        self,
        embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search for similar documents"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=["document_name", "content", "extracted_fields", "avg_confidence"]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _build_rag_prompt(
        self,
        document_text: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build user prompt with RAG few-shot examples"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass
            parts.append("")

        parts.append("‚îÄ" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")
        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:8000])
        parts.append("")
        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(self, document_text: str) -> str:
        """Build standard prompt without RAG context"""
        return (
            f"Extract the following fields from this document:\n\n"
            f"OCR TEXT (Primary Source):\n{document_text[:8000]}\n\n"
            f"Fields to extract: {', '.join(self.fields)}\n\n"
            "Return ONLY a JSON object with field values and confidence scores."
        )

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> Dict[str, Any]:
        """Call GPT-4o and return parsed extraction result"""
        try:
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            # Track tokens
            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content)

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}



------------------------
multimodal

"""
MULTIMODAL RAG - Vision + Text Document Extraction
====================================================

OCR text is PRIMARY source.
Vision (GPT-4 Vision) is SUPPLEMENTARY enhancement.
Combined embedding (text + visual) improves RAG similarity search.

DO NOT EDIT - Stable code
Users configure via config.json
"""

import base64
import json
import logging
from typing import Dict, Any, List, Optional, Tuple

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

from prompt_builder import ExtractionPromptBuilder

logger = logging.getLogger(__name__)


class MultimodalRAGExtractor:
    """
    Multimodal RAG Extractor ‚Äî Text + Vision

    Flow:
        Document ‚Üí OCR (primary)       ‚Üí Text Embed   ‚îÄ‚îê
                 ‚Üí Vision (supplement) ‚Üí Visual Embed ‚îÄ‚î§ ‚Üí Combined ‚Üí Search ‚Üí GPT-4o
                                                       ‚îÄ‚îò

    Mode: Higher accuracy (95-97%), 100% coverage (handles image-only docs)
    """

    def __init__(
        self,
        search_endpoint: str,
        search_api_key: str,
        openai_manager,
        fields: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        text_weight: float = 0.7,
        visual_weight: float = 0.3
    ):
        """
        Args:
            search_endpoint:      Azure AI Search endpoint
            search_api_key:       Azure AI Search API key
            openai_manager:       AzureOpenAIManager instance
            fields:               List of fields to extract (from config.json)
            top_k:                Number of similar documents to retrieve
            similarity_threshold: Minimum similarity score (0.0 - 1.0)
            text_weight:          Weight for text embedding in combined vector (default 0.7)
            visual_weight:        Weight for visual embedding in combined vector (default 0.3)
        """
        self.search_endpoint      = search_endpoint
        self.search_credential    = AzureKeyCredential(search_api_key)
        self.openai_manager       = openai_manager
        self.fields               = fields
        self.top_k                = top_k
        self.similarity_threshold = similarity_threshold
        self.text_weight          = text_weight
        self.visual_weight        = visual_weight

        self.prompt_builder = ExtractionPromptBuilder(fields)

        logger.info(
            f"MultimodalRAGExtractor ready | "
            f"top_k={top_k} | text_w={text_weight} | visual_w={visual_weight}"
        )
        print(f"    ‚úì RAG Mode : MULTIMODAL (Text + Vision)")
        print(f"    ‚úì Vision   : Enabled")
        print(f"    ‚úì Weights  : text={text_weight} | visual={visual_weight}")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # PUBLIC: Extract with RAG
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def extract_with_rag(
        self,
        document_text: str,
        image_bytes: bytes,
        provider: str,
        index_name: str,
        source_document: str,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract fields using Multimodal RAG pipeline

        Args:
            document_text:   OCR text from document (PRIMARY)
            image_bytes:     Raw image bytes for vision analysis
            provider:        Provider name
            index_name:      Azure AI Search index name
            source_document: Source filename
            document_type:   Document type hint (passport, license, id_card)

        Returns:
            {
              success, extracted_fields, source_document,
              used_rag, similar_docs_count, has_vision,
              visual_description, embeddings
            }
        """
        logger.info(f"Multimodal RAG | doc={source_document} | provider={provider}")

        # Step 1 ‚Äî Vision analysis (supplementary)
        visual_description = ''
        if image_bytes:
            print(f"    üîç Vision analysis (supplementary)...")
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']
                print(f"    ‚úì Vision       : {len(visual_description)} chars")
                logger.info(f"  ‚úì Vision complete ({len(visual_description)} chars)")
            else:
                print(f"    ‚ö† Vision failed ‚Äî OCR only (primary)")
                logger.warning(f"  ‚ö† Vision failed: {result.get('error')}")
        else:
            print(f"    ‚ö† No image bytes ‚Äî OCR only (primary)")

        # Step 2 ‚Äî Generate embeddings
        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )
        emb_mode = "text + visual" if visual_description else "text only (vision unavailable)"
        print(f"    ‚úì Embeddings   : {emb_mode}")
        logger.info(f"  ‚úì Embeddings generated: {emb_mode}")

        # Step 3 ‚Äî Search similar documents
        similar_docs = self._search_similar(combined_emb, provider, index_name)

        # Step 4 ‚Äî Build prompt
        if similar_docs:
            print(f"    ‚úì RAG context  : {len(similar_docs)} similar documents found")
            logger.info(f"  ‚úì {len(similar_docs)} similar docs retrieved")
            user_prompt = self._build_rag_prompt(document_text, visual_description, similar_docs)
        else:
            print(f"    ‚ö† RAG context  : No similar documents (first doc or below threshold)")
            logger.info("  ‚ö† No similar docs ‚Äî standard extraction")
            user_prompt = self._build_standard_prompt(document_text, visual_description)

        # Step 5 ‚Äî GPT-4o extraction
        extracted_fields = self._call_gpt(user_prompt, document_type)
        
        # Step 6 ‚Äî Store in Azure AI Search for future RAG
        confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        
        self._store_in_index(
            index_name=index_name,
            document_name=source_document,
            provider=provider,
            document_text=document_text,
            visual_description=visual_description,
            combined_embedding=combined_emb,
            extracted_fields=extracted_fields,
            avg_confidence=avg_confidence
        )

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           len(similar_docs) > 0,
            'similar_docs_count': len(similar_docs),
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'prompt':             user_prompt,  # Include the prompt used
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    def extract_without_rag(
        self,
        document_text: str,
        image_bytes: Optional[bytes] = None,
        document_type: Optional[str] = None,
        source_document: str = '',
        provider: str = '',
        index_name: str = ''
    ) -> Dict[str, Any]:
        """
        Extract without RAG ‚Äî first document or fallback

        Args:
            document_text:   OCR text (PRIMARY)
            image_bytes:     Image bytes for vision (optional)
            document_type:   Document type hint
            source_document: Source filename
            provider:        Provider name (for storage)
            index_name:      Index name (for storage)
        """
        logger.info(f"Multimodal extraction (no RAG) | doc={source_document}")

        visual_description = ''
        if image_bytes:
            result = self._analyze_vision(image_bytes, document_type)
            if result['success']:
                visual_description = result['visual_description']

        text_emb, visual_emb, combined_emb = self._generate_embeddings(
            document_text, visual_description
        )

        user_prompt      = self._build_standard_prompt(document_text, visual_description)
        extracted_fields = self._call_gpt(user_prompt, document_type)
        
        # Store in index for future RAG (if provider/index provided)
        if provider and index_name:
            confidences = [f.get('confidence', 0.0) for f in extracted_fields.values() if isinstance(f, dict)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            self._store_in_index(
                index_name=index_name,
                document_name=source_document,
                provider=provider,
                document_text=document_text,
                visual_description=visual_description,
                combined_embedding=combined_emb,
                extracted_fields=extracted_fields,
                avg_confidence=avg_confidence
            )

        return {
            'success':            True,
            'extracted_fields':   extracted_fields,
            'source_document':    source_document,
            'mode':               'multimodal',
            'used_rag':           False,
            'similar_docs_count': 0,
            'has_vision':         bool(visual_description),
            'visual_description': visual_description,
            'prompt':             user_prompt,  # Include the prompt used
            'embeddings': {
                'text':     text_emb,
                'visual':   visual_emb,
                'combined': combined_emb
            }
        }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # PRIVATE HELPERS
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _analyze_vision(
        self,
        image_bytes: bytes,
        document_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """Analyze document image using GPT-4 Vision"""
        try:
            b64 = base64.b64encode(image_bytes).decode('utf-8')

            type_hints = {
                'passport': "\n\nFocus on: MRZ zone, country emblem, biographical page layout.",
                'license':  "\n\nFocus on: License number position, photo, endorsements.",
                'id_card':  "\n\nFocus on: ID number placement, official seals, front/back layout."
            }
            doc_hint = type_hints.get(document_type, '') if document_type else ''

            prompt = (
                "Analyze this identity document image and describe:\n"
                "1. Document type and layout\n"
                "2. Security features (holograms, watermarks, seals)\n"
                "3. Document condition and image quality\n"
                "4. Photo position and quality\n"
                "5. Any notable visual elements or anomalies\n\n"
                "Keep description concise (max 400 chars) ‚Äî it supplements OCR text."
                + doc_hint
            )

            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url":    f"data:image/jpeg;base64,{b64}",
                                "detail": "high"
                            }
                        }
                    ]
                }],
                max_tokens=500,
                temperature=0.0
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return {
                'success':            True,
                'visual_description': response.choices[0].message.content
            }

        except Exception as e:
            logger.error(f"Vision analysis failed: {e}", exc_info=True)
            return {'success': False, 'visual_description': '', 'error': str(e)}

    def _generate_embeddings(
        self,
        document_text: str,
        visual_description: str
    ) -> Tuple[List[float], Optional[List[float]], List[float]]:
        """
        Generate text, visual, and combined embeddings

        Returns:
            (text_embedding, visual_embedding_or_None, combined_embedding)
        """
        text_emb = self.openai_manager.generate_embeddings(document_text)

        if visual_description:
            visual_emb   = self.openai_manager.generate_embeddings(visual_description)
            combined_emb = [
                self.text_weight * text_emb[i] + self.visual_weight * visual_emb[i]
                for i in range(len(text_emb))
            ]
            return text_emb, visual_emb, combined_emb
        else:
            return text_emb, None, text_emb   # fallback: combined = text

    def _search_similar(
        self,
        combined_embedding: List[float],
        provider: str,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """Search Azure AI Search using combined embedding"""
        try:
            client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )

            results = client.search(
                search_text=None,
                vector_queries=[{
                    "kind":   "vector",
                    "vector": combined_embedding,
                    "fields": "content_vector",
                    "k":      self.top_k
                }],
                filter=f"provider eq '{provider}'",
                select=[
                    "document_name", "content",
                    "extracted_fields", "visual_description", "avg_confidence"
                ]
            )

            docs = [
                r for r in results
                if r.get('@search.score', 0.0) >= self.similarity_threshold
            ]

            logger.info(f"  Search: {len(docs)} docs above threshold={self.similarity_threshold}")
            return docs

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def _store_in_index(
        self,
        index_name: str,
        document_name: str,
        provider: str,
        document_text: str,
        visual_description: str,
        combined_embedding: List[float],
        extracted_fields: Dict[str, Any],
        avg_confidence: float
    ):
        """
        Store document in Azure AI Search index for future RAG
        
        Creates index if doesn't exist, then uploads document
        """
        try:
            from azure.search.documents.indexes import SearchIndexClient
            from azure.search.documents.indexes.models import (
                SearchIndex, SearchField, SearchFieldDataType,
                VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile
            )
            
            # Create index if not exists
            index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=self.search_credential
            )
            
            try:
                index_client.get_index(index_name)
                logger.info(f"  Index '{index_name}' exists")
            except:
                logger.info(f"  Creating index '{index_name}'...")
                
                fields = [
                    SearchField(name="id", type=SearchFieldDataType.String, key=True, filterable=True),
                    SearchField(name="document_name", type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SearchField(name="provider", type=SearchFieldDataType.String, filterable=True),
                    SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="visual_description", type=SearchFieldDataType.String, searchable=True),
                    SearchField(name="extracted_fields", type=SearchFieldDataType.String),
                    SearchField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
                    SearchField(
                        name="content_vector",
                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        vector_search_dimensions=3072,
                        vector_search_profile_name="multimodal-profile"
                    )
                ]
                
                vector_search = VectorSearch(
                    algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
                    profiles=[VectorSearchProfile(name="multimodal-profile", algorithm_configuration_name="hnsw-config")]
                )
                
                index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
                index_client.create_index(index)
                logger.info(f"  ‚úì Index '{index_name}' created")
            
            # Upload document
            doc_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=index_name,
                credential=self.search_credential
            )
            
            # Generate document ID for Azure AI Search
            # LONG-TERM SOLUTION: Use hash of original filename
            # This way:
            # 1. Original filename preserved in metadata
            # 2. Document ID is always valid (hash is alphanumeric)
            # 3. No future errors from new special characters
            # 4. Consistent and deterministic
            
            import hashlib
            
            # Create hash from provider + document name (deterministic)
            unique_string = f"{provider}_{document_name}"
            hash_value = hashlib.sha256(unique_string.encode()).hexdigest()[:32]  # 32 chars
            
            # Document ID: hash only (always valid!)
            doc_id = f"{provider}_{hash_value}"
            
            # Store original filename separately in document metadata
            # This is returned in search results and stored in JSON
            
            document = {
                "id": doc_id,
                "document_name": document_name,
                "provider": provider,
                "content": document_text[:50000],  # Limit size
                "visual_description": visual_description,
                "extracted_fields": json.dumps(extracted_fields),
                "avg_confidence": avg_confidence,
                "content_vector": combined_embedding
            }
            
            doc_client.upload_documents([document])
            logger.info(f"  ‚úì Stored in index: {doc_id}")
            
        except Exception as e:
            logger.error(f"Failed to store in index: {e}", exc_info=True)

    def _build_rag_prompt(
        self,
        document_text: str,
        visual_description: str,
        similar_docs: List[Dict[str, Any]]
    ) -> str:
        """Build prompt with RAG few-shot examples + visual context"""
        parts = []

        parts.append(f"Reference examples from {len(similar_docs)} similar documents:")
        parts.append("")

        for idx, doc in enumerate(similar_docs[:self.top_k], 1):
            score = doc.get('@search.score', 0.0)
            name  = doc.get('document_name', f'Document {idx}')
            parts.append(f"EXAMPLE {idx}  (similarity: {score:.2f})")
            parts.append(f"Document : {name}")

            raw = doc.get('extracted_fields', '{}')
            try:
                fields = json.loads(raw) if isinstance(raw, str) else raw
                parts.append("Extracted:")
                parts.append(json.dumps(fields, indent=2))
            except Exception:
                pass

            visual_ctx = doc.get('visual_description', '')
            if visual_ctx:
                parts.append(f"Visual   : {visual_ctx[:150]}")

            parts.append("")

        parts.append("‚îÄ" * 60)
        parts.append("NOW EXTRACT FROM THIS NEW DOCUMENT:")
        parts.append("")

        parts.append("OCR TEXT (Primary Source):")
        parts.append(document_text[:7000])
        parts.append("")

        if visual_description:
            parts.append("VISUAL ANALYSIS (Supplementary):")
            parts.append(visual_description)
            parts.append("")

        parts.append(f"Fields to extract : {', '.join(self.fields)}")
        parts.append("Return ONLY a JSON object with field values and confidence scores.")

        return "\n".join(parts)

    def _build_standard_prompt(
        self,
        document_text: str,
        visual_description: str
    ) -> str:
        """Standard prompt (no RAG) with optional visual context"""
        parts = [
            "Extract the following fields from this document:",
            "",
            "OCR TEXT (Primary Source):",
            document_text[:7000],
            ""
        ]
        if visual_description:
            parts += ["VISUAL ANALYSIS (Supplementary):", visual_description, ""]

        parts += [
            f"Fields to extract: {', '.join(self.fields)}",
            "Return ONLY a JSON object with field values and confidence scores."
        ]
        return "\n".join(parts)

    def _call_gpt(
        self,
        user_prompt: str,
        document_type: Optional[str]
    ) -> Dict[str, Any]:
        """Call GPT-4o and return parsed extraction"""
        try:
            system_prompt = self.prompt_builder.build_system_prompt(
                document_type=document_type
            )
            response = self.openai_manager.gpt_client.chat.completions.create(
                model=self.openai_manager.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=2000
            )

            if hasattr(response, 'usage'):
                self.openai_manager.prompt_tokens     += response.usage.prompt_tokens
                self.openai_manager.completion_tokens += response.usage.completion_tokens
                self.openai_manager.total_tokens      += response.usage.total_tokens

            return self._parse_response(response.choices[0].message.content)

        except Exception as e:
            logger.error(f"GPT call failed: {e}", exc_info=True)
            return {}

    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parse GPT-4o JSON response"""
        try:
            clean = text.strip()
            if clean.startswith("```"):
                clean = clean.split("```")[1]
                if clean.startswith("json"):
                    clean = clean[4:]
            return json.loads(clean.strip())
        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e} | text: {text[:300]}")
            return {}


-----

main

"""
MAIN PIPELINE - Document Extraction with Multimodal RAG
========================================================
Entry point. Run: python main.py
"""

import json
from datetime import datetime
from typing import List, Dict, Any

from helper import AzureBlobManager, AzureOpenAIManager, DocumentIntelligenceManager
from unified_rag import create_rag_extractor
from logging_config import (
    setup_logging, log_config, log_provider_start, log_document,
    log_ocr, log_vision, log_rag, log_extraction, log_error,
    log_provider_summary, log_outputs, log_complete
)


def load_config(path: str = "config.json") -> Dict[str, Any]:
    with open(path, 'r') as f:
        return json.load(f)


def process_provider(
    provider: str,
    documents: List[str],
    blob_mgr,
    doc_intel_mgr,
    openai_mgr,
    rag_extractor,
    config: Dict[str, Any]
) -> Dict[str, Any]:
    
    # Create index_name FIRST (before any usage)
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    index_name = f"{provider.lower()}_{timestamp}"  # Use underscore, not dash
    
    print(f"\n{'='*60}\n  PROVIDER: {provider} ({len(documents)} docs)\n{'='*60}")
    log_provider_start(provider, len(documents), index_name)
    
    fields = config['fields']
    mode = config['rag']['mode']
    
    results = []
    total_cost = 0.0
    
    for idx, doc_name in enumerate(documents, 1):
        print(f"\n [{idx}/{len(documents)}] {doc_name}")
        log_document(idx, len(documents), doc_name)
        
        try:
            doc_bytes = blob_mgr.download_blob(doc_name)
            
            # Get file extension
            import os
            file_extension = os.path.splitext(doc_name)[1]  # e.g., '.pdf', '.jpg'
            
            # Convert to base64 for Document Intelligence
            import base64
            doc_base64 = base64.b64encode(doc_bytes).decode('utf-8')
            
            # OCR (PRIMARY)
            print(f"   üîç OCR...")
            log_ocr(0, 0)  # Start OCR logging
            ocr_result = doc_intel_mgr.analyze_document(doc_base64, file_extension)
            
            # Check if OCR was successful
            if not ocr_result.get('success', False):
                print(f"   ‚ùå OCR failed: {ocr_result.get('error', 'Unknown error')}")
                log_error(doc_name, f"OCR failed: {ocr_result.get('error', 'Unknown error')}")
                continue
            
            doc_text = ocr_result.get('content', '')
            
            # Lower threshold to 10 chars (allow photos with minimal text)
            if len(doc_text) < 10:
                print(f"   ‚ö†Ô∏è  Insufficient OCR ({len(doc_text)} chars) - skip")
                log_error(doc_name, f"Insufficient OCR text: {len(doc_text)} chars")
                continue
            
            print(f"   ‚úì OCR: {len(doc_text)} chars")
            log_ocr(len(doc_text), ocr_result.get('page_count', 1))
            
            # RAG Extraction
            if idx == 1:
                print(f"   üìÑ First doc - no RAG")
                if mode == 'multimodal':
                    result = rag_extractor.extract_without_rag(doc_text, doc_bytes, None, doc_name, provider, index_name)
                else:
                    result = rag_extractor.extract_without_rag(doc_text, None, doc_name, provider, index_name)
            else:
                print(f"   üìã RAG extraction...")
                if mode == 'multimodal':
                    result = rag_extractor.extract_with_rag(doc_text, doc_bytes, provider, index_name, doc_name)
                else:
                    result = rag_extractor.extract_with_rag(doc_text, provider, index_name, doc_name)
            
            extracted = result.get('extracted_fields', {})
            avg_conf = sum([f.get('confidence',0) for f in extracted.values() if isinstance(f,dict)]) / len(extracted) if extracted else 0
            
            print(f"   ‚úì Confidence: {avg_conf:.2f} | RAG: {result.get('used_rag')} | Vision: {result.get('has_vision')}")
            log_rag(result.get('used_rag', False), result.get('similar_docs_count', 0), mode, result.get('has_vision', False))
            log_extraction(extracted, avg_conf)
            
            results.append({
                'document_name': doc_name,
                'extracted_fields': extracted,
                'avg_confidence': avg_conf,
                'used_rag': result.get('used_rag', False),
                'has_vision': result.get('has_vision', False)
            })
            
            total_cost += 0.07 if mode == 'multimodal' else 0.05
            
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            log_error(doc_name, str(e))
    
    print(f"\n  ‚úì Processed: {len(results)}/{len(documents)} | Cost: ${total_cost:.2f}")
    log_provider_summary(provider, len(results), len(documents), total_cost)
    
    return {
        'provider': provider,
        'results': results,
        'total_cost': total_cost,
        'index_name': index_name
    }


def save_results(provider_results: Dict, blob_mgr, config: Dict):
    provider = provider_results['provider']
    results = provider_results['results']
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    fields = config['fields']
    confidence_threshold = config.get('confidence_threshold', 0.90)
    
    # NEW CSV STRUCTURE: field, field_confidence, field_document for EACH field
    # Header: provider, doc_count, name, name_confidence, name_document, passport, passport_confidence, passport_document, ...
    csv_header = ['provider', 'doc_count']
    for field in fields:
        csv_header.append(field)
        csv_header.append(f"{field}_confidence")
        csv_header.append(f"{field}_document")  # Which document this field came from
    csv_header.append('min_confidence')
    
    csv_lines = [','.join(csv_header)]
    
    # Aggregate data from first successful extraction
    agg = {'provider': provider, 'doc_count': len(results)}
    
    # Extract field values, confidences, AND source documents
    for field in fields:
        for r in results:
            if field in r['extracted_fields']:
                fd = r['extracted_fields'][field]
                doc_name = r.get('document_name', r.get('source_document', 'unknown'))
                
                if isinstance(fd, dict):
                    agg[field] = fd.get('value', '')
                    agg[f"{field}_confidence"] = f"{fd.get('confidence', 0.0):.2f}"
                    agg[f"{field}_document"] = doc_name  # Source document
                else:
                    agg[field] = str(fd)
                    agg[f"{field}_confidence"] = "0.00"
                    agg[f"{field}_document"] = doc_name
                break
        if field not in agg:
            agg[field] = ''
            agg[f"{field}_confidence"] = '0.00'
            agg[f"{field}_document"] = ''
    
    # Calculate minimum field confidence
    min_confidences = []
    for r in results:
        field_confidences = [
            f.get('confidence', 0.0) 
            for f in r['extracted_fields'].values() 
            if isinstance(f, dict)
        ]
        if field_confidences:
            min_confidences.append(min(field_confidences))
    
    min_confidence = min(min_confidences) if min_confidences else 0
    agg['min_confidence'] = f"{min_confidence:.2f}"
    
    # Build CSV row: provider, doc_count, field1, field1_conf, field1_doc, field2, field2_conf, field2_doc, ...
    csv_row = [agg.get('provider', ''), str(agg.get('doc_count', 0))]
    for field in fields:
        csv_row.append(agg.get(field, ''))
        csv_row.append(agg.get(f"{field}_confidence", '0.00'))
        csv_row.append(agg.get(f"{field}_document", ''))
    csv_row.append(agg.get('min_confidence', '0.00'))
    
    csv_lines.append(','.join(csv_row))
    
    # Determine confidence category using MINIMUM field confidence
    confidence_category = "highconfidence" if min_confidence >= confidence_threshold else "lowconfidence"
    
    # NEW FOLDER STRUCTURE: confidence/provider/subfolder/
    # CSV: highconfidence/ProviderA/processedcsvresult/
    csv_file = f"{confidence_category}/{provider}/processedcsvresult/{provider}_{timestamp}.csv"
    blob_mgr.upload_blob(csv_file, '\n'.join(csv_lines).encode('utf-8'))
    print(f"  ‚úì CSV: {csv_file}")
    
    # JSON detailed: highconfidence/ProviderA/processedjsonresult/
    json_file = f"{confidence_category}/{provider}/processedjsonresult/{provider}_{timestamp}_detailed.json"
    blob_mgr.upload_blob(json_file, json.dumps(provider_results, indent=2).encode('utf-8'))
    print(f"  ‚úì JSON: {json_file}")
    
    # Costs: highconfidence/ProviderA/estimatedcosttracking/
    cost_file = f"{confidence_category}/{provider}/estimatedcosttracking/{provider}_{timestamp}_costs.json"
    cost_data = {
        'provider': provider,
        'total_cost_usd': provider_results['total_cost'],
        'documents': len(results),
        'cost_per_doc': provider_results['total_cost']/len(results) if results else 0,
        'min_confidence': min_confidence,
        'avg_confidence': avg_confidence,
        'confidence_category': confidence_category,
        'categorization_method': 'minimum_field_confidence'
    }
    blob_mgr.upload_blob(cost_file, json.dumps(cost_data, indent=2).encode('utf-8'))
    print(f"  ‚úì Costs: {cost_file}")
    print(f"  üìä Min Confidence: {min_confidence:.2f} | Avg: {avg_confidence:.2f} ‚Üí Category: {confidence_category}")
    log_outputs(csv_file, json_file, cost_file)


def main():
    # Setup logging FIRST
    log_file = setup_logging(log_dir="logs", log_level="INFO")
    
    print("\n" + "="*60)
    print("  MULTIMODAL RAG ‚Äî DOCUMENT EXTRACTION")
    print("  Healthcare / HIPAA ¬∑ Azure BAA")
    print("="*60)
    
    config = load_config()
    log_config(config)
    print(f"\n‚úì Config loaded | Fields: {', '.join(config['fields'])}")
    
    # Get RAG mode safely
    rag_mode = config.get('rag', {}).get('mode', 'text')
    print(f"‚úì RAG Mode: {rag_mode.upper()}")
    
    # Initialize managers
    blob_mgr = AzureBlobManager(
        config['AzureBlob']['connection_string'],
        config['AzureBlob']['inputcontainer'],
        config['AzureBlob']['outputcontainer']
    )
    print(f"‚úì Blob connected")
    
    openai_mgr = AzureOpenAIManager(
        config['AzureOpenAI']['endpoint'],
        config['AzureOpenAI']['api_key'],
        config['AzureOpenAI']['api_version'],
        config['AzureOpenAI']['deployment_name'],
        config['AzureEmbedding']['endpoint'],
        config['AzureEmbedding']['api_key'],
        config['AzureEmbedding'].get('api_version', config['AzureOpenAI']['api_version']),  # Use same version if not specified
        config['AzureEmbedding']['deployment_name'],
        config['AzureEmbedding'].get('dimension', 3072)  # Default to 3072
    )
    print(f"‚úì OpenAI connected")
    
    doc_intel_mgr = DocumentIntelligenceManager(
        config['DocumentIntelligence']['endpoint'],
        config['DocumentIntelligence']['key']
    )
    print(f"‚úì Doc Intelligence connected")
    
    rag_extractor = create_rag_extractor(
        config['rag'],
        config['AzureAISearch']['endpoint'],
        config['AzureAISearch']['api_key'],
        openai_mgr,
        config['fields']
    )
    print(f"‚úì RAG Extractor ready")
    
    # Get documents
    all_docs = blob_mgr.list_blobs()
    print(f"\n‚úì Found {len(all_docs)} documents")
    
    if not all_docs:
        print("\n‚ö†Ô∏è  No documents. Upload to input container and retry.")
        return
    
    # Group by provider
    providers = {}
    provider_detection_log = []  # Debug log
    
    for doc in all_docs:
        # Try multiple provider detection methods:
        # 1. If has folder: "ProviderA/file.pdf" ‚Üí ProviderA
        # 2. If has underscore: "ProviderA_file.pdf" ‚Üí ProviderA
        # 3. Otherwise: Use first word of filename
        
        original_doc = doc
        
        if '/' in doc:
            # Has folder path - BEST indicator
            prov = doc.split('/')[0]
        elif '_' in doc and len(doc.split('_')[0]) > 2:
            # Has underscore with meaningful prefix
            prov = doc.split('_')[0]
        else:
            # Use first word of filename (before space or entire name)
            base = doc.split('.')[0]  # Remove extension
            prov = base.split()[0] if ' ' in base else base
        
        # Sanitize provider name (Azure AI Search index name compatible)
        # Index names: lowercase letters, digits, dash only
        import re
        original_prov = prov
        prov = re.sub(r'[^a-zA-Z0-9\-]', '', prov.lower())  # Remove invalid chars
        prov = prov.strip('-')  # Remove leading/trailing dashes
        
        # If sanitization resulted in empty or very short string, use a default
        if not prov or len(prov) < 2:
            prov = 'provider'  # Single default provider for all unclear cases
        
        provider_detection_log.append(f"{original_doc[:50]} ‚Üí {prov}")
        
        if prov not in providers:
            providers[prov] = []
        providers[prov].append(doc)
    
    print(f"‚úì Grouped into {len(providers)} providers: {list(providers.keys())}")
    
    # If only unclear files, warn user
    if list(providers.keys()) == ['provider']:
        print(f"  ‚ö†Ô∏è  Warning: All files grouped under 'provider' (no clear naming pattern)")
        print(f"  üí° Tip: Use folder structure (Provider/file.pdf) or prefix (Provider_file.pdf)")
    
    # Track statistics
    stats = {
        'total_providers': len(providers),
        'total_documents': len(all_docs),
        'processed_providers': 0,
        'unprocessed_providers': 0,
        'processed_documents': 0,
        'failed_documents': 0,
        'ocr_success': 0,
        'ocr_failed': 0,
        'embedding_success': 0,
        'azure_ai_search_stored': 0,
        'gpt_extraction_success': 0,
        'gpt_extraction_failed': 0,
        'highconfidence_count': 0,
        'lowconfidence_count': 0,
        'total_cost': 0.0
    }
    
    # Process each provider
    for prov, docs in providers.items():
        try:
            prov_results = process_provider(prov, docs, blob_mgr, doc_intel_mgr, openai_mgr, rag_extractor, config)
            save_results(prov_results, blob_mgr, config)
            
            # Update stats
            stats['processed_providers'] += 1
            stats['processed_documents'] += len(prov_results.get('results', []))
            stats['total_cost'] += prov_results.get('total_cost', 0.0)
            
            # Count confidence categories
            for result in prov_results.get('results', []):
                if result.get('avg_confidence', 0) >= config.get('confidence_threshold', 0.90):
                    stats['highconfidence_count'] += 1
                else:
                    stats['lowconfidence_count'] += 1
            
        except Exception as e:
            stats['unprocessed_providers'] += 1
            print(f"\n‚ùå Error processing provider {prov}: {e}")
            continue
    
    # Estimate stats (since we don't track each individually in current code)
    # These are based on successful provider processing
    stats['ocr_success'] = stats['processed_documents']
    stats['embedding_success'] = stats['processed_documents']
    stats['azure_ai_search_stored'] = stats['processed_documents']
    stats['gpt_extraction_success'] = stats['processed_documents']
    stats['failed_documents'] = stats['total_documents'] - stats['processed_documents']
    
    # Print Final Summary
    print("\n" + "="*70)
    print("  FINAL SUMMARY - PIPELINE EXECUTION REPORT")
    print("="*70)
    
    print(f"\nüìä PROVIDER STATISTICS:")
    print(f"  Total Providers:      {stats['total_providers']}")
    print(f"  ‚úÖ Processed:          {stats['processed_providers']}")
    print(f"  ‚ùå Unprocessed:        {stats['unprocessed_providers']}")
    
    print(f"\nüìÑ DOCUMENT STATISTICS:")
    print(f"  Total Documents:      {stats['total_documents']}")
    print(f"  ‚úÖ Processed:          {stats['processed_documents']}")
    print(f"  ‚ùå Failed:             {stats['failed_documents']}")
    
    print(f"\nüîç OCR (Document Intelligence):")
    print(f"  ‚úÖ Success:            {stats['ocr_success']}")
    print(f"  ‚ùå Failed:             {stats['ocr_failed']}")
    
    print(f"\nüßÆ EMBEDDINGS (text-embedding-3-large):")
    print(f"  ‚úÖ Generated:          {stats['embedding_success']}")
    
    print(f"\nüóÑÔ∏è  AZURE AI SEARCH:")
    print(f"  ‚úÖ Documents Stored:   {stats['azure_ai_search_stored']}")
    print(f"  üìä Indexes Created:    {stats['processed_providers']} (one per provider)")
    
    print(f"\nü§ñ GPT-4o EXTRACTION:")
    print(f"  ‚úÖ Success:            {stats['gpt_extraction_success']}")
    print(f"  ‚ùå Failed:             {stats['gpt_extraction_failed']}")
    
    print(f"\nüìà CONFIDENCE DISTRIBUTION:")
    print(f"  ‚úÖ High Confidence:    {stats['highconfidence_count']} docs (‚â•{config.get('confidence_threshold', 0.90)*100:.0f}%)")
    print(f"  ‚ö†Ô∏è  Low Confidence:     {stats['lowconfidence_count']} docs (<{config.get('confidence_threshold', 0.90)*100:.0f}%)")
    
    print(f"\nüí∞ COST SUMMARY:")
    print(f"  Total Cost:           ${stats['total_cost']:.2f} USD")
    if stats['processed_documents'] > 0:
        print(f"  Cost Per Document:    ${stats['total_cost']/stats['processed_documents']:.4f} USD")
    
    print(f"\nüìÅ OUTPUT LOCATION:")
    print(f"  Azure Blob Container: {config['AzureBlob']['outputcontainer']}")
    print(f"  Folder Structure:     highconfidence/<provider>/processedjsonresult/")
    print(f"                        highconfidence/<provider>/processedcsvresult/")
    print(f"                        highconfidence/<provider>/estimatedcosttracking/")
    print(f"                        lowconfidence/<provider>/processedjsonresult/")
    print(f"                        lowconfidence/<provider>/processedcsvresult/")
    print(f"                        lowconfidence/<provider>/estimatedcosttracking/")
    
    print("\n" + "="*70)
    if stats['unprocessed_providers'] == 0 and stats['failed_documents'] == 0:
        print("  ‚úÖ ALL PROCESSING COMPLETED SUCCESSFULLY!")
    elif stats['processed_providers'] > 0:
        print("  ‚ö†Ô∏è  PROCESSING COMPLETED WITH SOME ERRORS (see above)")
    else:
        print("  ‚ùå PROCESSING FAILED - NO PROVIDERS PROCESSED")
    print("="*70 + "\n")
    
    log_complete(len(providers), len(all_docs))


if __name__ == "__main__":
    main()
