import os
import json
import base64
import logging
import hashlib
from datetime import datetime
from io import BytesIO
from typing import List, Dict, Any, Tuple

import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential

from azure.storage.blob import BlobServiceClient, ContentSettings
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchableField, SearchField, SearchFieldDataType,  # ADDED SearchField
    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,
    SemanticConfiguration, SemanticField, SemanticPrioritizedFields, SemanticSearch
)
from openai import AzureOpenAI

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)


class ConfigManager:
    def __init__(self, config_path: str = 'config.json'):
        self.config_path = config_path
        self.config = self.load_config()

    def load_config(self) -> Dict[str, Any]:
        with open(self.config_path, 'r') as f:
            cfg = json.load(f)
        return cfg

    def get(self, section: str, key: str = None) -> Any:
        if key:
            return self.config.get(section, {}).get(key)
        return self.config.get(section)


class AzureBlobManager:
    def __init__(self, connection_string: str, input_container: str, output_container: str):
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self.input_container = input_container
        self.output_container = output_container
        self.supported_extensions = {'.pdf', '.doc', '.docx', '.png', '.jpg', '.jpeg', '.tiff', '.tif'}

    def get_providers(self) -> List[str]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs()
        providers = set()
        for blob in blobs:
            parts = blob.name.split('/')
            if parts[0]:
                providers.add(parts[0])
        return sorted(list(providers))
    
    def list_blobs(self) -> List[str]:
        """List all blobs in input container (for main.py compatibility)"""
        container_client = self.blob_service_client.get_container_client(self.input_container)
        return [blob.name for blob in container_client.list_blobs()]
    
    def download_blob(self, blob_name: str) -> bytes:
        """Download blob as bytes (for main.py compatibility)"""
        blob_client = self.blob_service_client.get_blob_client(self.input_container, blob_name)
        return blob_client.download_blob().readall()
    
    def upload_blob(self, blob_name: str, data: bytes, overwrite: bool = True):
        """Upload blob to output container (for main.py compatibility)"""
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_name)
        blob_client.upload_blob(data, overwrite=overwrite)

    def get_provider_files(self, provider: str) -> List[Dict[str, str]]:
        container_client = self.blob_service_client.get_container_client(self.input_container)
        blobs = container_client.list_blobs(name_starts_with=f"{provider}/")
        files = []
        for blob in blobs:
            ext = os.path.splitext(blob.name)[1].lower()
            if ext in self.supported_extensions:
                files.append({
                    'name': blob.name,
                    'filename': os.path.basename(blob.name),
                    'provider': provider,
                    'size': blob.size,
                    'extension': ext
                })
        return files

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def download_blob_as_base64(self, blob_name: str) -> str:
        blob_client = self.blob_service_client.get_blob_client(self.input_container, blob_name)
        data = blob_client.download_blob().readall()
        return base64.b64encode(data).decode('utf-8')

    def upload_to_blob(self, data, blob_path: str, content_type: str = 'text/plain'):
        """Upload data to blob storage"""
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        content_settings = ContentSettings(content_type=content_type)
        if isinstance(data, str):
            data = data.encode('utf-8')
        blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
        logger.info(f"Uploaded to blob: {blob_path}")

    def upload_dataframe_as_csv(self, df: pd.DataFrame, blob_path: str):
        csv_buffer = BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_data = csv_buffer.getvalue()
        blob_client = self.blob_service_client.get_blob_client(self.output_container, blob_path)
        content_settings = ContentSettings(content_type='text/csv')
        blob_client.upload_blob(csv_data, overwrite=True, content_settings=content_settings)
        logger.info(f"Uploaded CSV to blob: {blob_path}")


class DocumentIntelligenceManager:
    def __init__(self, endpoint: str, key: str):
        self.client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def analyze_document(self, base64_data: str, file_extension: str) -> Dict[str, Any]:
        try:
            # Handle .doc and .docx files - convert to PDF first
            if file_extension.lower() in ['.doc', '.docx']:
                logger.info(f"Converting {file_extension} to PDF for OCR processing")
                try:
                    from docx2pdf import convert as docx_to_pdf_convert
                    import tempfile
                    
                    # Decode base64 to bytes
                    document_bytes = base64.b64decode(base64_data)
                    
                    # Create temporary files
                    with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as temp_word:
                        temp_word.write(document_bytes)
                        temp_word_path = temp_word.name
                    
                    temp_pdf_path = temp_word_path.replace(file_extension, '.pdf')
                    
                    # Convert to PDF
                    docx_to_pdf_convert(temp_word_path, temp_pdf_path)
                    
                    # Read PDF and convert back to base64
                    with open(temp_pdf_path, 'rb') as pdf_file:
                        pdf_bytes = pdf_file.read()
                        base64_data = base64.b64encode(pdf_bytes).decode('utf-8')
                    
                    # Clean up temp files
                    os.remove(temp_word_path)
                    os.remove(temp_pdf_path)
                    
                    # Process as PDF
                    file_extension = '.pdf'
                    logger.info(f"Successfully converted to PDF")
                    
                except ImportError:
                    logger.warning("docx2pdf not installed, trying alternative method with python-docx")
                    # Alternative: Extract text directly from Word document
                    try:
                        from docx import Document
                        import io
                        
                        document_bytes = base64.b64decode(base64_data)
                        doc = Document(io.BytesIO(document_bytes))
                        
                        text = ''
                        page_count = 1  # Word doesn't have pages in same way, estimate
                        
                        for para in doc.paragraphs:
                            text += para.text + "\n"
                        
                        # Also extract text from tables
                        for table in doc.tables:
                            for row in table.rows:
                                for cell in row.cells:
                                    text += cell.text + " "
                                text += "\n"
                        
                        logger.info(f"Extracted text from Word document: {len(text)} characters")
                        
                        return {
                            'success': True,
                            'text': text.strip(),
                            'page_count': max(1, len(text) // 3000)  # Estimate pages
                        }
                    except Exception as e:
                        logger.error(f"Failed to extract text from Word document: {e}")
                        return {
                            'success': False,
                            'text': '',
                            'page_count': 0,
                            'error': f'Word document processing failed: {str(e)}'
                        }
            
            # Standard processing for PDF and images
            content_type_map = {
                '.pdf': 'application/pdf',
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.tiff': 'image/tiff',
                '.tif': 'image/tiff',
                '.bmp': 'image/bmp'
            }
            content_type = content_type_map.get(file_extension.lower(), 'application/pdf')
            document_bytes = base64.b64decode(base64_data)
            
            logger.info(f"Starting OCR for {file_extension}, size: {len(document_bytes)} bytes")
            
            poller = self.client.begin_analyze_document(
                model_id="prebuilt-read",
                analyze_request=document_bytes,
                content_type=content_type
            )
            
            result = poller.result()
            text = ''
            page_count = 0
            
            if hasattr(result, 'pages') and result.pages:
                page_count = len(result.pages)
                for page in result.pages:
                    if hasattr(page, 'lines') and page.lines:
                        for line in page.lines:
                            if hasattr(line, 'content'):
                                text += line.content + "\n"
            
            logger.info(f"OCR completed: {page_count} pages, {len(text)} characters")
            
            return {
                'success': True,
                'text': text.strip(),
                'page_count': page_count
            }
            
        except Exception as e:
            logger.error(f"OCR failed: {e}", exc_info=True)
            return {
                'success': False,
                'text': '',
                'page_count': 0,
                'error': str(e)
            }


class AzureOpenAIManager:
    """Manages both GPT extraction and embeddings with separate clients"""
    
    def __init__(self, gpt_endpoint: str, gpt_api_key: str, gpt_api_version: str, gpt_deployment: str,
                 embedding_endpoint: str, embedding_api_key: str, embedding_api_version: str, 
                 embedding_deployment: str, embedding_dimension: int = 3072):
        
        # GPT-4o client for field extraction
        self.gpt_client = AzureOpenAI(
            azure_endpoint=gpt_endpoint,
            api_key=gpt_api_key,
            api_version=gpt_api_version
        )
        self.gpt_deployment = gpt_deployment
        
        # Separate embedding client (important for separate deployment)
        self.embedding_client = AzureOpenAI(
            azure_endpoint=embedding_endpoint,
            api_key=embedding_api_key,
            api_version=embedding_api_version
        )
        self.embedding_deployment = embedding_deployment
        self.embedding_dimension = embedding_dimension
        
        # Token tracking
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def extract_fields(self, text: str, fields: List[str], source_document: str) -> Dict[str, Any]:
        try:
            system_prompt = f"""You are a document extraction expert. Extract the following fields: {', '.join(fields)}.

Return ONLY a JSON object with this structure:
{{
    "field_name": {{"value": "extracted_value", "confidence": 0.95}},
    ...
}}

Be precise with confidence scores."""
            
            user_prompt = f"Document text:\n\n{text[:8000]}"
            
            response = self.gpt_client.chat.completions.create(
                model=self.gpt_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                max_tokens=2000
            )
            
            self.total_tokens += response.usage.total_tokens
            self.prompt_tokens += response.usage.prompt_tokens
            self.completion_tokens += response.usage.completion_tokens

            content = response.choices[0].message.content.strip()
            
            # Remove markdown code blocks
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()
            
            data = json.loads(content)
            
            # Normalize data format
            normalized_data = {}
            for field_name in data:
                field_value = data[field_name]
                
                if isinstance(field_value, dict) and 'value' in field_value:
                    normalized_data[field_name] = field_value
                    normalized_data[field_name]['source_document'] = source_document
                else:
                    # Wrap direct values
                    normalized_data[field_name] = {
                        'value': str(field_value),
                        'confidence': 0.5,
                        'source_document': source_document
                    }
            
            logger.info(f"Extracted {len(normalized_data)} fields successfully")
            
            return {
                'success': True,
                'extracted_fields': normalized_data,
                'raw_response': content
            }
            
        except Exception as e:
            logger.error(f"Field extraction failed: {e}", exc_info=True)
            return {
                'success': False,
                'extracted_fields': {},
                'raw_response': '',
                'error': str(e)
            }

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
    def generate_embeddings(self, text: str) -> List[float]:
        """Generate embeddings using separate embedding deployment"""
        try:
            # Validate input
            if not text or not isinstance(text, str):
                logger.warning("Invalid text for embeddings, using placeholder")
                text = "No content available"
            
            text = str(text).strip()
            
            if len(text) < 3:
                logger.warning("Text too short, using placeholder")
                text = "No content available"
            
            # Truncate if needed
            if len(text) > 30000:
                text = text[:30000]
                logger.info("Text truncated for embedding")
            
            # Call embedding endpoint (separate from GPT)
            response = self.embedding_client.embeddings.create(
                model=self.embedding_deployment,
                input=text
            )
            
            embeddings = response.data[0].embedding
            logger.info(f"Generated embeddings: dimension={len(embeddings)}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}", exc_info=True)
            # Return zero vector as fallback
            logger.warning(f"Returning zero vector of dimension {self.embedding_dimension}")
            return [0.0] * self.embedding_dimension

    def get_token_usage(self) -> Dict[str, int]:
        return {
            'total_tokens': self.total_tokens,
            'prompt_tokens': self.prompt_tokens,
            'completion_tokens': self.completion_tokens
        }

    def calculate_cost(self, costs_config: Dict[str, float]) -> Dict[str, float]:
        input_cost = (self.prompt_tokens / 1000) * costs_config.get('gpt4o_input_per_1k', 0)
        output_cost = (self.completion_tokens / 1000) * costs_config.get('gpt4o_output_per_1k', 0)
        total_cost = input_cost + output_cost
        return {
            'input_cost': input_cost,
            'output_cost': output_cost,
            'total_cost': total_cost
        }


class AzureAISearchManager:
    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.credential = AzureKeyCredential(api_key)
        self.index_client = SearchIndexClient(endpoint=endpoint, credential=self.credential)

    def get_index_name(self, provider: str, timestamp: str = None) -> str:
        """
        Sanitize provider name for Azure AI Search index
        
        If timestamp provided: Returns providername_timestamp (e.g., "anand_20240211_143022")
        If no timestamp: Returns providername only (e.g., "anand")
        
        Args:
            provider: Provider name
            timestamp: Optional timestamp string (YYYYMMDD_HHMMSS)
        
        Returns:
            Sanitized index name
        """
        # Sanitize provider name
        sanitized = provider.lower().replace(' ', '-').replace('_', '-')
        sanitized = ''.join(c for c in sanitized if c.isalnum() or c == '-')
        sanitized = sanitized.strip('-')
        
        # Add timestamp if provided (for unique index per run)
        if timestamp:
            # Replace underscores with hyphens for index name
            clean_timestamp = timestamp.replace('_', '')  # Remove underscores: 20240211143022
            index_name = f"{sanitized}-{clean_timestamp}"
        else:
            index_name = sanitized
        
        return index_name

    def create_index(self, provider: str, embedding_dimension: int = 3072, timestamp: str = None):
        """
        Create search index with vector field and all required fields
        
        Args:
            provider: Provider name
            embedding_dimension: Vector dimension (default 3072)
            timestamp: Optional timestamp for unique index name (YYYYMMDD_HHMMSS)
        """
        index_name = self.get_index_name(provider, timestamp)
        
        logger.info(f"Creating index '{index_name}' with vector dimension {embedding_dimension}")
        
        # Define fields including vector field using SearchField
        fields = [
            SimpleField(name="id", type=SearchFieldDataType.String, key=True),
            SearchableField(name="provider_id", type=SearchFieldDataType.String, filterable=True, sortable=True),
            SearchableField(name="provider", type=SearchFieldDataType.String, filterable=True),
            SearchableField(name="document_name", type=SearchFieldDataType.String, filterable=True),
            SimpleField(name="document_type", type=SearchFieldDataType.String, filterable=True),
            SimpleField(name="file_extension", type=SearchFieldDataType.String, filterable=True),
            SearchableField(name="content", type=SearchFieldDataType.String),
            SimpleField(name="page_count", type=SearchFieldDataType.Int32, filterable=True),
            SimpleField(name="total_documents", type=SearchFieldDataType.Int32, filterable=True),
            SimpleField(name="extraction_datetime", type=SearchFieldDataType.String, sortable=True),
            SearchableField(name="extracted_fields", type=SearchFieldDataType.String),
            SimpleField(name="avg_confidence", type=SearchFieldDataType.Double, filterable=True, sortable=True),
            # MULTIMODAL FIELDS
            SearchableField(name="visual_description", type=SearchFieldDataType.String),
            SimpleField(name="is_multimodal", type=SearchFieldDataType.Boolean, filterable=True),
            SimpleField(name="modality", type=SearchFieldDataType.String, filterable=True),
            # CRITICAL: Use SearchField for vector field
            SearchField(
                name="content_vector",
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                searchable=True,
                vector_search_dimensions=embedding_dimension,
                vector_search_profile_name="vector-profile"
            )
        ]
        
        # Vector search configuration
        vector_search = VectorSearch(
            algorithms=[HnswAlgorithmConfiguration(name="hnsw-config")],
            profiles=[VectorSearchProfile(name="vector-profile", algorithm_configuration_name="hnsw-config")]
        )
        
        # Create index
        index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)
        self.index_client.create_or_update_index(index)
        
        logger.info(f"Index '{index_name}' created successfully with multimodal fields")
        return index_name

    def upload_documents(self, provider: str, documents: List[Dict[str, Any]], timestamp: str = None):
        """
        Upload documents to search index
        
        Args:
            provider: Provider name
            documents: List of documents to upload
            timestamp: Optional timestamp to match index name
        """
        if not documents:
            logger.warning("No documents to upload")
            return
        
        index_name = self.get_index_name(provider, timestamp)
        
        logger.info(f"Uploading {len(documents)} documents to index '{index_name}'")
        
        try:
            client = SearchClient(
                endpoint=self.endpoint,
                index_name=index_name,
                credential=self.credential
            )
            
            result = client.upload_documents(documents=documents)
            
            # Check results
            success_count = sum(1 for r in result if r.succeeded)
            logger.info(f"Uploaded {success_count}/{len(documents)} documents successfully")
            
            if success_count < len(documents):
                failed = [r for r in result if not r.succeeded]
                for fail in failed:
                    logger.error(f"Failed to upload document: {fail.key} - {fail.error_message}")
            
        except Exception as e:
            logger.error(f"Document upload failed: {e}", exc_info=True)
            raise

















-----------------
"""
MAIN PIPELINE - Document Extraction with Multimodal RAG
========================================================
Entry point. Run: python main.py
"""

import json
from datetime import datetime
from typing import List, Dict, Any

from helper import AzureBlobManager, AzureOpenAIManager, DocumentIntelligenceManager
from unified_rag import create_rag_extractor
from logging_config import (
    setup_logging, log_config, log_provider_start, log_document,
    log_ocr, log_vision, log_rag, log_extraction, log_error,
    log_provider_summary, log_outputs, log_complete
)


def load_config(path: str = "config.json") -> Dict[str, Any]:
    with open(path, 'r') as f:
        return json.load(f)


def process_provider(
    provider: str,
    documents: List[str],
    blob_mgr,
    doc_intel_mgr,
    openai_mgr,
    rag_extractor,
    config: Dict[str, Any]
) -> Dict[str, Any]:
    
    print(f"\n{'='*60}\n  PROVIDER: {provider} ({len(documents)} docs)\n{'='*60}")
    log_provider_start(provider, len(documents), index_name)
    
    fields = config['fields']
    mode = config['rag']['mode']
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    index_name = f"{provider.lower()}-{timestamp}"
    
    results = []
    total_cost = 0.0
    
    for idx, doc_name in enumerate(documents, 1):
        print(f"\n [{idx}/{len(documents)}] {doc_name}")
        log_document(idx, len(documents), doc_name)
        
        try:
            doc_bytes = blob_mgr.download_blob(doc_name)
            
            # Get file extension
            import os
            file_extension = os.path.splitext(doc_name)[1]  # e.g., '.pdf', '.jpg'
            
            # Convert to base64 for Document Intelligence
            import base64
            doc_base64 = base64.b64encode(doc_bytes).decode('utf-8')
            
            # OCR (PRIMARY)
            print(f"   üîç OCR...")
            log_ocr(0, 0)  # Start OCR logging
            ocr_result = doc_intel_mgr.analyze_document(doc_base64, file_extension)
            doc_text = ocr_result.get('content', '')
            
            if len(doc_text) < 50:
                print(f"   ‚ö†Ô∏è  Insufficient OCR - skip")
                log_error(doc_name, "Insufficient OCR text")
                continue
            
            print(f"   ‚úì OCR: {len(doc_text)} chars")
            log_ocr(len(doc_text), ocr_result.get('page_count', 1))
            
            # RAG Extraction
            if idx == 1:
                print(f"   üìÑ First doc - no RAG")
                if mode == 'multimodal':
                    result = rag_extractor.extract_without_rag(doc_text, doc_bytes, None, doc_name, provider, index_name)
                else:
                    result = rag_extractor.extract_without_rag(doc_text, None, doc_name, provider, index_name)
            else:
                print(f"   üìã RAG extraction...")
                if mode == 'multimodal':
                    result = rag_extractor.extract_with_rag(doc_text, doc_bytes, provider, index_name, doc_name)
                else:
                    result = rag_extractor.extract_with_rag(doc_text, provider, index_name, doc_name)
            
            extracted = result.get('extracted_fields', {})
            avg_conf = sum([f.get('confidence',0) for f in extracted.values() if isinstance(f,dict)]) / len(extracted) if extracted else 0
            
            print(f"   ‚úì Confidence: {avg_conf:.2f} | RAG: {result.get('used_rag')} | Vision: {result.get('has_vision')}")
            log_rag(result.get('used_rag', False), result.get('similar_docs_count', 0), mode, result.get('has_vision', False))
            log_extraction(extracted, avg_conf)
            
            results.append({
                'document_name': doc_name,
                'extracted_fields': extracted,
                'avg_confidence': avg_conf,
                'used_rag': result.get('used_rag', False),
                'has_vision': result.get('has_vision', False)
            })
            
            total_cost += 0.07 if mode == 'multimodal' else 0.05
            
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            log_error(doc_name, str(e))
    
    print(f"\n  ‚úì Processed: {len(results)}/{len(documents)} | Cost: ${total_cost:.2f}")
    log_provider_summary(provider, len(results), len(documents), total_cost)
    
    return {
        'provider': provider,
        'results': results,
        'total_cost': total_cost,
        'index_name': index_name
    }


def save_results(provider_results: Dict, blob_mgr, config: Dict):
    provider = provider_results['provider']
    results = provider_results['results']
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    fields = config['fields']
    confidence_threshold = config.get('confidence_threshold', 0.90)
    
    # CSV - one row
    csv_lines = [','.join(['provider','doc_count'] + fields + ['avg_confidence'])]
    agg = {'provider': provider, 'doc_count': len(results)}
    
    for field in fields:
        for r in results:
            if field in r['extracted_fields']:
                fd = r['extracted_fields'][field]
                agg[field] = fd.get('value','') if isinstance(fd,dict) else str(fd)
                break
        if field not in agg:
            agg[field] = ''
    
    avg = sum([r['avg_confidence'] for r in results]) / len(results) if results else 0
    agg['avg_confidence'] = f"{avg:.2f}"
    
    csv_lines.append(','.join([agg.get('provider',''), str(agg.get('doc_count',0))] + [agg.get(f,'') for f in fields] + [agg.get('avg_confidence','0.00')]))
    
    # Determine folder: highconfidence or lowconfidence
    folder = "highconfidence" if avg >= confidence_threshold else "lowconfidence"
    
    # CSV
    csv_file = f"{folder}/{provider}_{timestamp}.csv"
    blob_mgr.upload_blob(csv_file, '\n'.join(csv_lines).encode('utf-8'))
    print(f"  ‚úì CSV: {csv_file}")
    
    # JSON - detailed
    json_file = f"{folder}/{provider}_{timestamp}_detailed.json"
    blob_mgr.upload_blob(json_file, json.dumps(provider_results, indent=2).encode('utf-8'))
    print(f"  ‚úì JSON: {json_file}")
    
    # Costs
    cost_file = f"{folder}/{provider}_{timestamp}_costs.json"
    cost_data = {
        'provider': provider,
        'total_cost_usd': provider_results['total_cost'],
        'documents': len(results),
        'cost_per_doc': provider_results['total_cost']/len(results) if results else 0,
        'avg_confidence': avg,
        'confidence_category': folder
    }
    blob_mgr.upload_blob(cost_file, json.dumps(cost_data, indent=2).encode('utf-8'))
    print(f"  ‚úì Costs: {cost_file}")
    print(f"  üìä Confidence: {avg:.2f} ‚Üí Saved to '{folder}/' folder")
    log_outputs(csv_file, json_file, cost_file)


def main():
    # Setup logging FIRST
    log_file = setup_logging(log_dir="logs", log_level="INFO")
    
    print("\n" + "="*60)
    print("  MULTIMODAL RAG ‚Äî DOCUMENT EXTRACTION")
    print("  Healthcare / HIPAA ¬∑ Azure BAA")
    print("="*60)
    
    config = load_config()
    log_config(config)
    print(f"\n‚úì Config loaded | Fields: {', '.join(config['fields'])}")
    
    # Get RAG mode safely
    rag_mode = config.get('rag', {}).get('mode', 'text')
    print(f"‚úì RAG Mode: {rag_mode.upper()}")
    
    # Initialize managers
    blob_mgr = AzureBlobManager(
        config['AzureBlob']['connection_string'],
        config['AzureBlob']['inputcontainer'],
        config['AzureBlob']['outputcontainer']
    )
    print(f"‚úì Blob connected")
    
    openai_mgr = AzureOpenAIManager(
        config['AzureOpenAI']['endpoint'],
        config['AzureOpenAI']['api_key'],
        config['AzureOpenAI']['api_version'],
        config['AzureOpenAI']['deployment_name'],
        config['AzureEmbedding']['endpoint'],
        config['AzureEmbedding']['api_key'],
        config['AzureEmbedding'].get('api_version', config['AzureOpenAI']['api_version']),  # Use same version if not specified
        config['AzureEmbedding']['deployment_name'],
        config['AzureEmbedding'].get('dimension', 3072)  # Default to 3072
    )
    print(f"‚úì OpenAI connected")
    
    doc_intel_mgr = DocumentIntelligenceManager(
        config['DocumentIntelligence']['endpoint'],
        config['DocumentIntelligence']['key']
    )
    print(f"‚úì Doc Intelligence connected")
    
    rag_extractor = create_rag_extractor(
        config['rag'],
        config['AzureAISearch']['endpoint'],
        config['AzureAISearch']['api_key'],
        openai_mgr,
        config['fields']
    )
    print(f"‚úì RAG Extractor ready")
    
    # Get documents
    all_docs = blob_mgr.list_blobs()
    print(f"\n‚úì Found {len(all_docs)} documents")
    
    if not all_docs:
        print("\n‚ö†Ô∏è  No documents. Upload to input container and retry.")
        return
    
    # Group by provider
    providers = {}
    for doc in all_docs:
        prov = doc.split('_')[0] if '_' in doc else 'default'
        if prov not in providers:
            providers[prov] = []
        providers[prov].append(doc)
    
    print(f"‚úì Grouped into {len(providers)} providers")
    
    # Process each
    for prov, docs in providers.items():
        prov_results = process_provider(prov, docs, blob_mgr, doc_intel_mgr, openai_mgr, rag_extractor, config)
        save_results(prov_results, blob_mgr, config)
    
    print("\n" + "="*60)
    print("  COMPLETE ‚úì")
    print("="*60 + "\n")
    log_complete(len(providers), len(all_docs))


if __name__ == "__main__":
    main()
