import base64
import os
import json
import tempfile
import streamlit as st
from dotenv import load_dotenv
from openai import AzureOpenAI
from pathlib import Path
import fitz  # PyMuPDF
import pandas as pd
from datetime import datetime
import io
import zipfile
from azure.storage.blob import BlobServiceClient, ContentSettings

# Load environment variables from .env file
load_dotenv()

# Azure OpenAI environment variables
aoai_endpoint = os.getenv("AOAI_ENDPOINT")
aoai_api_key = os.getenv("AOAI_API_KEY")
aoai_deployment_name = os.getenv("AOAI_DEPLOYMENT")

# Azure Blob Storage environment variables
azure_storage_connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
azure_storage_container_name = os.getenv("AZURE_STORAGE_CONTAINER_NAME", "pdf-extraction-results")

# Initialize the Azure OpenAI client
@st.cache_resource
def get_client():
    return AzureOpenAI(
        azure_endpoint=aoai_endpoint,
        api_key=aoai_api_key,
        api_version="2024-08-01-preview"
    )

# Initialize the Azure Blob Storage client
@st.cache_resource
def get_blob_service_client():
    return BlobServiceClient.from_connection_string(azure_storage_connection_string)

def image_to_data_url(image_bytes, mime_type='image/png'):
    """
    Convert image bytes to a data URL.
    """
    base64_encoded_data = base64.b64encode(image_bytes).decode('utf-8')
    return f"data:{mime_type};base64,{base64_encoded_data}"

def call_azure_openai_vision(prompt, image_data_url, client, deployment_name):
    """
    Call the Azure OpenAI Vision service to analyze an image.
    """
    try:
        completion = client.chat.completions.create(
            model=deployment_name,
            messages=[{
                "role": "system",
                "content": "You are an AI helpful assistant that extracts information from invoice documents. Your task is to extract the following fields from invoices: VendorName, InvoiceNumber, InvoiceDate, CustomerName, PurchaseOrder, StockCode, UnitPrice, InvoiceAmount, Freight, Salestax, and Total. Return a JSON object with these keys. For each field, also include a confidence score between 0 and 1. The response format should be: {\"VendorName\": {\"value\": \"ABC Corp\", \"confidence\": 0.95}, \"InvoiceNumber\": {\"value\": \"INV-12345\", \"confidence\": 0.87}, ...and so on for each field.}"
            }, {
                "role": "user",
                "content": [{
                    "type": "text",
                    "text": prompt
                }, {
                    "type": "image_url",
                    "image_url": {
                        "url": image_data_url
                    }
                }]
            }],
            max_tokens=2000,
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        # Extract and parse the response content
        response_content = completion.choices[0].message.content
        return json.loads(response_content)
    except Exception as e:
        st.error(f"Error calling Azure OpenAI: {str(e)}")
        return {"error": str(e)}

def process_pdf(pdf_file, prompt, client, deployment_name, progress_bar=None, progress_text=None):
    """
    Process a PDF file and extract information from all pages.
    """
    try:
        # Create a temporary file to store the uploaded PDF
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_path = tmp_file.name
        
        # Get the filename
        filename = pdf_file.name
        
        try:
            # Open the PDF file
            with fitz.open(tmp_path) as doc:
                page_count = len(doc)
                
                if progress_text:
                    progress_text.text(f"Processing {filename} - {page_count} pages...")
                
                # Create a list to store extracted data from all pages
                all_page_results = []
                
                # Process each page in the PDF
                for page_num in range(page_count):
                    try:
                        # Update progress
                        if progress_bar:
                            progress_bar.progress((page_num + 1) / page_count)
                        if progress_text:
                            progress_text.text(f"Processing {filename} - Page {page_num+1}/{page_count}")
                        
                        # Load the current page
                        page = doc.load_page(page_num)
                        
                        # Process image
                        zoom = 2  # Zoom factor for image quality
                        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
                        image_bytes = pix.tobytes()
                        
                        # Convert image to data URL
                        image_data_url = image_to_data_url(image_bytes)
                        
                        # Call Azure OpenAI Vision to extract structured information
                        extracted_info = call_azure_openai_vision(prompt, image_data_url, client, deployment_name)
                        
                        # Add page info to the collected results
                        extracted_info_with_page = {
                            "page": page_num + 1,
                            "data": extracted_info
                        }
                        
                        # Add to our collection of all page results
                        all_page_results.append(extracted_info_with_page)
                        
                    except Exception as e:
                        error_msg = f"Error processing page {page_num+1} of {filename}: {e}"
                        st.warning(error_msg)
                        all_page_results.append({
                            "page": page_num + 1,
                            "data": {"error": str(e)}
                        })
        finally:
            # Clean up the temporary file
            try:
                os.unlink(tmp_path)
            except Exception as e:
                st.warning(f"Could not remove temporary file {tmp_path}: {e}")
        
        # Create a result object that contains all pages' data
        final_result = {
            "filename": filename,
            "total_pages": page_count,
            "pages": all_page_results
        }
        
        return final_result
        
    except Exception as e:
        error_msg = f"Error processing {pdf_file.name}: {e}"
        st.error(error_msg)
        return {
            "filename": pdf_file.name,
            "error": str(e),
            "total_pages": 0,
            "pages": []
        }

def create_results_dataframe(all_pdf_results):
    """
    Create a pandas DataFrame from the extracted results for easy viewing.
    """
    rows = []
    
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    for pdf_result in all_pdf_results:
        filename = pdf_result["filename"]
        
        for page in pdf_result["pages"]:
            page_num = page["page"]
            data = page["data"]
            
            # Check for errors
            if "error" in data:
                row_data = {
                    "Filename": filename,
                    "Page": page_num
                }
                
                # Add placeholders for all fields and confidence values
                for field in fields:
                    row_data[field] = "N/A"
                    row_data[f"{field} Confidence"] = 0
                
                rows.append(row_data)
                continue
            
            # Initialize row data
            row_data = {
                "Filename": filename,
                "Page": page_num
            }
            
            # Process each field
            for field in fields:
                field_data = data.get(field, {})
                
                if isinstance(field_data, dict):
                    value = field_data.get("value", "N/A")
                    confidence = field_data.get("confidence", 0)
                else:
                    value = field_data if field_data else "N/A"
                    confidence = 0
                
                # Ensure values are strings to avoid PyArrow errors
                if isinstance(value, (list, dict)):
                    value = str(value)
                
                # Add to row data
                row_data[field] = value
                row_data[f"{field} Confidence"] = round(confidence * 100, 2)
            
            # Add completed row to rows
            rows.append(row_data)
    
    try:
        # First method: Try creating a DataFrame with string type
        # This avoids PyArrow conversion issues for mixed types
        return pd.DataFrame(rows, dtype=str)
    except Exception as e:
        st.warning(f"Error creating DataFrame: {e}. Trying alternative method...")
        
        try:
            # Second method: Try with pandas default types but disable PyArrow
            with pd.option_context('mode.dtype_backend', 'numpy'):  # Use NumPy instead of PyArrow
                return pd.DataFrame(rows)
        except Exception as e:
            st.warning(f"Second method failed: {e}. Using final fallback method...")
            
            try:
                # Third method: Convert all values to strings explicitly before creating DataFrame
                string_rows = []
                for row in rows:
                    string_row = {}
                    for key, value in row.items():
                        string_row[key] = str(value)
                    string_rows.append(string_row)
                return pd.DataFrame(string_rows)
            except Exception as e:
                st.error(f"All DataFrame creation methods failed: {e}")
                # Return empty DataFrame as absolute last resort
                return pd.DataFrame()

def create_text_files_zip(all_pdf_results):
    """
    Create a zip file containing text files for each PDF.
    """
    # Create a BytesIO object to store the zip file
    zip_buffer = io.BytesIO()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create a ZipFile object
    with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:
        for pdf_result in all_pdf_results:
            filename = pdf_result["filename"]
            base_filename = os.path.splitext(filename)[0]
            
            # Create the text content for this PDF (only key-value pairs)
            page_results_text = create_page_results_text(pdf_result)
            
            # Add structured data as a text file with timestamp
            zip_file.writestr(f"{base_filename}_{timestamp}.txt", page_results_text)
    
    # Seek to the beginning of the BytesIO object
    zip_buffer.seek(0)
    return zip_buffer

def create_page_results_text(pdf_result):
    """
    Create a text file containing only the key-value pairs from each page.
    Returns a string with the formatted key-value pairs.
    """
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    result_text = ""
    
    for page in pdf_result["pages"]:
        page_num = page["page"]
        data = page["data"]
        
        result_text += f"--- PAGE {page_num} ---\n"
        
        if "error" in data:
            result_text += f"error: {data['error']}\n\n"
            continue
            
        # Process fields with confidence scores
        for field in fields:
            display_field = ''.join(' ' + char if char.isupper() else char for char in field).strip().lower()
            
            field_data = data.get(field, {})
            if isinstance(field_data, dict):
                value = field_data.get("value", "N/A")
                confidence = field_data.get("confidence", 0)
                result_text += f"{display_field}: {value}\n"
                result_text += f"{display_field} confidence: {round(confidence * 100, 2)}%\n"
            else:
                result_text += f"{display_field}: {field_data}\n"
        
        result_text += "\n"
        
    return result_text

def evaluate_extraction_results(all_pdf_results):
    """
    Evaluate the quality and completeness of extraction results using the
    confidence scores provided by Azure AI Vision.
    """
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    evaluation_results = {
        "total_documents": len(all_pdf_results),
        "total_pages": 0,
        "successful_pages": 0,
        "failed_pages": 0,
        "field_confidence": {},
        "documents_with_errors": []
    }
    
    # Initialize field confidence data structure
    for field in fields:
        evaluation_results["field_confidence"][field] = {
            "total": 0,
            "average_confidence": 0,
            "pages_above_threshold": 0,
            "percent_above_threshold": 0
        }
    
    confidence_threshold = 0.7  # 70% confidence threshold
    
    # Collect all confidence scores by field
    all_confidences = {}
    for field in fields:
        all_confidences[field] = []
    
    for pdf_result in all_pdf_results:
        filename = pdf_result["filename"]
        document_has_error = False
        
        for page in pdf_result["pages"]:
            evaluation_results["total_pages"] += 1
            data = page["data"]
            
            if "error" in data:
                evaluation_results["failed_pages"] += 1
                document_has_error = True
                continue
                
            page_successful = True
            
            # Check each field for confidence scores
            for field in fields:
                field_data = data.get(field, {})
                
                if isinstance(field_data, dict) and "confidence" in field_data:
                    confidence = field_data.get("confidence", 0)
                    evaluation_results["field_confidence"][field]["total"] += 1
                    all_confidences[field].append(confidence)
                    
                    # Count pages above threshold
                    if confidence >= confidence_threshold:
                        evaluation_results["field_confidence"][field]["pages_above_threshold"] += 1
                    else:
                        page_successful = False
                else:
                    page_successful = False
            
            if page_successful:
                evaluation_results["successful_pages"] += 1
            else:
                evaluation_results["failed_pages"] += 1
                document_has_error = True
        
        if document_has_error:
            evaluation_results["documents_with_errors"].append(filename)
    
    # Calculate average confidence for each field
    for field in all_confidences:
        confidences = all_confidences[field]
        if confidences:
            avg_confidence = sum(confidences) / len(confidences)
            evaluation_results["field_confidence"][field]["average_confidence"] = round(avg_confidence * 100, 2)
            
            # Calculate percentage of pages above threshold
            total = evaluation_results["field_confidence"][field]["total"]
            if total > 0:
                above_threshold = evaluation_results["field_confidence"][field]["pages_above_threshold"]
                evaluation_results["field_confidence"][field]["percent_above_threshold"] = round((above_threshold / total) * 100, 2)
    
    # Calculate overall success rate
    if evaluation_results["total_pages"] > 0:
        evaluation_results["success_rate"] = round((evaluation_results["successful_pages"] / evaluation_results["total_pages"]) * 100, 2)
    
    # Calculate overall confidence score (average of field confidence)
    field_scores = [field_data["average_confidence"] for field_data in evaluation_results["field_confidence"].values() if "average_confidence" in field_data]
    if field_scores:
        evaluation_results["overall_confidence_score"] = round(sum(field_scores) / len(field_scores), 2)
    
    return evaluation_results

def upload_to_blob_storage(blob_service_client, container_name, blob_name, data, content_type):
    """
    Upload data to Azure Blob Storage.
    """
    try:
        # Get the blob client
        container_client = blob_service_client.get_container_client(container_name)
        
        # Create the container if it doesn't exist
        if not container_client.exists():
            container_client.create_container()
        
        # Upload blob
        blob_client = container_client.get_blob_client(blob_name)
        
        # Set content settings
        content_settings = ContentSettings(content_type=content_type)
        
        # Upload the file
        blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
        
        return True, blob_client.url
    except Exception as e:
        return False, str(e)

def main():
    st.set_page_config(
        page_title="PDF Financial Data Extractor",
        page_icon="📊",
        layout="wide"
    )
    
    st.title("PDF Financial Data Extractor")
    st.subheader("Upload financial documents to extract customer information")
    
    # Check if Azure OpenAI credentials are available
    if not all([aoai_endpoint, aoai_api_key, aoai_deployment_name]):
        st.error("Azure OpenAI credentials are missing. Please set AOAI_ENDPOINT, AOAI_API_KEY, and AOAI_DEPLOYMENT environment variables.")
        return
    
    # Check if Azure Blob Storage credentials are available
    if not azure_storage_connection_string:
        st.warning("Azure Blob Storage connection string is missing. Results will not be saved to the cloud. Please set AZURE_STORAGE_CONNECTION_STRING environment variable.")
    
    # Initialize the clients
    client = get_client()
    blob_service_client = get_blob_service_client() if azure_storage_connection_string else None
    
    # File uploader
    uploaded_files = st.file_uploader(
        "Upload PDF files", 
        type="pdf", 
        accept_multiple_files=True,
        help="Upload one or more PDF files containing financial statements"
    )
    
    # Advanced settings in an expandable section
    with st.expander("Advanced Settings"):
        prompt = st.text_area(
            "Extraction Prompt", 
            """Based on this image, extract the following information from the invoice:   
            1) What is the vendor name?
            2) What is the invoice number?
            3) What is the invoice date?
            4) What is the customer name?
            5) What is the purchase order number?
            6) What is the stock code?
            7) What is the unit price?
            8) What is the invoice amount?
            9) What is the freight cost?
            10) What is the sales tax?
            11) What is the total amount?""",
            help="Customize the prompt sent to Azure OpenAI Vision to extract information"
        )
    
    # Process button
    if uploaded_files and st.button("Process Documents", type="primary"):
        if not uploaded_files:
            st.warning("Please upload at least one PDF file.")
            return
            
        with st.spinner("Processing documents..."):
            # Create containers for progress tracking
            progress_container = st.container()
            with progress_container:
                progress_bar = st.progress(0)
                progress_text = st.empty()
                
                # Store all PDF results
                all_pdf_results = []
                
                # Process each uploaded PDF
                for i, pdf_file in enumerate(uploaded_files):
                    progress_text.text(f"Processing file {i+1}/{len(uploaded_files)}: {pdf_file.name}")
                    
                    # Process the PDF and get results
                    pdf_result = process_pdf(
                        pdf_file, 
                        prompt, 
                        client, 
                        aoai_deployment_name,
                        progress_bar,
                        progress_text
                    )
                    
                    # Add to our collection of all PDF results
                    all_pdf_results.append(pdf_result)
                    
                    # Update overall progress
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                # Create a timestamp for the filename
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # Save individual text files for each PDF and upload to Azure Blob Storage
                blob_upload_results = []
                text_files_info = []
                
                if blob_service_client:
                    for pdf_result in all_pdf_results:
                        filename = pdf_result["filename"]
                        base_filename = os.path.splitext(filename)[0]
                        
                        # Create the text content with key-value pairs
                        page_results_text = create_page_results_text(pdf_result)
                        
                        # Create timestamp filename
                        timestamp_filename = f"{base_filename}_{timestamp}"
                        
                        # Upload text file to blob storage
                        text_blob_name = f"{timestamp_filename}.txt"
                        text_success, text_url = upload_to_blob_storage(
                            blob_service_client,
                            azure_storage_container_name,
                            text_blob_name,
                            page_results_text,
                            "text/plain"
                        )
                        
                        # Upload JSON to blob storage
                        json_blob_name = f"{timestamp_filename}.json"
                        pdf_json = json.dumps(pdf_result, ensure_ascii=False, indent=2)
                        json_success, json_url = upload_to_blob_storage(
                            blob_service_client,
                            azure_storage_container_name,
                            json_blob_name,
                            pdf_json,
                            "application/json"
                        )
                        
                        # Store results
                        blob_upload_results.append({
                            "filename": filename,
                            "text_success": text_success,
                            "text_url": text_url if text_success else None,
                            "json_success": json_success,
                            "json_url": json_url if json_success else None
                        })
                        
                        # Store text file info for display
                        text_files_info.append({
                            "filename": filename,
                            "text_content": page_results_text,
                            "timestamp_filename": timestamp_filename
                        })
                
                # Run evaluation on extraction results
                evaluation_results = evaluate_extraction_results(all_pdf_results)
                
                progress_text.text("Processing complete!")
                progress_bar.progress(1.0)
                
            # Display results
            st.subheader("Extraction Results")
            
            # Create a DataFrame view
            if all_pdf_results:
                try:
                    results_df = create_results_dataframe(all_pdf_results)
                    st.dataframe(results_df, use_container_width=True)
                except Exception as e:
                    st.error(f"Error displaying results table: {e}")
                    st.info("Continuing with other outputs despite table display error.")
                
                # Create zip file with text extractions
                text_zip = create_text_files_zip(all_pdf_results)
                
                # Display evaluation results
                st.subheader("Extraction Quality Evaluation")
                
                # Field-specific confidence scores
                field_confidence = evaluation_results["field_confidence"]
                
                # Create evaluation metrics display for overall and key fields
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Overall Confidence", f"{evaluation_results.get('overall_confidence_score', 0)}%")
                with col2:
                    st.metric("Vendor Name Confidence", f"{field_confidence['VendorName'].get('average_confidence', 0)}%")
                with col3:
                    st.metric("Customer Name Confidence", f"{field_confidence['CustomerName'].get('average_confidence', 0)}%")
                
                # Additional key metrics
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Invoice Number Confidence", f"{field_confidence['InvoiceNumber'].get('average_confidence', 0)}%")
                with col2:
                    st.metric("Total Amount Confidence", f"{field_confidence['Total'].get('average_confidence', 0)}%")
                with col3:
                    st.metric("Stock Code Confidence", f"{field_confidence['StockCode'].get('average_confidence', 0)}%")
                
                # Success rate metrics
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Total Pages", evaluation_results["total_pages"])
                with col2:
                    st.metric("Success Rate", f"{evaluation_results.get('success_rate', 0)}%")
                with col3:
                    st.metric("Failed Pages", evaluation_results["failed_pages"])
                
                # Display more detailed metrics
                with st.expander("Detailed Evaluation Metrics"):
                    # Field confidence table
                    st.subheader("Field Confidence Analysis")
                    
                    # Define field display names
                    field_display_names = {
                        "VendorName": "Vendor Name",
                        "InvoiceNumber": "Invoice Number",
                        "InvoiceDate": "Invoice Date",
                        "CustomerName": "Customer Name",
                        "PurchaseOrder": "Purchase Order",
                        "StockCode": "Stock Code",
                        "UnitPrice": "Unit Price",
                        "InvoiceAmount": "Invoice Amount",
                        "Freight": "Freight",
                        "Salestax": "Sales Tax",
                        "Total": "Total"
                    }
                    
                    # Create data for confidence table
                    field_names = []
                    avg_confidences = []
                    pages_above_threshold = []
                    total_pages = []
                    pct_above_threshold = []
                    
                    for field, display_name in field_display_names.items():
                        field_names.append(display_name)
                        avg_confidences.append(f"{field_confidence[field].get('average_confidence', 0)}%")
                        pages_above_threshold.append(field_confidence[field].get('pages_above_threshold', 0))
                        total_pages.append(field_confidence[field].get('total', 0))
                        pct_above_threshold.append(f"{field_confidence[field].get('percent_above_threshold', 0)}%")
                    
                    confidence_data = {
                        "Field": field_names,
                        "Average Confidence": avg_confidences,
                        "Pages Above 70%": pages_above_threshold,
                        "Total Pages": total_pages,
                        "% Above Threshold": pct_above_threshold
                    }
                    st.dataframe(pd.DataFrame(confidence_data), use_container_width=True)
                    
                    # List documents with errors
                    if evaluation_results["documents_with_errors"]:
                        st.subheader("Documents With Extraction Issues")
                        for doc in evaluation_results["documents_with_errors"]:
                            st.markdown(f"- {doc}")
                            
                    # Provide extraction quality assessment
                    st.subheader("Extraction Quality Assessment")
                    
                    # Assess each field's confidence
                    field_display_names = {
                        "VendorName": "Vendor Name",
                        "InvoiceNumber": "Invoice Number",
                        "InvoiceDate": "Invoice Date",
                        "CustomerName": "Customer Name",
                        "PurchaseOrder": "Purchase Order",
                        "StockCode": "Stock Code",
                        "UnitPrice": "Unit Price",
                        "InvoiceAmount": "Invoice Amount",
                        "Freight": "Freight",
                        "Salestax": "Sales Tax",
                        "Total": "Total"
                    }
                    
                    for field, display_name in field_display_names.items():
                        score = field_confidence[field].get('average_confidence', 0)
                        if score >= 90:
                            st.success(f"✅ {display_name}: Excellent extraction quality ({score}%)")
                        elif score >= 70:
                            st.info(f"ℹ️ {display_name}: Good extraction quality ({score}%)")
                        elif score >= 50:
                            st.warning(f"⚠️ {display_name}: Fair extraction quality ({score}%)")
                        else:
                            st.error(f"❌ {display_name}: Poor extraction quality ({score}%)")
                            
                    # Overall assessment
                    overall_score = evaluation_results.get('overall_confidence_score', 0)
                    st.subheader("Overall Assessment")
                    if overall_score >= 90:
                        st.success(f"✅ Overall extraction quality is excellent ({overall_score}%)")
                    elif overall_score >= 70:
                        st.info(f"ℹ️ Overall extraction quality is good ({overall_score}%)")
                    elif overall_score >= 50:
                        st.warning(f"⚠️ Overall extraction quality is fair ({overall_score}%)")
                    else:
                        st.error(f"❌ Overall extraction quality is poor ({overall_score}%)")
                    
                # Download buttons
                st.subheader("Download Options")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    # Download all text files as a zip
                    st.download_button(
                        label="Download All Text Files (ZIP)",
                        data=text_zip,
                        file_name=f"extracted_data_{timestamp}.zip",
                        mime="application/zip"
                    )
                
                with col2:
                    try:
                        # Download as CSV
                        if 'results_df' in locals() and not results_df.empty:
                            csv = results_df.to_csv(index=False)
                            st.download_button(
                                label="Download CSV Results",
                                data=csv,
                                file_name=f"financial_data_extraction_{timestamp}.csv",
                                mime="text/csv"
                            )
                        else:
                            st.info("CSV download not available due to DataFrame creation issues.")
                    except Exception as e:
                        st.error(f"Error creating CSV: {e}")
                        st.info("CSV download is unavailable due to an error.")
                
                with col3:
                    # Download evaluation results as JSON
                    eval_json = json.dumps(evaluation_results, ensure_ascii=False, indent=2)
                    st.download_button(
                        label="Download Evaluation Report",
                        data=eval_json,
                        file_name=f"extraction_evaluation_{timestamp}.json",
                        mime="application/json"
                    )
                
                with col4:
                    # Download field confidence report as CSV
                    try:
                        confidence_df = pd.DataFrame(confidence_data)
                        confidence_csv = confidence_df.to_csv(index=False)
                        st.download_button(
                            label="Download Confidence Report",
                            data=confidence_csv,
                            file_name=f"field_confidence_{timestamp}.csv",
                            mime="text/csv"
                        )
                    except Exception as e:
                        st.error(f"Error creating confidence report: {e}")
                
                # Display Blob Storage upload results
                if blob_service_client and blob_upload_results:
                    st.subheader("Azure Blob Storage Upload Results")
                    
                    # Create a table to show upload results
                    upload_rows = []
                    for result in blob_upload_results:
                        upload_rows.append({
                            "Filename": result["filename"],
                            "Text File": "✅ Uploaded" if result["text_success"] else "❌ Failed",
                            "JSON File": "✅ Uploaded" if result["json_success"] else "❌ Failed"
                        })
                    
                    upload_df = pd.DataFrame(upload_rows)
                    st.dataframe(upload_df, use_container_width=True)
                
                # Text files preview section
                st.subheader("Extracted Key-Value Pairs Preview")
                
                # Create tabs for each PDF
                if len(all_pdf_results) > 0:
                    pdf_tabs = st.tabs([pdf_result["filename"] for pdf_result in all_pdf_results])
                    
                    for i, tab in enumerate(pdf_tabs):
                        with tab:
                            pdf_result = all_pdf_results[i]
                            filename = pdf_result["filename"]
                            base_filename = os.path.splitext(filename)[0]
                            
                            # Generate or retrieve page results text
                            if text_files_info:
                                page_results_text = next((info["text_content"] for info in text_files_info if info["filename"] == filename), "")
                                timestamp_filename = next((info["timestamp_filename"] for info in text_files_info if info["filename"] == filename), "")
                            else:
                                page_results_text = create_page_results_text(pdf_result)
                                timestamp_filename = f"{base_filename}_{timestamp}"
                            
                            # Display the key-value page results
                            st.text_area(
                                f"Extracted Data for {filename}",
                                value=page_results_text,
                                height=300,
                                disabled=True
                            )
                            
                            # Allow downloading individual text file
                            st.download_button(
                                label=f"Download Text File for {filename}",
                                data=page_results_text,
                                file_name=f"{timestamp_filename}.txt",
                                mime="text/plain"
                            )
            else:
                st.warning("No results were extracted from the PDFs.")

if __name__ == "__main__":
    main()
