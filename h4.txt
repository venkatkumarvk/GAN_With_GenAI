import os
import io
import json
import base64
import logging
import time
from datetime import datetime
from openai import AzureOpenAI

class DocumentClassifier:
    def __init__(self, config, processing_mode='general'):
        """
        Initialize Azure OpenAI client for document classification
        
        :param config: Configuration dictionary
        :param processing_mode: 'general' or 'batch'
        """
        self.config = config
        self.processing_mode = processing_mode
        
        try:
            # Select configuration based on processing mode
            if processing_mode == 'general':
                azure_cfg = config['azure_openai']
            elif processing_mode == 'batch':
                azure_cfg = config['azure_openai_batch']
            else:
                raise ValueError(f"Invalid processing mode: {processing_mode}")
            
            # Initialize Azure OpenAI client
            self.client = AzureOpenAI(
                api_key=azure_cfg['api_key'],
                api_version=azure_cfg['api_version'],
                azure_endpoint=azure_cfg['endpoint']
            )
            
            self.deployment_name = azure_cfg['deployment_name']
            self.model_params = {
                'max_tokens': azure_cfg.get('max_tokens', 300),
                'temperature': azure_cfg.get('temperature', 0.2),
                'top_p': azure_cfg.get('top_p', 0.95)
            }
        except Exception as e:
            logging.error(f"Failed to initialize Azure OpenAI client: {e}")
            raise

    def classify_document(self, document_image, reference_dir):
        """
        Classify document using specified processing mode
        """
        if self.processing_mode == 'general':
            return self._classify_general(document_image, reference_dir)
        elif self.processing_mode == 'batch':
            return self._classify_batch_single(document_image, reference_dir)
        else:
            raise ValueError(f"Invalid processing mode: {self.processing_mode}")

    def _classify_general(self, document_image, reference_dir):
        """
        General classification using synchronous API
        """
        try:
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": self._create_classification_prompt(reference_dir)},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Classify this document page"},
                            {"type": "image", "image_base64": document_image}
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                **self.model_params
            )

            result = json.loads(response.choices[0].message.content)
            
            return {
                'main_category': result.get('main_category', 'unknown').lower(),
                'subcategory': result.get('subcategory', 'unknown').lower(),
                'confidence': float(result.get('confidence_score', 0.0)),
                'reasoning': result.get('reasoning', 'No reasoning'),
                'token_usage': self._calculate_token_cost(response.usage)
            }
        except Exception as e:
            logging.error(f"General classification error: {e}")
            return self._default_classification()

    def _classify_batch_single(self, document_image, reference_dir):
        """
        Batch classification for a single document
        """
        try:
            # Prepare batch processing for a single image
            base64_strings = [document_image]
            prompts = ["Classify this document page"]
            systemprompt = self._create_classification_prompt(reference_dir)

            # Process batch
            results = self.process_batch(base64_strings, prompts, systemprompt)

            # Parse the first (and only) result
            if results and results[0]:
                try:
                    result = json.loads(results[0])
                    return {
                        'main_category': result.get('main_category', 'unknown').lower(),
                        'subcategory': result.get('subcategory', 'unknown').lower(),
                        'confidence': float(result.get('confidence_score', 0.0)),
                        'reasoning': result.get('reasoning', 'No reasoning'),
                        'token_usage': None  # Token usage tracking for batch is different
                    }
                except json.JSONDecodeError:
                    logging.error("Failed to parse batch classification result")
                    return self._default_classification()
            else:
                return self._default_classification()
        except Exception as e:
            logging.error(f"Batch classification error: {e}")
            return self._default_classification()

    def process_batch(self, base64_strings, prompts, systemprompt=None):
        """
        Process images using the Batch API (asynchronous, efficient for large batches).
        
        Args:
            base64_strings: List of base64 encoded images
            prompts: List of user prompts (one per image)
            systemprompt: System prompt (optional)
        
        Returns:
            List of API responses (strings) in same order as input
        """
        logging.info(f"Processing {len(base64_strings)} images using Batch API")
        
        try:
            # Step 1: Create batch requests
            batch_requests = self._create_batch_requests(base64_strings, prompts, systemprompt)
            
            # Step 2: Upload batch file
            batch_file_id = self._upload_batch_file(batch_requests)
            if not batch_file_id:
                logging.error("Failed to upload batch file")
                return [None] * len(base64_strings)
            
            # Step 3: Create batch job
            batch_id = self._create_batch_job(batch_file_id)
            if not batch_id:
                logging.error("Failed to create batch job")
                return [None] * len(base64_strings)
            
            # Step 4: Wait for batch completion
            batch_completed = self._wait_for_batch_completion(batch_id)
            if not batch_completed:
                logging.error("Batch processing failed or timed out")
                return [None] * len(base64_strings)
            
            # Step 5: Retrieve batch results
            results = self._retrieve_batch_results(batch_id, len(base64_strings))
            
            logging.info(f"Batch API processing complete: {len([r for r in results if r])}/{len(results)} successful")
            return results
            
        except Exception as e:
            logging.error(f"Error in batch processing: {str(e)}")
            return [None] * len(base64_strings)

    def _create_batch_requests(self, base64_strings, prompts, systemprompt=None):
        """
        Create batch request objects for Batch API.
        """
        batch_requests = []
        
        for i, (base64_string, user_prompt) in enumerate(zip(base64_strings, prompts)):
            # Prepare messages
            messages = []
            
            # Add system message if provided
            if systemprompt:
                messages.append({
                    "role": "system",
                    "content": systemprompt
                })
            
            # Add user message with image
            user_content = [
                {
                    "type": "text",
                    "text": user_prompt
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{base64_string}"
                    }
                }
            ]
            
            messages.append({
                "role": "user",
                "content": user_content
            })
            
            # Create batch request item (JSONL format)
            batch_request = {
                "custom_id": f"request_{i}",
                "method": "POST",
                "url": "/chat/completions",
                "body": {
                    "model": self.deployment_name,
                    "messages": messages,
                    "max_tokens": 2000,
                    "temperature": 0.1,
                    "response_format": {"type": "json_object"}
                }
            }
            
            batch_requests.append(batch_request)
        
        logging.debug(f"Created {len(batch_requests)} batch requests")
        return batch_requests

    def _upload_batch_file(self, batch_requests):
        """
        Upload batch requests as JSONL file to Azure.
        """
        temp_filename = None
        try:
            # Convert to JSONL format (one JSON per line)
            jsonl_content = "\n".join([json.dumps(req) for req in batch_requests])
            
            # Create temporary JSONL file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_filename = f"batch_requests_{timestamp}.jsonl"
            
            with open(temp_filename, 'w', encoding='utf-8') as f:
                f.write(jsonl_content)
            
            logging.debug(f"Created batch file: {temp_filename}")
            
            # Upload file to Azure OpenAI
            with open(temp_filename, 'rb') as f:
                response = self.client.files.create(
                    file=f,
                    purpose="batch"
                )
            
            file_id = response.id
            
            # Clean up temporary file
            os.remove(temp_filename)
            
            logging.info(f"Uploaded batch file: {file_id}")
            return file_id
            
        except Exception as e:
            logging.error(f"Error uploading batch file: {str(e)}")
            if temp_filename and os.path.exists(temp_filename):
                os.remove(temp_filename)
            return None

    def _create_batch_job(self, batch_file_id):
        """
        Create a batch processing job.
        """
        try:
            response = self.client.batches.create(
                input_file_id=batch_file_id,
                endpoint="/chat/completions",
                completion_window="24h"
            )
            
            batch_id = response.id
            logging.info(f"Created batch job: {batch_id}")
            logging.debug(f"Batch status: {response.status}")
            
            return batch_id
            
        except Exception as e:
            logging.error(f"Error creating batch job: {str(e)}")
            return None

    def _wait_for_batch_completion(self, batch_id, max_wait_seconds=3600, check_interval=30):
        """
        Wait for batch job to complete.
        """
        try:
            start_time = time.time()
            
            logging.info(f"Waiting for batch {batch_id} to complete...")
            logging.info(f"Max wait time: {max_wait_seconds}s, Check interval: {check_interval}s")
            
            while True:
                # Check elapsed time
                elapsed = time.time() - start_time
                if elapsed > max_wait_seconds:
                    logging.error(f"Batch processing timed out after {elapsed:.0f}s")
                    return False
                
                # Get batch status
                batch = self.client.batches.retrieve(batch_id)
                status = batch.status
                
                logging.debug(f"Batch status: {status} (elapsed: {elapsed:.0f}s)")
                
                # Check status
                if status == "completed":
                    logging.info(f"Batch completed successfully after {elapsed:.0f}s")
                    return True
                elif status == "failed":
                    logging.error(f"Batch processing failed: {batch.errors}")
                    return False
                elif status == "cancelled":
                    logging.error("Batch processing was cancelled")
                    return False
                elif status in ["validating", "in_progress", "finalizing"]:
                    # Still processing
                    logging.debug(f"Batch still processing: {status}")
                    time.sleep(check_interval)
                else:
                    logging.warning(f"Unknown batch status: {status}")
                    time.sleep(check_interval)
            
        except Exception as e:
            logging.error(f"Error waiting for batch completion: {str(e)}")
            return False

    def _retrieve_batch_results(self, batch_id, expected_count):
        """
        Retrieve and parse batch results.
        """
        try:
            # Get batch details
            batch = self.client.batches.retrieve(batch_id)
            
            if not batch.output_file_id:
                logging.error("No output file available for batch")
                return [None] * expected_count
            
            # Download output file
            output_file_id = batch.output_file_id
            logging.info(f"Downloading batch results: {output_file_id}")
            
            response = self.client.files.content(output_file_id)
            content = response.read()
            
            # Parse JSONL results
            results_dict = {}
            for line in content.decode('utf-8').strip().split('\n'):
                if not line:
                    continue
                
                try:
                    result = json.loads(line)
                    custom_id = result.get("custom_id")
                    
                    # Extract response content
                    if result.get("response") and result["response"].get("body"):
                        body = result["response"]["body"]
                        if body.get("choices") and len(body["choices"]) > 0:
                            content_text = body["choices"][0]["message"]["content"]
                            results_dict[custom_id] = content_text
                        else:
                            results_dict[custom_id] = None
                    else:
                        results_dict[custom_id] = None
                
                except json.JSONDecodeError as e:
                    logging.error(f"Error parsing result line: {e}")
                    continue
            
            # Order results by custom_id (request_0, request_1, etc.)
            ordered_results = []
            for i in range(expected_count):
                custom_id = f"request_{i}"
                result = results_dict.get(custom_id)
                ordered_results.append(result)
            
            success_count = len([r for r in ordered_results if r is not None])
            logging.info(f"Retrieved {success_count}/{expected_count} successful results")
            
            return ordered_results
            
        except Exception as e:
            logging.error(f"Error retrieving batch results: {str(e)}")
            return [None] * expected_count

    def _create_classification_prompt(self, reference_dir):
        """
        Create classification prompt
        """
        return f"""
        Advanced Document Classification AI System V2.0

        COMPREHENSIVE CLASSIFICATION FRAMEWORK:
        - Perform multi-dimensional document analysis
        - Leverage extensive reference document insights
        - Provide precise, confidence-driven categorization

        REFERENCE DOCUMENT LANDSCAPE:
        {self._get_reference_details(reference_dir)}

        RESPONSE FORMAT:
        {{
            "main_category": "Exact main category or 'unknown'",
            "subcategory": "Exact subcategory or 'unknown'",
            "confidence_score": 0.0-1.0,
            "reasoning": "Detailed explanation of classification decision"
        }}
        """

    def _get_reference_details(self, reference_dir):
        """
        Collect reference document details
        """
        try:
            reference_details = {}
            for main_category in os.listdir(reference_dir):
                main_path = os.path.join(reference_dir, main_category)
                if not os.path.isdir(main_path):
                    continue
                
                reference_details[main_category] = {}
                
                for subcategory in os.listdir(main_path):
                    subcat_path = os.path.join(main_path, subcategory)
                    if not os.path.isdir(subcat_path):
                        continue
                    
                    reference_details[main_category][subcategory] = {
                        'document_count': len(os.listdir(subcat_path)),
                        'document_types': list(set(os.path.splitext(f)[1] for f in os.listdir(subcat_path)))
                    }
            
            return json.dumps(reference_details)
        except Exception as e:
            logging.error(f"Error getting reference details: {e}")
            return json.dumps({})

    def _calculate_token_cost(self, usage):
        """
        Calculate token usage cost
        """
        try:
            pricing = self.config.get('token_pricing', {}).get('gpt-4o', {
                'input': {'price_per_million': 2.50},
                'output': {'price_per_million': 10.00}
            })

            input_cost = (usage.prompt_tokens / 1_000_000) * pricing['input'].get('price_per_million', 2.50)
            output_cost = (usage.completion_tokens / 1_000_000) * pricing['output'].get('price_per_million', 10.00)
            
            return {
                'input_tokens': usage.prompt_tokens,
                'output_tokens': usage.completion_tokens,
                'total_cost': round(input_cost + output_cost, 4)
            }
        except Exception as e:
            logging.error(f"Token cost calculation error: {e}")
            return None

    def _default_classification(self):
        """
        Default classification when processing fails
        """
        return {
            'main_category': 'unknown',
            'subcategory': 'unknown',
            'confidence': 0.0,
            'reasoning': 'Classification failed',
            'token_usage': None
        }


-----

def process_azure_documents(config, input_folder, confidence_threshold=0.6, 
                             archive_enabled=True, 
                             clean_input_container=True, 
                             processing_mode='general'):
    # Pass processing_mode to DocumentClassifier
    classifier = DocumentClassifier(config, processing_mode=processing_mode)
    
    # Rest of the existing code remains the same


----

              def parse_arguments():
    parser.add_argument(
        '--processing-mode', 
        choices=['general', 'batch'], 
        default='general',
        help='Processing mode: general or batch Azure OpenAI'
    )
    # ... other existing arguments

def main():
    # Add processing mode to document processing
    processed_files, unprocessed_files = process_documents(
        source='azure',
        config_path=args.config,
        input_folder=args.inputfolder,
        confidence_threshold=args.confidence,
        processing_mode=args.processing_mode
    )

              
