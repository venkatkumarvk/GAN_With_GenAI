def process_local_documents(config, input_folder=None, confidence_threshold=0.6, processing_mode='general'):
    """
    Process documents from local filesystem
    """
    # Use paths from configuration for local source
    input_dir = input_folder or config['paths']['input_dir']
    output_dir = config['paths']['output_dir']
    
    # Create output directories
    source_dir = os.path.join(output_dir, 'source')
    classified_dir = os.path.join(output_dir, 'classified')
    unclassified_dir = os.path.join(output_dir, 'unclassified')
    unprocessed_dir = os.path.join(output_dir, 'unprocessed')
    
    # Create directories
    for directory in [source_dir, classified_dir, unclassified_dir, unprocessed_dir]:
        os.makedirs(directory, exist_ok=True)
    
    # Initialize document classifier with processing mode
    classifier = DocumentClassifier(config, processing_mode=processing_mode)
    
    # Reference directory
    reference_dir = config['paths']['reference_dir']
    
    # Track processed and unprocessed files
    processed_files = []
    unprocessed_files = []

    # Process each document in local input directory
    for fname in os.listdir(input_dir):
        try:
            fpath = os.path.join(input_dir, fname)
            
            # Skip directories and hidden files
            if not os.path.isfile(fpath) or fname.startswith('.'):
                continue

            # Determine file type
            file_type = os.path.splitext(fname)[1].lower()
            
            # Process only supported file types
            if file_type not in ['.pdf', '.jpg', '.jpeg', '.png']:
                # Move unprocessable files to unprocessed folder
                unprocessed_path = os.path.join(unprocessed_dir, fname)
                shutil.copy2(fpath, unprocessed_path)
                logging.warning(f"Unsupported file type, moved to unprocessed: {fname}")
                unprocessed_files.append(fpath)
                continue

            # Convert to PDF if needed
            pdf_path = convert_to_pdf(fpath)
            if not pdf_path:
                raise ValueError(f"Failed to convert {fname} to PDF")

            # Copy source PDF
            shutil.copy2(pdf_path, os.path.join(source_dir, os.path.basename(pdf_path)))

            # Open PDF
            doc = fitz.open(pdf_path)
            total_pages = len(doc)

            # Track page classifications
            classified_pages = []
            unclassified_pages = []

            # Process each page
            for page_num in range(total_pages):
                try:
                    # Extract page image
                    page_image = extract_page_image(pdf_path, page_num)
                    
                    # Classify page
                    classification = classifier.classify_document(page_image, reference_dir)
                    
                    # Check classification confidence
                    if (classification['confidence'] >= confidence_threshold 
                        and classification['main_category'] != 'unknown'):
                        classified_pages.append({
                            'page': page_num,
                            'category': classification['main_category'],
                            'subcategory': classification['subcategory'],
                            'confidence': classification['confidence']
                        })
                    else:
                        unclassified_pages.append(page_num)

                except Exception as page_error:
                    logging.error(f"Error processing page {page_num} in {fname}: {page_error}")
                    unclassified_pages.append(page_num)

            # Close original document
            doc.close()

            # Determine document processing status
            if classified_pages:
                # Process classified pages
                # Group by subcategory
                subcategory_groups = {}
                for page in classified_pages:
                    key = (page['category'], page['subcategory'])
                    if key not in subcategory_groups:
                        subcategory_groups[key] = []
                    subcategory_groups[key].append(page['page'])

                # Create PDFs for each subcategory
                for (main_cat, sub_cat), pages in subcategory_groups.items():
                    # Open original PDF
                    doc = fitz.open(pdf_path)
                    classified_doc = fitz.open()
                    
                    # Add only classified pages to the new PDF
                    for page_num in pages:
                        classified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                    
                    # Sanitize original filename
                    base_name = os.path.splitext(fname)[0]
                    sanitized_base = sanitize_filename(base_name)
                    
                    # Create page number string for classified pages
                    classified_page_nums = '_'.join(str(p+1) for p in pages)
                    
                    # Prepare output path
                    category_path = os.path.join(classified_dir, main_cat, sub_cat)
                    os.makedirs(category_path, exist_ok=True)
                    
                    # Create output filename
                    output_filename = f"{sanitized_base}_page{classified_page_nums}.pdf"
                    output_path = os.path.join(category_path, output_filename)
                    
                    # Save PDF
                    classified_doc.save(output_path)
                    classified_doc.close()
                    doc.close()

                    # Track processed file
                    processed_files.append({
                        'original_path': pdf_path,
                        'processed_path': output_path,
                        'main_category': main_cat,
                        'subcategory': sub_cat,
                        'classified_pages': pages
                    })

            # Process unclassified pages
            if unclassified_pages:
                # Open original PDF
                doc = fitz.open(pdf_path)
                unclassified_doc = fitz.open()
                
                # Add unclassified pages
                for page_num in unclassified_pages:
                    unclassified_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                
                # Sanitize original filename
                base_name = os.path.splitext(fname)[0]
                sanitized_base = sanitize_filename(base_name)
                
                # Create page number string for unclassified pages
                unclassified_page_nums = '_'.join(str(p+1) for p in unclassified_pages)
                
                # Save unclassified PDF
                output_filename = f"{sanitized_base}_unclassified_pages{unclassified_page_nums}.pdf"
                output_path = os.path.join(unclassified_dir, output_filename)
                
                unclassified_doc.save(output_path)
                unclassified_doc.close()
                doc.close()

                # If no classified pages, track the file as processed
                if not classified_pages:
                    processed_files.append({
                        'original_path': pdf_path,
                        'processed_path': output_path,
                        'main_category': 'unknown',
                        'subcategory': 'unknown',
                        'unclassified_pages': unclassified_pages
                    })

        except Exception as doc_error:
            logging.error(f"Error processing document {fname}: {doc_error}")
            logging.error(traceback.format_exc())
            
            # Move source document to unprocessed folder
            unprocessed_path = os.path.join(unprocessed_dir, fname)
            try:
                shutil.copy2(fpath, unprocessed_path)
                logging.info(f"Moved unprocessed document to: {unprocessed_path}")
            except Exception as move_error:
                logging.error(f"Could not move unprocessed document {fname}: {move_error}")
            
            unprocessed_files.append(fpath)

    return processed_files, unprocessed_files

def process_documents(source='local', config_path='config.json', **kwargs):
    """
    Main document processing dispatcher
    """
    # Load configuration
    config = load_config(config_path)
    
    # Determine processing method based on source
    if source == 'local':
        return process_local_documents(
            config, 
            processing_mode=kwargs.get('processing_mode', 'general'),
            **kwargs
        )
    elif source == 'azure':
        return process_azure_documents(config, **kwargs)
    else:
        raise ValueError(f"Unsupported source: {source}")







#updated

import os
import io
import json
import base64
import logging
import time
from datetime import datetime
from openai import AzureOpenAI
from PIL import Image
import fitz  # PyMuPDF


def prepare_reference_details(self, reference_dir):
    """
    Generate detailed reference document metadata
    """
    try:
        reference_details = {}
        
        # Main categories from config
        categories = self.config.get('categories', {})
        
        for main_category, subcategories in categories.items():
            reference_details[main_category] = {}
            
            # Check if reference directory exists for main category
            main_category_path = os.path.join(reference_dir, main_category)
            if not os.path.isdir(main_category_path):
                continue
            
            for subcategory in subcategories:
                # Check subcategory path
                subcategory_path = os.path.join(main_category_path, subcategory)
                if not os.path.isdir(subcategory_path):
                    continue
                
                # Collect reference document details
                reference_details[main_category][subcategory] = {
                    'document_count': len(os.listdir(subcategory_path)),
                    'document_types': list(set(os.path.splitext(f)[1] for f in os.listdir(subcategory_path) if os.path.isfile(os.path.join(subcategory_path, f))))
                }
        
        return reference_details
    except Exception as e:
        logging.error(f"Error preparing reference details: {e}")
        return {}

def _create_classification_prompt(self, reference_dir):
    """
    Create a comprehensive classification prompt
    """
    # Prepare reference details
    reference_details = self.prepare_reference_details(reference_dir)
    
    return f"""
    Advanced Document Classification AI System

    CONTEXT:
    - Carefully analyze the document page
    - Identify precise document category
    - Provide confident, data-driven classification

    REFERENCE DOCUMENT ANALYSIS:
    {json.dumps(reference_details, indent=2)}

    AVAILABLE CATEGORIES:
    {json.dumps(self.config.get('categories', {}), indent=2)}

    CLASSIFICATION CRITERIA:
    1. Examine visual document structure
    2. Compare against reference documents
    3. Match document characteristics precisely
    4. Prioritize accuracy over breadth
    5. Provide clear reasoning

    CLASSIFICATION CONSTRAINTS:
    - Main category must be: {', '.join(self.config.get('categories', {}).keys())}
    - Subcategories must match reference document structure
    - Confidence score reflects matching precision

    RESPONSE FORMAT:
    {{
        "main_category": "Exact category from references",
        "subcategory": "Precise subcategory",
        "confidence_score": 0.0-1.0,
        "reasoning": "Detailed classification explanation"
    }}
    """

class DocumentClassifier:
    def __init__(self, config, processing_mode='general'):
        """
        Initialize Azure OpenAI client for document classification
        
        :param config: Configuration dictionary
        :param processing_mode: 'general' or 'batch'
        """
        self.config = config
        self.processing_mode = processing_mode
        
        try:
            # Select configuration based on processing mode
            if processing_mode == 'general':
                azure_cfg = config['azure_openai']
            elif processing_mode == 'batch':
                azure_cfg = config['azure_openai_batch']
            else:
                raise ValueError(f"Invalid processing mode: {processing_mode}")
            
            # Initialize Azure OpenAI client
            self.client = AzureOpenAI(
                api_key=azure_cfg['api_key'],
                api_version=azure_cfg['api_version'],
                azure_endpoint=azure_cfg['endpoint']
            )
            
            self.deployment_name = azure_cfg['deployment_name']
            self.model_params = {
                'max_tokens': azure_cfg.get('max_tokens', 300),
                'temperature': azure_cfg.get('temperature', 0.2),
                'top_p': azure_cfg.get('top_p', 0.95)
            }
        except Exception as e:
            logging.error(f"Failed to initialize Azure OpenAI client: {e}")
            raise

    def preprocess_image(self, document_image):
        """
        Preprocess image to ensure compatibility with GPT-4o
        """
        try:
            # Open image from bytes
            img = Image.open(io.BytesIO(document_image))
            
            # Convert to RGB if needed
            if img.mode != 'RGB':
                img = img.convert('RGB')
            
            # Resize image
            img = img.resize((800, 600), Image.LANCZOS)
            
            # Compress image
            buffer = io.BytesIO()
            img.save(buffer, format="PNG", optimize=True, quality=85)
            
            # Convert to base64
            base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            return base64_image
        
        except Exception as e:
            logging.error(f"Image preprocessing error: {e}")
            return None

    def prepare_reference_details(self, reference_dir):
        """
        Generate detailed reference document metadata
        """
        try:
            reference_details = {}
            for main_category in os.listdir(reference_dir):
                main_path = os.path.join(reference_dir, main_category)
                if not os.path.isdir(main_path):
                    continue
                
                reference_details[main_category] = {}
                
                for subcategory in os.listdir(main_path):
                    subcat_path = os.path.join(main_path, subcategory)
                    if not os.path.isdir(subcat_path):
                        continue
                    
                    reference_details[main_category][subcategory] = {
                        'document_count': len(os.listdir(subcat_path)),
                        'document_types': list(set(os.path.splitext(f)[1] for f in os.listdir(subcat_path)))
                    }
            
            return reference_details
        except Exception as e:
            logging.error(f"Error preparing reference details: {e}")
            return {}

    def _create_classification_prompt(self, reference_dir):
        """
        Create a comprehensive classification prompt
        """
        # Prepare reference details
        reference_details = self.prepare_reference_details(reference_dir)
        
        return f"""
        Advanced Document Classification AI System V2.0

        COMPREHENSIVE CLASSIFICATION FRAMEWORK:
        - Perform multi-dimensional document analysis
        - Leverage extensive reference document insights
        - Provide precise, confidence-driven categorization

        REFERENCE DOCUMENT LANDSCAPE:
        {json.dumps(reference_details, indent=2)}

        CANONICAL DOCUMENT CATEGORIES:
        Analyze the document considering the following key categories and their potential subcategories.

        CLASSIFICATION PROTOCOL:
        1. Visual Structural Analysis
            - Examine document layout topology
            - Identify characteristic visual markers
            - Cross-reference with reference document patterns

        2. Content Semantic Evaluation
            - Decode implicit and explicit document signals
            - Match against reference document semantic profiles
            - Assess information density and structural coherence

        RESPONSE FORMAT:
        {{
            "main_category": "Primary document domain (medical/financial/legal)",
            "subcategory": "Specialized document type",
            "confidence_score": "Precision metric [0.0-1.0]",
            "reasoning": "Comprehensive classification rationale"
        }}

        CRITICAL CLASSIFICATION CONSTRAINTS:
        - Maintain taxonomic integrity
        - Prioritize precision over broad categorization
        - Provide transparent, data-driven reasoning
        """

    def classify_document(self, document_image, reference_dir):
        """
        Classify document using specified processing mode
        
        Supports:
        - Multi-page PDF processing
        - Single PDF processing
        - Image processing
        """
        if self.processing_mode == 'general':
            return self._classify_general(document_image, reference_dir)
        elif self.processing_mode == 'batch':
            return self._classify_batch_single(document_image, reference_dir)
        else:
            raise ValueError(f"Invalid processing mode: {self.processing_mode}")

    def _classify_general(self, document_image, reference_dir):
        """
        General classification using synchronous API
        """
        try:
            # Preprocess image
            base64_image = self.preprocess_image(document_image)
            
            if not base64_image:
                return self._default_classification()

            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": self._create_classification_prompt(reference_dir)},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Classify this document page"},
                            {"type": "image", "image_base64": base64_image}
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                **self.model_params
            )

            result = json.loads(response.choices[0].message.content)
            
            return {
                'main_category': result.get('main_category', 'unknown').lower(),
                'subcategory': result.get('subcategory', 'unknown').lower(),
                'confidence': float(result.get('confidence_score', 0.0)),
                'reasoning': result.get('reasoning', 'No reasoning'),
                'token_usage': self._calculate_token_cost(response.usage)
            }
        except Exception as e:
            logging.error(f"General classification error: {e}")
            return self._default_classification()

    def _classify_batch_single(self, document_image, reference_dir):
        """
        Batch classification for a single document
        """
        try:
            # Preprocess image
            base64_image = self.preprocess_image(document_image)
            
            if not base64_image:
                return self._default_classification()

            # Prepare batch processing for a single image
            base64_strings = [base64_image]
            prompts = ["Classify this document page"]
            systemprompt = self._create_classification_prompt(reference_dir)

            # Process batch
            results = self.process_batch(base64_strings, prompts, systemprompt)

            # Parse the first (and only) result
            if results and results[0]:
                try:
                    result = json.loads(results[0])
                    return {
                        'main_category': result.get('main_category', 'unknown').lower(),
                        'subcategory': result.get('subcategory', 'unknown').lower(),
                        'confidence': float(result.get('confidence_score', 0.0)),
                        'reasoning': result.get('reasoning', 'No reasoning'),
                        'token_usage': None  # Token usage tracking for batch is different
                    }
                except json.JSONDecodeError:
                    logging.error("Failed to parse batch classification result")
                    return self._default_classification()
            else:
                return self._default_classification()
        except Exception as e:
            logging.error(f"Batch classification error: {e}")
            return self._default_classification()

    def process_batch(self, base64_strings, prompts, systemprompt=None):
        """
        Process images using the Batch API (asynchronous, efficient for large batches).
        
        Args:
            base64_strings: List of base64 encoded images
            prompts: List of user prompts (one per image)
            systemprompt: System prompt (optional)
        
        Returns:
            List of API responses (strings) in same order as input
        """
        logging.info(f"Processing {len(base64_strings)} images using Batch API")
        
        try:
            # Step 1: Create batch requests
            batch_requests = self._create_batch_requests(base64_strings, prompts, systemprompt)
            
            # Step 2: Upload batch file
            batch_file_id = self._upload_batch_file(batch_requests)
            if not batch_file_id:
                logging.error("Failed to upload batch file")
                return [None] * len(base64_strings)
            
            # Step 3: Create batch job
            batch_id = self._create_batch_job(batch_file_id)
            if not batch_id:
                logging.error("Failed to create batch job")
                return [None] * len(base64_strings)
            
            # Step 4: Wait for batch completion
            batch_completed = self._wait_for_batch_completion(batch_id)
            if not batch_completed:
                logging.error("Batch processing failed or timed out")
                return [None] * len(base64_strings)
            
            # Step 5: Retrieve batch results
            results = self._retrieve_batch_results(batch_id, len(base64_strings))
            
            logging.info(f"Batch API processing complete: {len([r for r in results if r])}/{len(results)} successful")
            return results
            
        except Exception as e:
            logging.error(f"Error in batch processing: {str(e)}")
            return [None] * len(base64_strings)

    def _create_batch_requests(self, base64_strings, prompts, systemprompt=None):
        """
        Create batch request objects for Batch API.
        """
        batch_requests = []
        
        for i, (base64_string, user_prompt) in enumerate(zip(base64_strings, prompts)):
            # Prepare messages
            messages = []
            
            # Add system message if provided
            if systemprompt:
                messages.append({
                    "role": "system",
                    "content": systemprompt
                })
            
            # Add user message with image
            user_content = [
                {
                    "type": "text",
                    "text": user_prompt
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{base64_string}"
                    }
                }
            ]
            
            messages.append({
                "role": "user",
                "content": user_content
            })
            
            # Create batch request item (JSONL format)
            batch_request = {
                "custom_id": f"request_{i}",
                "method": "POST",
                "url": "/chat/completions",
                "body": {
                    "model": self.deployment_name,
                    "messages": messages,
                    "max_tokens": 2000,
                    "temperature": 0.1,
                    "response_format": {"type": "json_object"}
                }
            }
            
            batch_requests.append(batch_request)
        
        logging.debug(f"Created {len(batch_requests)} batch requests")
        return batch_requests

    # Remaining batch processing methods (_upload_batch_file, _create_batch_job, 
    # _wait_for_batch_completion, _retrieve_batch_results) are the same as in the 
    # previous implementation I provided

    def _default_classification(self):
        """
        Default classification when processing fails
        """
        return {
            'main_category': 'unknown',
            'subcategory': 'unknown',
            'confidence': 0.0,
            'reasoning': 'Classification failed',
            'token_usage': None
        }

    def _calculate_token_cost(self, usage):
        """
        Calculate token usage cost
        """
        try:
            pricing = self.config.get('token_pricing', {}).get('gpt-4o', {
                'input': {'price_per_million': 2.50},
                'output': {'price_per_million': 10.00}
            })

            input_cost = (usage.prompt_tokens / 1_000_000) * pricing['input'].get('price_per_million', 2.50)
            output_cost = (usage.completion_tokens / 1_000_000) * pricing['output'].get('price_per_million', 10.00)
            
            return {
                'input_tokens': usage.prompt_tokens,
                'output_tokens': usage.completion_tokens,
                'total_cost': round(input_cost + output_cost, 4)
            }
        except Exception as e:
            logging.error(f"Token cost calculation error: {e}")
            return None




-----------
import os
import io
import json
import base64
import logging
import time
from datetime import datetime
from openai import AzureOpenAI

class DocumentClassifier:
    def __init__(self, config, processing_mode='general'):
        """
        Initialize Azure OpenAI client for document classification
        
        :param config: Configuration dictionary
        :param processing_mode: 'general' or 'batch'
        """
        self.config = config
        self.processing_mode = processing_mode
        
        try:
            # Select configuration based on processing mode
            if processing_mode == 'general':
                azure_cfg = config['azure_openai']
            elif processing_mode == 'batch':
                azure_cfg = config['azure_openai_batch']
            else:
                raise ValueError(f"Invalid processing mode: {processing_mode}")
            
            # Initialize Azure OpenAI client
            self.client = AzureOpenAI(
                api_key=azure_cfg['api_key'],
                api_version=azure_cfg['api_version'],
                azure_endpoint=azure_cfg['endpoint']
            )
            
            self.deployment_name = azure_cfg['deployment_name']
            self.model_params = {
                'max_tokens': azure_cfg.get('max_tokens', 300),
                'temperature': azure_cfg.get('temperature', 0.2),
                'top_p': azure_cfg.get('top_p', 0.95)
            }
        except Exception as e:
            logging.error(f"Failed to initialize Azure OpenAI client: {e}")
            raise

    def classify_document(self, document_image, reference_dir):
        """
        Classify document using specified processing mode
        """
        if self.processing_mode == 'general':
            return self._classify_general(document_image, reference_dir)
        elif self.processing_mode == 'batch':
            return self._classify_batch_single(document_image, reference_dir)
        else:
            raise ValueError(f"Invalid processing mode: {self.processing_mode}")

    def _classify_general(self, document_image, reference_dir):
        """
        General classification using synchronous API
        """
        try:
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": self._create_classification_prompt(reference_dir)},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Classify this document page"},
                            {"type": "image", "image_base64": document_image}
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                **self.model_params
            )

            result = json.loads(response.choices[0].message.content)
            
            return {
                'main_category': result.get('main_category', 'unknown').lower(),
                'subcategory': result.get('subcategory', 'unknown').lower(),
                'confidence': float(result.get('confidence_score', 0.0)),
                'reasoning': result.get('reasoning', 'No reasoning'),
                'token_usage': self._calculate_token_cost(response.usage)
            }
        except Exception as e:
            logging.error(f"General classification error: {e}")
            return self._default_classification()

    def _classify_batch_single(self, document_image, reference_dir):
        """
        Batch classification for a single document
        """
        try:
            # Prepare batch processing for a single image
            base64_strings = [document_image]
            prompts = ["Classify this document page"]
            systemprompt = self._create_classification_prompt(reference_dir)

            # Process batch
            results = self.process_batch(base64_strings, prompts, systemprompt)

            # Parse the first (and only) result
            if results and results[0]:
                try:
                    result = json.loads(results[0])
                    return {
                        'main_category': result.get('main_category', 'unknown').lower(),
                        'subcategory': result.get('subcategory', 'unknown').lower(),
                        'confidence': float(result.get('confidence_score', 0.0)),
                        'reasoning': result.get('reasoning', 'No reasoning'),
                        'token_usage': None  # Token usage tracking for batch is different
                    }
                except json.JSONDecodeError:
                    logging.error("Failed to parse batch classification result")
                    return self._default_classification()
            else:
                return self._default_classification()
        except Exception as e:
            logging.error(f"Batch classification error: {e}")
            return self._default_classification()

    def process_batch(self, base64_strings, prompts, systemprompt=None):
        """
        Process images using the Batch API (asynchronous, efficient for large batches).
        
        Args:
            base64_strings: List of base64 encoded images
            prompts: List of user prompts (one per image)
            systemprompt: System prompt (optional)
        
        Returns:
            List of API responses (strings) in same order as input
        """
        logging.info(f"Processing {len(base64_strings)} images using Batch API")
        
        try:
            # Step 1: Create batch requests
            batch_requests = self._create_batch_requests(base64_strings, prompts, systemprompt)
            
            # Step 2: Upload batch file
            batch_file_id = self._upload_batch_file(batch_requests)
            if not batch_file_id:
                logging.error("Failed to upload batch file")
                return [None] * len(base64_strings)
            
            # Step 3: Create batch job
            batch_id = self._create_batch_job(batch_file_id)
            if not batch_id:
                logging.error("Failed to create batch job")
                return [None] * len(base64_strings)
            
            # Step 4: Wait for batch completion
            batch_completed = self._wait_for_batch_completion(batch_id)
            if not batch_completed:
                logging.error("Batch processing failed or timed out")
                return [None] * len(base64_strings)
            
            # Step 5: Retrieve batch results
            results = self._retrieve_batch_results(batch_id, len(base64_strings))
            
            logging.info(f"Batch API processing complete: {len([r for r in results if r])}/{len(results)} successful")
            return results
            
        except Exception as e:
            logging.error(f"Error in batch processing: {str(e)}")
            return [None] * len(base64_strings)

    def _create_batch_requests(self, base64_strings, prompts, systemprompt=None):
        """
        Create batch request objects for Batch API.
        """
        batch_requests = []
        
        for i, (base64_string, user_prompt) in enumerate(zip(base64_strings, prompts)):
            # Prepare messages
            messages = []
            
            # Add system message if provided
            if systemprompt:
                messages.append({
                    "role": "system",
                    "content": systemprompt
                })
            
            # Add user message with image
            user_content = [
                {
                    "type": "text",
                    "text": user_prompt
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{base64_string}"
                    }
                }
            ]
            
            messages.append({
                "role": "user",
                "content": user_content
            })
            
            # Create batch request item (JSONL format)
            batch_request = {
                "custom_id": f"request_{i}",
                "method": "POST",
                "url": "/chat/completions",
                "body": {
                    "model": self.deployment_name,
                    "messages": messages,
                    "max_tokens": 2000,
                    "temperature": 0.1,
                    "response_format": {"type": "json_object"}
                }
            }
            
            batch_requests.append(batch_request)
        
        logging.debug(f"Created {len(batch_requests)} batch requests")
        return batch_requests

    def _upload_batch_file(self, batch_requests):
        """
        Upload batch requests as JSONL file to Azure.
        """
        temp_filename = None
        try:
            # Convert to JSONL format (one JSON per line)
            jsonl_content = "\n".join([json.dumps(req) for req in batch_requests])
            
            # Create temporary JSONL file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_filename = f"batch_requests_{timestamp}.jsonl"
            
            with open(temp_filename, 'w', encoding='utf-8') as f:
                f.write(jsonl_content)
            
            logging.debug(f"Created batch file: {temp_filename}")
            
            # Upload file to Azure OpenAI
            with open(temp_filename, 'rb') as f:
                response = self.client.files.create(
                    file=f,
                    purpose="batch"
                )
            
            file_id = response.id
            
            # Clean up temporary file
            os.remove(temp_filename)
            
            logging.info(f"Uploaded batch file: {file_id}")
            return file_id
            
        except Exception as e:
            logging.error(f"Error uploading batch file: {str(e)}")
            if temp_filename and os.path.exists(temp_filename):
                os.remove(temp_filename)
            return None

    def _create_batch_job(self, batch_file_id):
        """
        Create a batch processing job.
        """
        try:
            response = self.client.batches.create(
                input_file_id=batch_file_id,
                endpoint="/chat/completions",
                completion_window="24h"
            )
            
            batch_id = response.id
            logging.info(f"Created batch job: {batch_id}")
            logging.debug(f"Batch status: {response.status}")
            
            return batch_id
            
        except Exception as e:
            logging.error(f"Error creating batch job: {str(e)}")
            return None

    def _wait_for_batch_completion(self, batch_id, max_wait_seconds=3600, check_interval=30):
        """
        Wait for batch job to complete.
        """
        try:
            start_time = time.time()
            
            logging.info(f"Waiting for batch {batch_id} to complete...")
            logging.info(f"Max wait time: {max_wait_seconds}s, Check interval: {check_interval}s")
            
            while True:
                # Check elapsed time
                elapsed = time.time() - start_time
                if elapsed > max_wait_seconds:
                    logging.error(f"Batch processing timed out after {elapsed:.0f}s")
                    return False
                
                # Get batch status
                batch = self.client.batches.retrieve(batch_id)
                status = batch.status
                
                logging.debug(f"Batch status: {status} (elapsed: {elapsed:.0f}s)")
                
                # Check status
                if status == "completed":
                    logging.info(f"Batch completed successfully after {elapsed:.0f}s")
                    return True
                elif status == "failed":
                    logging.error(f"Batch processing failed: {batch.errors}")
                    return False
                elif status == "cancelled":
                    logging.error("Batch processing was cancelled")
                    return False
                elif status in ["validating", "in_progress", "finalizing"]:
                    # Still processing
                    logging.debug(f"Batch still processing: {status}")
                    time.sleep(check_interval)
                else:
                    logging.warning(f"Unknown batch status: {status}")
                    time.sleep(check_interval)
            
        except Exception as e:
            logging.error(f"Error waiting for batch completion: {str(e)}")
            return False

    def _retrieve_batch_results(self, batch_id, expected_count):
        """
        Retrieve and parse batch results.
        """
        try:
            # Get batch details
            batch = self.client.batches.retrieve(batch_id)
            
            if not batch.output_file_id:
                logging.error("No output file available for batch")
                return [None] * expected_count
            
            # Download output file
            output_file_id = batch.output_file_id
            logging.info(f"Downloading batch results: {output_file_id}")
            
            response = self.client.files.content(output_file_id)
            content = response.read()
            
            # Parse JSONL results
            results_dict = {}
            for line in content.decode('utf-8').strip().split('\n'):
                if not line:
                    continue
                
                try:
                    result = json.loads(line)
                    custom_id = result.get("custom_id")
                    
                    # Extract response content
                    if result.get("response") and result["response"].get("body"):
                        body = result["response"]["body"]
                        if body.get("choices") and len(body["choices"]) > 0:
                            content_text = body["choices"][0]["message"]["content"]
                            results_dict[custom_id] = content_text
                        else:
                            results_dict[custom_id] = None
                    else:
                        results_dict[custom_id] = None
                
                except json.JSONDecodeError as e:
                    logging.error(f"Error parsing result line: {e}")
                    continue
            
            # Order results by custom_id (request_0, request_1, etc.)
            ordered_results = []
            for i in range(expected_count):
                custom_id = f"request_{i}"
                result = results_dict.get(custom_id)
                ordered_results.append(result)
            
            success_count = len([r for r in ordered_results if r is not None])
            logging.info(f"Retrieved {success_count}/{expected_count} successful results")
            
            return ordered_results
            
        except Exception as e:
            logging.error(f"Error retrieving batch results: {str(e)}")
            return [None] * expected_count

    def _create_classification_prompt(self, reference_dir):
        """
        Create classification prompt
        """
        return f"""
        Advanced Document Classification AI System V2.0

        COMPREHENSIVE CLASSIFICATION FRAMEWORK:
        - Perform multi-dimensional document analysis
        - Leverage extensive reference document insights
        - Provide precise, confidence-driven categorization

        REFERENCE DOCUMENT LANDSCAPE:
        {self._get_reference_details(reference_dir)}

        RESPONSE FORMAT:
        {{
            "main_category": "Exact main category or 'unknown'",
            "subcategory": "Exact subcategory or 'unknown'",
            "confidence_score": 0.0-1.0,
            "reasoning": "Detailed explanation of classification decision"
        }}
        """

    def _get_reference_details(self, reference_dir):
        """
        Collect reference document details
        """
        try:
            reference_details = {}
            for main_category in os.listdir(reference_dir):
                main_path = os.path.join(reference_dir, main_category)
                if not os.path.isdir(main_path):
                    continue
                
                reference_details[main_category] = {}
                
                for subcategory in os.listdir(main_path):
                    subcat_path = os.path.join(main_path, subcategory)
                    if not os.path.isdir(subcat_path):
                        continue
                    
                    reference_details[main_category][subcategory] = {
                        'document_count': len(os.listdir(subcat_path)),
                        'document_types': list(set(os.path.splitext(f)[1] for f in os.listdir(subcat_path)))
                    }
            
            return json.dumps(reference_details)
        except Exception as e:
            logging.error(f"Error getting reference details: {e}")
            return json.dumps({})

    def _calculate_token_cost(self, usage):
        """
        Calculate token usage cost
        """
        try:
            pricing = self.config.get('token_pricing', {}).get('gpt-4o', {
                'input': {'price_per_million': 2.50},
                'output': {'price_per_million': 10.00}
            })

            input_cost = (usage.prompt_tokens / 1_000_000) * pricing['input'].get('price_per_million', 2.50)
            output_cost = (usage.completion_tokens / 1_000_000) * pricing['output'].get('price_per_million', 10.00)
            
            return {
                'input_tokens': usage.prompt_tokens,
                'output_tokens': usage.completion_tokens,
                'total_cost': round(input_cost + output_cost, 4)
            }
        except Exception as e:
            logging.error(f"Token cost calculation error: {e}")
            return None

    def _default_classification(self):
        """
        Default classification when processing fails
        """
        return {
            'main_category': 'unknown',
            'subcategory': 'unknown',
            'confidence': 0.0,
            'reasoning': 'Classification failed',
            'token_usage': None
        }


-----

def process_azure_documents(config, input_folder, confidence_threshold=0.6, 
                             archive_enabled=True, 
                             clean_input_container=True, 
                             processing_mode='general'):
    # Pass processing_mode to DocumentClassifier
    classifier = DocumentClassifier(config, processing_mode=processing_mode)
    
    # Rest of the existing code remains the same


----

              def parse_arguments():
    parser.add_argument(
        '--processing-mode', 
        choices=['general', 'batch'], 
        default='general',
        help='Processing mode: general or batch Azure OpenAI'
    )
    # ... other existing arguments

def main():
    # Add processing mode to document processing
    processed_files, unprocessed_files = process_documents(
        source='azure',
        config_path=args.config,
        input_folder=args.inputfolder,
        confidence_threshold=args.confidence,
        processing_mode=args.processing_mode
    )

              
