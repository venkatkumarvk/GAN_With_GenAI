import base64
import os
import json
import tempfile
import streamlit as st
from dotenv import load_dotenv
from openai import AzureOpenAI
from pathlib import Path
import fitz  # PyMuPDF
import pandas as pd
from datetime import datetime
import io
import zipfile
from azure.storage.blob import BlobServiceClient, ContentSettings

# Load environment variables from .env file
load_dotenv()

# Azure OpenAI environment variables
aoai_endpoint = os.getenv("AOAI_ENDPOINT")
aoai_api_key = os.getenv("AOAI_API_KEY")
aoai_deployment_name = os.getenv("AOAI_DEPLOYMENT")

# Azure Blob Storage environment variables
azure_storage_connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
azure_storage_container_name = os.getenv("AZURE_STORAGE_CONTAINER_NAME", "pdf-extraction-results")

# Initialize the Azure OpenAI client
@st.cache_resource
def get_client():
    return AzureOpenAI(
        azure_endpoint=aoai_endpoint,
        api_key=aoai_api_key,
        api_version="2024-08-01-preview"
    )

# Initialize the Azure Blob Storage client
@st.cache_resource
def get_blob_service_client():
    return BlobServiceClient.from_connection_string(azure_storage_connection_string)

def get_blob_containers(blob_service_client):
    """
    Get a list of available containers in the Azure Blob Storage account.
    """
    try:
        containers = []
        for container in blob_service_client.list_containers():
            containers.append(container.name)
        return containers
    except Exception as e:
        st.error(f"Error listing containers: {e}")
        return []

def get_blob_folders(blob_service_client, container_name):
    """
    Get a list of "folders" (common prefixes) in the Azure Blob Storage container.
    Note: Blob storage doesn't have actual folders, but we can simulate them using prefixes.
    """
    try:
        container_client = blob_service_client.get_container_client(container_name)
        
        # Get all blobs in the container
        blobs = container_client.list_blobs()
        
        # Extract folder paths (common prefixes before the last '/')
        folders = set()
        for blob in blobs:
            name = blob.name
            if '/' in name:
                folder = name.rsplit('/', 1)[0] + '/'
                folders.add(folder)
            
        # Add a root option
        folders.add("")  # root directory
        
        return sorted(list(folders))
    except Exception as e:
        st.error(f"Error listing folders in container {container_name}: {e}")
        return []

def list_pdf_blobs(blob_service_client, container_name, folder_prefix=""):
    """
    List all PDF blobs in the specified container and folder prefix.
    """
    try:
        container_client = blob_service_client.get_container_client(container_name)
        
        # Get blobs with the folder prefix that are PDFs
        pdf_blobs = []
        for blob in container_client.list_blobs(name_starts_with=folder_prefix):
            if blob.name.lower().endswith('.pdf'):
                pdf_blobs.append(blob.name)
                
        return pdf_blobs
    except Exception as e:
        st.error(f"Error listing PDF blobs: {e}")
        return []

def download_blob_to_memory(blob_service_client, container_name, blob_name):
    """
    Download a blob to memory.
    """
    try:
        container_client = blob_service_client.get_container_client(container_name)
        blob_client = container_client.get_blob_client(blob_name)
        
        # Download the blob content
        download_stream = blob_client.download_blob()
        content = download_stream.readall()
        
        return content
    except Exception as e:
        st.error(f"Error downloading blob {blob_name}: {e}")
        return None

def image_to_data_url(image_bytes, mime_type='image/png'):
    """
    Convert image bytes to a data URL.
    """
    base64_encoded_data = base64.b64encode(image_bytes).decode('utf-8')
    return f"data:{mime_type};base64,{base64_encoded_data}"

def call_azure_openai_vision(prompt, image_data_url, client, deployment_name):
    """
    Call the Azure OpenAI Vision service to analyze an image.
    """
    try:
        completion = client.chat.completions.create(
            model=deployment_name,
            messages=[{
                "role": "system",
                "content": "You are an AI helpful assistant that extracts information from invoice documents. Your task is to extract the following fields from invoices: VendorName, InvoiceNumber, InvoiceDate, CustomerName, PurchaseOrder, StockCode, UnitPrice, InvoiceAmount, Freight, Salestax, and Total. Return a JSON object with these keys. For each field, also include a confidence score between 0 and 1. The response format should be: {\"VendorName\": {\"value\": \"ABC Corp\", \"confidence\": 0.95}, \"InvoiceNumber\": {\"value\": \"INV-12345\", \"confidence\": 0.87}, ...and so on for each field.}"
            }, {
                "role": "user",
                "content": [{
                    "type": "text",
                    "text": prompt
                }, {
                    "type": "image_url",
                    "image_url": {
                        "url": image_data_url
                    }
                }]
            }],
            max_tokens=2000,
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        # Extract and parse the response content
        response_content = completion.choices[0].message.content
        return json.loads(response_content)
    except Exception as e:
        st.error(f"Error calling Azure OpenAI: {str(e)}")
        return {"error": str(e)}

def process_pdf(pdf_file, prompt, client, deployment_name, progress_bar=None, progress_text=None):
    """
    Process a PDF file and extract information from all pages.
    """
    try:
        # Create a temporary file to store the uploaded PDF
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_path = tmp_file.name
        
        # Get the filename
        filename = pdf_file.name
        
        try:
            # Open the PDF file
            with fitz.open(tmp_path) as doc:
                page_count = len(doc)
                
                if progress_text:
                    progress_text.text(f"Processing {filename} - {page_count} pages...")
                
                # Create a list to store extracted data from all pages
                all_page_results = []
                
                # Process each page in the PDF
                for page_num in range(page_count):
                    try:
                        # Update progress
                        if progress_bar:
                            progress_bar.progress((page_num + 1) / page_count)
                        if progress_text:
                            progress_text.text(f"Processing {filename} - Page {page_num+1}/{page_count}")
                        
                        # Load the current page
                        page = doc.load_page(page_num)
                        
                        # Process image
                        zoom = 2  # Zoom factor for image quality
                        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
                        image_bytes = pix.tobytes()
                        
                        # Convert image to data URL
                        image_data_url = image_to_data_url(image_bytes)
                        
                        # Call Azure OpenAI Vision to extract structured information
                        extracted_info = call_azure_openai_vision(prompt, image_data_url, client, deployment_name)
                        
                        # Add page info to the collected results
                        extracted_info_with_page = {
                            "page": page_num + 1,
                            "data": extracted_info
                        }
                        
                        # Add to our collection of all page results
                        all_page_results.append(extracted_info_with_page)
                        
                    except Exception as e:
                        error_msg = f"Error processing page {page_num+1} of {filename}: {e}"
                        st.warning(error_msg)
                        all_page_results.append({
                            "page": page_num + 1,
                            "data": {"error": str(e)}
                        })
        finally:
            # Clean up the temporary file
            try:
                os.unlink(tmp_path)
            except Exception as e:
                st.warning(f"Could not remove temporary file {tmp_path}: {e}")
        
        # Create a result object that contains all pages' data
        final_result = {
            "filename": filename,
            "total_pages": page_count,
            "pages": all_page_results
        }
        
        return final_result
        
    except Exception as e:
        error_msg = f"Error processing {pdf_file.name}: {e}"
        st.error(error_msg)
        return {
            "filename": pdf_file.name,
            "error": str(e),
            "total_pages": 0,
            "pages": []
        }

def process_blob_pdfs(blob_service_client, container_name, pdf_blobs, prompt, client, deployment_name):
    """
    Process PDF blobs from Azure Blob Storage.
    """
    all_pdf_results = []
    
    # Create progress tracking
    progress_bar = st.progress(0)
    progress_text = st.empty()
    
    for i, blob_name in enumerate(pdf_blobs):
        progress_text.text(f"Processing file {i+1}/{len(pdf_blobs)}: {blob_name}")
        
        # Download blob to memory
        blob_content = download_blob_to_memory(blob_service_client, container_name, blob_name)
        
        if blob_content is None:
            st.warning(f"Could not download blob: {blob_name}")
            continue
        
        # Create a BytesIO object from the blob content
        blob_file = io.BytesIO(blob_content)
        blob_file.name = blob_name.split('/')[-1]  # Set the filename to the blob name without folder path
        
        # Process the PDF
        pdf_result = process_pdf(
            blob_file,
            prompt,
            client,
            deployment_name,
            progress_bar,
            progress_text
        )
        
        # Add to results
        all_pdf_results.append(pdf_result)
        
        # Update overall progress
        progress_bar.progress((i + 1) / len(pdf_blobs))
    
    progress_text.text("Processing complete!")
    progress_bar.progress(1.0)
    
    return all_pdf_results

def create_results_dataframe(all_pdf_results):
    """
    Create a pandas DataFrame from the extracted results for easy viewing.
    """
    rows = []
    
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    for pdf_result in all_pdf_results:
        filename = pdf_result["filename"]
        
        for page in pdf_result["pages"]:
            page_num = page["page"]
            data = page["data"]
            
            # Check for errors
            if "error" in data:
                row_data = {
                    "Filename": filename,
                    "Page": page_num
                }
                
                # Add placeholders for all fields and confidence values
                for field in fields:
                    row_data[field] = "N/A"
                    row_data[f"{field} Confidence"] = 0
                
                rows.append(row_data)
                continue
            
            # Initialize row data
            row_data = {
                "Filename": filename,
                "Page": page_num
            }
            
            # Process each field
            for field in fields:
                field_data = data.get(field, {})
                
                if isinstance(field_data, dict):
                    value = field_data.get("value", "N/A")
                    confidence = field_data.get("confidence", 0)
                else:
                    value = field_data if field_data else "N/A"
                    confidence = 0
                
                # Ensure values are strings to avoid PyArrow errors
                if isinstance(value, (list, dict)):
                    value = str(value)
                
                # Add to row data
                row_data[field] = value
                row_data[f"{field} Confidence"] = round(confidence * 100, 2)
            
            # Add completed row to rows
            rows.append(row_data)
    
    try:
        # First method: Try creating a DataFrame with string type
        # This avoids PyArrow conversion issues for mixed types
        return pd.DataFrame(rows, dtype=str)
    except Exception as e:
        st.warning(f"Error creating DataFrame: {e}. Trying alternative method...")
        
        try:
            # Second method: Try with pandas default types but disable PyArrow
            with pd.option_context('mode.dtype_backend', 'numpy'):  # Use NumPy instead of PyArrow
                return pd.DataFrame(rows)
        except Exception as e:
            st.warning(f"Second method failed: {e}. Using final fallback method...")
            
            try:
                # Third method: Convert all values to strings explicitly before creating DataFrame
                string_rows = []
                for row in rows:
                    string_row = {}
                    for key, value in row.items():
                        string_row[key] = str(value)
                    string_rows.append(string_row)
                return pd.DataFrame(string_rows)
            except Exception as e:
                st.error(f"All DataFrame creation methods failed: {e}")
                # Return empty DataFrame as absolute last resort
                return pd.DataFrame()

def create_text_files_zip(all_pdf_results):
    """
    Create a zip file containing text files for each PDF.
    """
    # Create a BytesIO object to store the zip file
    zip_buffer = io.BytesIO()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create a ZipFile object
    with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:
        for pdf_result in all_pdf_results:
            filename = pdf_result["filename"]
            base_filename = os.path.splitext(filename)[0]
            
            # Create the text content for this PDF (only key-value pairs)
            page_results_text = create_page_results_text(pdf_result)
            
            # Add structured data as a text file with timestamp
            zip_file.writestr(f"{base_filename}_{timestamp}.txt", page_results_text)
    
    # Seek to the beginning of the BytesIO object
    zip_buffer.seek(0)
    return zip_buffer

def create_page_results_text(pdf_result):
    """
    Create a text file containing only the key-value pairs from each page.
    Returns a string with the formatted key-value pairs.
    """
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    result_text = ""
    
    for page in pdf_result["pages"]:
        page_num = page["page"]
        data = page["data"]
        
        result_text += f"--- PAGE {page_num} ---\n"
        
        if "error" in data:
            result_text += f"error: {data['error']}\n\n"
            continue
            
        # Process fields with confidence scores
        for field in fields:
            display_field = ''.join(' ' + char if char.isupper() else char for char in field).strip().lower()
            
            field_data = data.get(field, {})
            if isinstance(field_data, dict):
                value = field_data.get("value", "N/A")
                confidence = field_data.get("confidence", 0)
                result_text += f"{display_field}: {value}\n"
                result_text += f"{display_field} confidence: {round(confidence * 100, 2)}%\n"
            else:
                result_text += f"{display_field}: {field_data}\n"
        
        result_text += "\n"
        
    return result_text

def evaluate_extraction_results(all_pdf_results):
    """
    Evaluate the quality and completeness of extraction results using the
    confidence scores provided by Azure AI Vision.
    """
    # Define the fields we're extracting
    fields = [
        "VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
        "Freight", "Salestax", "Total"
    ]
    
    evaluation_results = {
        "total_documents": len(all_pdf_results),
        "total_pages": 0,
        "successful_pages": 0,
        "failed_pages": 0,
        "field_confidence": {},
        "documents_with_errors": []
    }
    
    # Initialize field confidence data structure
    for field in fields:
        evaluation_results["field_confidence"][field] = {
            "total": 0,
            "average_confidence": 0,
            "pages_above_threshold": 0,
            "percent_above_threshold": 0
        }
    
    confidence_threshold = 0.9  # 90% confidence threshold
    
    # Collect all confidence scores by field
    all_confidences = {}
    for field in fields:
        all_confidences[field] = []
    
    for pdf_result in all_pdf_results:
        filename = pdf_result["filename"]
        document_has_error = False
        
        for page in pdf_result["pages"]:
            evaluation_results["total_pages"] += 1
            data = page["data"]
            
            if "error" in data:
                evaluation_results["failed_pages"] += 1
                document_has_error = True
                continue
                
            page_successful = True
            
            # Check each field for confidence scores
            for field in fields:
                field_data = data.get(field, {})
                
                if isinstance(field_data, dict) and "confidence" in field_data:
                    confidence = field_data.get("confidence", 0)
                    evaluation_results["field_confidence"][field]["total"] += 1
                    all_confidences[field].append(confidence)
                    
                    # Count pages above threshold
                    if confidence >= confidence_threshold:
                        evaluation_results["field_confidence"][field]["pages_above_threshold"] += 1
                    else:
                        page_successful = False
                else:
                    page_successful = False
            
            if page_successful:
                evaluation_results["successful_pages"] += 1
            else:
                evaluation_results["failed_pages"] += 1
                document_has_error = True
        
        if document_has_error:
            evaluation_results["documents_with_errors"].append(filename)
    
    # Calculate average confidence for each field
    for field in all_confidences:
        confidences = all_confidences[field]
        if confidences:
            avg_confidence = sum(confidences) / len(confidences)
            evaluation_results["field_confidence"][field]["average_confidence"] = round(avg_confidence * 100, 2)
            
            # Calculate percentage of pages above threshold
            total = evaluation_results["field_confidence"][field]["total"]
            if total > 0:
                above_threshold = evaluation_results["field_confidence"][field]["pages_above_threshold"]
                evaluation_results["field_confidence"][field]["percent_above_threshold"] = round((above_threshold / total) * 100, 2)
    
    # Calculate overall success rate
    if evaluation_results["total_pages"] > 0:
        evaluation_results["success_rate"] = round((evaluation_results["successful_pages"] / evaluation_results["total_pages"]) * 100, 2)
    
    # Calculate overall confidence score (average of field confidence)
    field_scores = [field_data["average_confidence"] for field_data in evaluation_results["field_confidence"].values() if "average_confidence" in field_data]
    if field_scores:
        evaluation_results["overall_confidence_score"] = round(sum(field_scores) / len(field_scores), 2)
    
    return evaluation_results

def upload_to_blob_storage(blob_service_client, container_name, blob_name, data, content_type):
    """
    Upload data to Azure Blob Storage.
    """
    try:
        # Get the blob client
        container_client = blob_service_client.get_container_client(container_name)
        
        # Create the container if it doesn't exist
        if not container_client.exists():
            container_client.create_container()
        
        # Upload blob
        blob_client = container_client.get_blob_client(blob_name)
        
        # Set content settings
        content_settings = ContentSettings(content_type=content_type)
        
        # Upload the file
        blob_client.upload_blob(data, overwrite=True, content_settings=content_settings)
        
        return True, blob_client.url
    except Exception as e:
        return False, str(e)

def main():
    st.set_page_config(
        page_title="PDF Financial Data Extractor",
        page_icon="üìä",
        layout="wide"
    )
    
    st.title("PDF Financial Data Extractor")
    st.subheader("Extract invoice data from PDF files")
    
    # Check if Azure OpenAI credentials are available
    if not all([aoai_endpoint, aoai_api_key, aoai_deployment_name]):
        st.error("Azure OpenAI credentials are missing. Please set AOAI_ENDPOINT, AOAI_API_KEY, and AOAI_DEPLOYMENT environment variables.")
        return
    
    # Check if Azure Blob Storage credentials are available
    if not azure_storage_connection_string:
        st.warning("Azure Blob Storage connection string is missing. Some features will be disabled. Please set AZURE_STORAGE_CONNECTION_STRING environment variable.")
    
    # Initialize the clients
    client = get_client()
    blob_service_client = get_blob_service_client() if azure_storage_connection_string else None
    
    # Advanced settings in an expandable section
    with st.expander("Advanced Settings"):
        prompt = st.text_area(
            "Extraction Prompt", 
            """Based on this image, extract the following information from the invoice:   
            1) What is the vendor name?
            2) What is the invoice number?
            3) What is the invoice date?
            4) What is the customer name?
            5) What is the purchase order number?
            6) What is the stock code?
            7) What is the unit price?
            8) What is the invoice amount?
            9) What is the freight cost?
            10) What is the sales tax?
            11) What is the total amount?""",
            help="Customize the prompt sent to Azure OpenAI Vision to extract information"
        )
    
    # Input method selection
    input_method = st.radio(
        "Select Input Method",
        ["Upload Files", "Azure Blob Storage"],
        help="Choose how to input PDF files for processing"
    )
    
    if input_method == "Upload Files":
        # File uploader
        uploaded_files = st.file_uploader(
            "Upload PDF files", 
            type="pdf", 
            accept_multiple_files=True,
            help="Upload one or more PDF files containing financial statements"
        )
        
        # Process button
        process_button = st.button("Process Documents", type="primary")
        
        if process_button and not uploaded_files:
            st.warning("Please upload at least one PDF file.")
            return
            
        files_to_process = uploaded_files if process_button else None
    
    else:  # Azure Blob Storage
        if not blob_service_client:
            st.error("Azure Blob Storage connection is required for this option. Please set AZURE_STORAGE_CONNECTION_STRING environment variable.")
            return
        
        # Get available containers
        containers = get_blob_containers(blob_service_client)
        
        if not containers:
            st.error("No containers found in the Azure Blob Storage account. Please create at least one container.")
            return
        
        # Container selection
        selected_container = st.selectbox(
            "Select Container",
            containers,
            help="Choose an Azure Blob Storage container"
        )
        
        # Get folders in the selected container
        folders = get_blob_folders(blob_service_client, selected_container)
        
        # Folder selection
        selected_folder = st.selectbox(
            "Select Folder",
            folders,
            format_func=lambda x: "Root (No Folder)" if x == "" else x,
            help="Choose a folder within the container"
        )
        
        # List available PDFs
        pdf_blobs = list_pdf_blobs(blob_service_client, selected_container, selected_folder)
        
        if not pdf_blobs:
            st.warning(f"No PDF files found in the selected location. Please choose another container or folder.")
            return
        
        # Show available PDFs
        st.write(f"Found {len(pdf_blobs)} PDF files:")
        
        # Create columns for better display
        pdf_cols = st.columns(3)
        for i, pdf in enumerate(pdf_blobs):
            display_name = pdf.split('/')[-1]  # Remove folder path for display
            pdf_cols[i % 3].write(f"- {display_name}")
            if i >= 11:  # Limit display to avoid cluttering
                pdf_cols[(i + 1) % 3].write("...")
                break
        
        # Process button
        process_button = st.button("Process Blob Documents", type="primary")
        
        files_to_process = pdf_blobs if process_button else None
    
    # Process files if requested
    if files_to_process:
        with st.spinner("Processing documents..."):
            # Create a container for the whole processing section
            processing_container = st.container()
            
            with processing_container:
                # Store all PDF results
                all_pdf_results = []
                
                # Create a timestamp for the filename
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # Process the PDFs based on input method
                if input_method == "Upload Files":
                    # Process uploaded PDFs
                    progress_bar = st.progress(0)
                    progress_text = st.empty()
                    
                    # Process each uploaded PDF
                    for i, pdf_file in enumerate(files_to_process):
                        progress_text.text(f"Processing file {i+1}/{len(files_to_process)}: {pdf_file.name}")
                        
                        # Process the PDF and get results
                        pdf_result = process_pdf(
                            pdf_file, 
                            prompt, 
                            client, 
                            aoai_deployment_name,
                            progress_bar,
                            progress_text
                        )
                        
                        # Add to our collection of all PDF results
                        all_pdf_results.append(pdf_result)
                        
                        # Update overall progress
                        progress_bar.progress((i + 1) / len(files_to_process))
                        
                    progress_text.text("Processing complete!")
                    progress_bar.progress(1.0)
                else:
                    # Process PDFs from Blob Storage
                    all_pdf_results = process_blob_pdfs(
                        blob_service_client,
                        selected_container,
                        files_to_process,
                        prompt,
                        client,
                        aoai_deployment_name
                    )
                
                # Save individual text files for each PDF and upload to Azure Blob Storage
                blob_upload_results = []
                text_files_info = []
                
                if blob_service_client:
                    result_upload_container = st.text_input(
                        "Output Container Name",
                        value=azure_storage_container_name,
                        help="Container where results will be uploaded (will be created if doesn't exist)"
                    )
                    
                    st.info(f"Uploading results to Azure Blob Storage container: {result_upload_container}")
                    
                    for pdf_result in all_pdf_results:
                        filename = pdf_result["filename"]
                        base_filename = os.path.splitext(filename)[0]
                        
                        # Create the text content with key-value pairs
                        page_results_text = create_page_results_text(pdf_result)
                        
                        # Create timestamp filename
                        timestamp_filename = f"{base_filename}_{timestamp}"
                        
                        # Upload text file to blob storage
                        text_blob_name = f"{timestamp_filename}.txt"
                        text_success, text_url = upload_to_blob_storage(
                            blob_service_client,
                            result_upload_container,
                            text_blob_name,
                            page_results_text,
                            "text/plain"
                        )
                        
                        # Upload JSON to blob storage
                        json_blob_name = f"{timestamp_filename}.json"
                        pdf_json = json.dumps(pdf_result, ensure_ascii=False, indent=2)
                        json_success, json_url = upload_to_blob_storage(
                            blob_service_client,
                            result_upload_container,
                            json_blob_name,
                            pdf_json,
                            "application/json"
                        )
                        
                        # Store results
                        blob_upload_results.append({
                            "filename": filename,
                            "text_success": text_success,
                            "text_url": text_url if text_success else None,
                            "json_success": json_success,
                            "json_url": json_url if json_success else None
                        })
                        
                        # Store text file info for display
                        text_files_info.append({
                            "filename": filename,
                            "text_content": page_results_text,
                            "timestamp_filename": timestamp_filename
                        })
                
                # 1. Display extraction results
                st.subheader("1. Extraction Results")
                
                # Create a DataFrame view
                if all_pdf_results:
                    try:
                        results_df = create_results_dataframe(all_pdf_results)
                        
                        if not results_df.empty:
                            st.dataframe(results_df, use_container_width=True)
                        else:
                            st.warning("Could not create results table due to data format issues.")
                            
                    except Exception as e:
                        st.error(f"Error displaying results table: {e}")
                        st.info("Continuing with other outputs despite table display error.")
                    
                    # Create zip file with text extractions
                    text_zip = create_text_files_zip(all_pdf_results)
                    
                    # 2. Display extracted key-value pairs
                    st.subheader("2. Extracted Key-Value Pairs")
                    
                    # Create tabs for each PDF
                    if len(all_pdf_results) > 0:
                        pdf_tabs = st.tabs([pdf_result["filename"] for pdf_result in all_pdf_results])
                        
                        for i, tab in enumerate(pdf_tabs):
                            with tab:
                                pdf_result = all_pdf_results[i]
                                filename = pdf_result["filename"]
                                base_filename = os.path.splitext(filename)[0]
                                
                                # Generate or retrieve page results text
                                if text_files_info:
                                    page_results_text = next((info["text_content"] for info in text_files_info if info["filename"] == filename), "")
                                    timestamp_filename = next((info["timestamp_filename"] for info in text_files_info if info["filename"] == filename), "")
                                else:
                                    page_results_text = create_page_results_text(pdf_result)
                                    timestamp_filename = f"{base_filename}_{timestamp}"
                                
                                # Display the key-value page results
                                st.text_area(
                                    f"Extracted Data for {filename}",
                                    value=page_results_text,
                                    height=300,
                                    disabled=True
                                )
                    
                    # 3. Run evaluation and display results in table format
                    evaluation_results = evaluate_extraction_results(all_pdf_results)
                    st.subheader("3. Evaluation Results")
                    
                    # Create table of confidence levels by document
                    confidence_by_document = {}
                    
                    for pdf_result in all_pdf_results:
                        filename = pdf_result["filename"]
                        confidence_by_document[filename] = {
                            "high": 0,  # 90-100%
                            "low": 0    # <90%
                        }
                        
                        # Count fields in each confidence range for this document
                        for page in pdf_result["pages"]:
                            data = page["data"]
                            
                            if "error" in data:
                                continue
                                
                            for field in ["VendorName", "InvoiceNumber", "InvoiceDate", "CustomerName", 
                                        "PurchaseOrder", "StockCode", "UnitPrice", "InvoiceAmount", 
                                        "Freight", "Salestax", "Total"]:
                                field_data = data.get(field, {})
                                
                                if isinstance(field_data, dict) and "confidence" in field_data:
                                    confidence = field_data.get("confidence", 0) * 100
                                    
                                    if confidence >= 90:
                                        confidence_by_document[filename]["high"] += 1
                                    else:
                                        confidence_by_document[filename]["low"] += 1
                    
                    # Create table data with just high and low categories
                    doc_table_data = {
                        "Document": [],
                        "High Confidence (90-100%)": [],
                        "Low Confidence (<90%)": []
                    }
                    
                    for filename, counts in confidence_by_document.items():
                        doc_table_data["Document"].append(filename)
                        doc_table_data["High Confidence (90-100%)"].append(counts["high"])
                        doc_table_data["Low Confidence (<90%)"].append(counts["low"])
                    
                    # Display the document confidence table
                    doc_confidence_df = pd.DataFrame(doc_table_data)
                    st.dataframe(doc_confidence_df, use_container_width=True)
                    
                    # 4. Documents that need manual verification
                    st.subheader("4. Documents Needing Manual Verification")
                    
                    # Get documents with fields in low confidence range
                    docs_to_verify = []
                    
                    for filename, counts in confidence_by_document.items():
                        if counts["low"] > 0:
                            docs_to_verify.append({
                                "filename": filename,
                                "low_count": counts["low"]
                            })
                    
                    if docs_to_verify:
                        for doc in docs_to_verify:
                            st.warning(f"‚ö†Ô∏è {doc['filename']} - Needs verification ({doc['low_count']} fields with <90% confidence)")
                    else:
                        st.success("‚úÖ No documents need manual verification (all fields above 90% confidence)")
                    
                    # 5. Display Blob Storage upload results
                    if blob_service_client and blob_upload_results:
                        st.subheader("5. Azure Blob Storage Upload Results")
                        
                        # Create a table to show upload results
                        upload_rows = []
                        for result in blob_upload_results:
                            upload_rows.append({
                                "Filename": result["filename"],
                                "Text File": "‚úÖ Uploaded" if result["text_success"] else "‚ùå Failed",
                                "JSON File": "‚úÖ Uploaded" if result["json_success"] else "‚ùå Failed"
                            })
                        
                        upload_df = pd.DataFrame(upload_rows)
                        st.dataframe(upload_df, use_container_width=True)
                    
                    # 6. Download options
                    st.subheader("6. Download Options")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        # Download all text files as a zip
                        st.download_button(
                            label="Download All Text Files (ZIP)",
                            data=text_zip,
                            file_name=f"extracted_data_{timestamp}.zip",
                            mime="application/zip"
                        )
                    
                    with col2:
                        try:
                            # Download as CSV
                            if 'results_df' in locals() and not results_df.empty:
                                csv = results_df.to_csv(index=False)
                                st.download_button(
                                    label="Download CSV Results",
                                    data=csv,
                                    file_name=f"financial_data_extraction_{timestamp}.csv",
                                    mime="text/csv"
                                )
                            else:
                                st.info("CSV download not available due to DataFrame creation issues.")
                        except Exception as e:
                            st.error(f"Error creating CSV: {e}")
                            st.info("CSV download is unavailable due to an error.")
                    
                    with col3:
                        # Download document verification report
                        if docs_to_verify:
                            verification_data = {
                                "Document": [doc["filename"] for doc in docs_to_verify],
                                "Low Confidence Fields (<90%)": [doc["low_count"] for doc in docs_to_verify]
                            }
                            verification_df = pd.DataFrame(verification_data)
                            verification_csv = verification_df.to_csv(index=False)
                            
                            st.download_button(
                                label="Download Verification Report",
                                data=verification_csv,
                                file_name=f"verification_report_{timestamp}.csv",
                                mime="text/csv"
                            )
                        else:
                            st.download_button(
                                label="Download Verification Report",
                                data="No documents require verification",
                                file_name=f"verification_report_{timestamp}.txt",
                                mime="text/plain",
                                disabled=True
                            )
                else:
                    st.warning("No results were extracted from the PDFs.")

if __name__ == "__main__":
    main()
