For the RAG AI development and LLM evaluation stack, we’re considering the following libraries and tools:

LangChain – Framework for building RAG pipelines, chaining LLM calls, and integrating vector stores and tools.

Azure Prompt Flow (promptflow, promptflow-evals) – Microsoft’s native orchestration and evaluation framework, fully integrated within Azure.

RAGAS – Provides RAG-specific evaluation metrics (faithfulness, context precision/recall); can run locally and use Azure OpenAI as a judge.

DeepEval – Unit-testing framework for validating LLM outputs (similar to pytest for LLM systems).

MLflow – Experiment tracking and model lifecycle management; integrates natively with Azure ML.

Microsoft Presidio – PHI/PII detection and anonymization prior to evaluation to support compliance requirements.
