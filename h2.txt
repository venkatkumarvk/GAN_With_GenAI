**Here's the COMPLETE list of ALL blocks you need to change for OCR integration:**

---

## **SUMMARY: 16 BLOCKS ACROSS 5 FILES**

| # | File | Block | Description |
|---|------|-------|-------------|
| 1 | config.json | Add DI config | Add azure_document_intelligence section |
| 2 | config.json | Update doc types | Add use_ocr flag to each document type |
| 3 | document_intelligence_helper.py | NEW FILE | Create OCR helper class |
| 4 | llm.py | process_general | Add ocr_texts parameter and OCR mode |
| 5 | llm.py | process_batch | Add ocr_texts parameter |
| 6 | llm.py | prepare_batch_jsonl | Add ocr_texts parameter and OCR mode |
| 7 | h2.py | --no-ocr arg | Add command line argument |
| 8 | h2.py | Handle arg | Handle enable_ocr variable |
| 9 | h2.py | get_document_config | Return use_ocr flag |
| 10 | h2.py | Function signature | Update process_azure_pdf_files |
| 11 | h2.py | Function call | Pass enable_ocr parameter |
| 12 | h2.py | Get config | Get use_ocr from config |
| 13 | h2.py | Initialize DI | Initialize Document Intelligence |
| 14 | h2.py | Extract OCR | Extract OCR per page in loop |
| 15 | h2.py | Pass to LLM | Pass OCR texts to batch processing |
| 16 | requirements.txt | Add package | Add azure-ai-formrecognizer |

---

## **BLOCK 1: config.json - Add Azure Document Intelligence**

**LOCATION:** Top level, after `azure_openai`

**FIND:**
```json
{
  "azure_openai": {
    ...
  },
  "azure_storage": {
    ...
  }
}
```

**ADD BETWEEN azure_openai AND azure_storage:**
```json
"azure_document_intelligence": {
  "endpoint": "https://your-doc-intelligence.cognitiveservices.azure.com/",
  "api_key": "your-di-api-key",
  "ocr_read_enabled": true
},
```

---

## **BLOCK 2: config.json - Add use_ocr to Document Types**

**LOCATION:** Inside `processing.document_types`

**FIND:**
```json
"claim": {
  "extraction_fields": [...],
  "prompt_module": "claim_prompt",
  "overlay_enabled": true,
  "preprocessing_enabled": true
}
```

**CHANGE TO:**
```json
"claim": {
  "extraction_fields": [...],
  "prompt_module": "claim_prompt",
  "overlay_enabled": true,
  "preprocessing_enabled": true,
  "use_ocr": true
},
"invoice": {
  "extraction_fields": [...],
  "prompt_module": "invoice_prompt",
  "use_ocr": false
},
"eob": {
  "extraction_fields": [...],
  "prompt_module": "eob_prompt",
  "use_ocr": false
}
```

---

## **BLOCK 3: document_intelligence_helper.py - NEW FILE**

**CREATE NEW FILE:** `document_intelligence_helper.py`

```python
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
import logging


class DocumentIntelligenceHelper:
    """Azure Document Intelligence helper for OCR with multi-page support."""
    
    def __init__(self, endpoint, api_key, logger=None):
        self.endpoint = endpoint
        self.api_key = api_key
        self.logger = logger or logging.getLogger("doc_intelligence")
        
        self.client = DocumentAnalysisClient(
            endpoint=self.endpoint,
            credential=AzureKeyCredential(self.api_key)
        )
        
        self.logger.info("Document Intelligence initialized")
        self.logger.info(f"  Endpoint: {self.endpoint}")
    
    def extract_text_per_page(self, document_bytes):
        """
        Extract OCR text per page.
        
        Args:
            document_bytes: Document content as bytes
        
        Returns:
            List of (page_number, text) tuples or None on error
        """
        try:
            self.logger.debug("Starting OCR analysis")
            
            poller = self.client.begin_analyze_document(
                "prebuilt-read",
                document=document_bytes
            )
            result = poller.result()
            
            pages_text = []
            
            for page in result.pages:
                page_num = page.page_number - 1  # 0-indexed
                
                # Combine all lines in this page
                page_lines = []
                for line in page.lines:
                    page_lines.append(line.content)
                
                page_text = "\n".join(page_lines)
                pages_text.append((page_num, page_text))
                
                self.logger.debug(f"Page {page_num + 1}: extracted {len(page_text)} characters")
            
            self.logger.info(f"OCR extracted text from {len(pages_text)} pages")
            return pages_text
        
        except Exception as e:
            self.logger.error(f"OCR error: {str(e)}")
            return None
```

---

## **BLOCK 4: llm.py - Update process_general**

**LOCATION:** Around line 343-425

**FIND:** Your current `process_general` method

**REPLACE WITH:**
```python
def process_general(self, image_base64_strings, prompts, systemprompt=None, ocr_texts=None):
    """
    Process images using the general (non-batch) API.
    
    Parameters:
    - image_base64_strings: List of base64-encoded image strings
    - prompts: List of corresponding prompts
    - systemprompt: System prompt (optional)
    - ocr_texts: List of OCR extracted text (optional, one per image)
    
    Returns:
    - List of API responses
    """
    results = []
    
    self.logger.info(f"Processing {len(image_base64_strings)} images using General API")
    if ocr_texts:
        self.logger.info(f"OCR text provided for {len([t for t in ocr_texts if t])} images")
    
    for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
        try:
            self.logger.debug(f"Processing image {i+1}/{len(image_base64_strings)}")
            
            # Check if OCR text is available for this image
            has_ocr = ocr_texts and i < len(ocr_texts) and ocr_texts[i]
            
            if has_ocr:
                # OCR MODE: system prompt, OCR text, prompt, image all in user content
                self.logger.debug(f"Using OCR mode for image {i+1}")
                
                ocr_text = ocr_texts[i]
                
                response = self.openai_client.chat.completions.create(
                    model=self.deployment_name_openai,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": systemprompt if systemprompt else ""
                                },
                                {
                                    "type": "text",
                                    "text": ocr_text
                                },
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_img}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=2000,
                    temperature=0.5,
                    response_format={"type": "json_object"}
                )
            
            else:
                # NORMAL MODE: separate system message + user message with image
                self.logger.debug(f"Using standard mode for image {i+1}")
                
                messages = []
                
                if systemprompt:
                    messages.append({
                        "role": "system",
                        "content": systemprompt
                    })
                
                messages.append({
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_img}"
                            }
                        }
                    ]
                })
                
                response = self.openai_client.chat.completions.create(
                    model=self.deployment_name_openai,
                    messages=messages,
                    max_tokens=2000,
                    temperature=0.5,
                    response_format={"type": "json_object"}
                )
            
            self.track_tokens(response)
            
            if hasattr(response, 'choices') and len(response.choices) > 0:
                content = response.choices[0].message.content
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "response": {
                        "body": {
                            "choices": [
                                {
                                    "message": {
                                        "content": content
                                    }
                                }
                            ]
                        }
                    }
                }))
            else:
                results.append(json.dumps({
                    "custom_id": f"request-{i+1}",
                    "error": "No response content"
                }))
        
        except Exception as e:
            print(f"Error processing image {i+1}: {str(e)}")
            results.append(json.dumps({
                "custom_id": f"request-{i+1}",
                "error": str(e)
            }))
    
    self.logger.info(f"General API processing complete: {len([r for r in results if 'error' not in r])}/{len(results)} successful")
    
    return results
```

---

## **BLOCK 5: llm.py - Update process_batch signature**

**LOCATION:** Around line 500

**FIND:**
```python
def process_batch(self, image_base64_strings, prompts, systemprompt=None):
```

**CHANGE TO:**
```python
def process_batch(self, image_base64_strings, prompts, systemprompt=None, ocr_texts=None):
    """
    Process images in a batch using the Azure OpenAI batch API.
    Tracks token usage.
    
    Parameters:
    - image_base64_strings: List of base64-encoded image strings
    - prompts: List of corresponding prompts
    - systemprompt: System prompt (optional)
    - ocr_texts: List of OCR extracted text (optional, one per image)
    
    Returns:
    - List of API responses
    """
    self.logger.info(f"Processing {len(image_base64_strings)} images using Batch API")
    if ocr_texts:
        self.logger.info(f"OCR text provided for {len([t for t in ocr_texts if t])} images")
```

**AND UPDATE THE CALL:**

**FIND:**
```python
jsonl_file = self.prepare_batch_jsonl(image_base64_strings, prompts, systemprompt)
```

**CHANGE TO:**
```python
jsonl_file = self.prepare_batch_jsonl(image_base64_strings, prompts, systemprompt, ocr_texts)
```

---

## **BLOCK 6: llm.py - Update prepare_batch_jsonl**

**LOCATION:** Around line 600+

**FIND:**
```python
def prepare_batch_jsonl(self, image_base64_strings, prompts, systemprompt=None):
```

**REPLACE ENTIRE METHOD WITH:**
```python
def prepare_batch_jsonl(self, image_base64_strings, prompts, systemprompt=None, ocr_texts=None):
    """
    Parameters:
    - image_base64_strings: List of base64-encoded image strings
    - prompts: List of corresponding prompts
    - systemprompt: System prompt (optional)
    - ocr_texts: List of OCR extracted text (optional, one per image)
    
    Returns:
    - BytesIO object containing the JSONL content
    """
    jsonl_file = BytesIO()
    
    for i, (base64_img, prompt) in enumerate(zip(image_base64_strings, prompts)):
        
        # Check if OCR text is available for this request
        has_ocr = ocr_texts and i < len(ocr_texts) and ocr_texts[i]
        
        if has_ocr:
            # OCR MODE: system prompt, OCR text, prompt, image in user content
            self.logger.debug(f"Using OCR mode for batch request {i+1}")
            
            ocr_text = ocr_texts[i]
            
            request = {
                "custom_id": f"request-{i+1}",
                "method": "POST",
                "url": "/chat/completions",
                "body": {
                    "model": self.deployment_name,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": systemprompt if systemprompt else ""
                                },
                                {
                                    "type": "text",
                                    "text": ocr_text
                                },
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_img}"
                                    }
                                }
                            ]
                        }
                    ],
                    "max_tokens": 2000,
                    "temperature": 0.7,
                    "response_format": {"type": "json_object"}
                }
            }
        
        else:
            # NORMAL MODE: separate system and user messages
            self.logger.debug(f"Using standard mode for batch request {i+1}")
            
            Content = systemprompt
            
            request = {
                "custom_id": f"request-{i+1}",
                "method": "POST",
                "url": "/chat/completions",
                "body": {
                    "model": self.deployment_name,
                    "messages": [
                        {
                            "role": "system",
                            "content": Content
                        },
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_img}"
                                    }
                                }
                            ]
                        }
                    ],
                    "max_tokens": 2000,
                    "temperature": 0.7,
                    "response_format": {"type": "json_object"}
                }
            }
        
        jsonl_file.write((json.dumps(request) + "\n").encode('utf-8'))
    
    jsonl_file.seek(0)
    return jsonl_file
```

---

## **BLOCK 7: h2.py - Add --no-ocr Argument**

**LOCATION:** Around line 850

**FIND:**
```python
parser.add_argument("--no-preprocessing", action="store_true",
                 help="Disable image preprocessing")

args = parser.parse_args()
```

**ADD BEFORE args = parser.parse_args():**
```python
parser.add_argument("--no-ocr", action="store_true",
                 help="Disable OCR extraction")
```

---

## **BLOCK 8: h2.py - Handle enable_ocr**

**LOCATION:** Around line 880

**FIND:**
```python
enable_preprocessing = None
if args.enable_preprocessing:
    enable_preprocessing = True
elif args.no_preprocessing:
    enable_preprocessing = False

# Override archive setting
```

**ADD BEFORE "Override archive setting":**
```python
enable_ocr = None
if args.no_ocr:
    enable_ocr = False
    logger.info("OCR disabled via --no-ocr")
```

---

## **BLOCK 9: h2.py - Update get_document_config**

**LOCATION:** Around line 60

**FIND:**
```python
preprocessing_config = doc_config.get("preprocessing_config", {}) if preprocessing_enabled else None

return extraction_fields, systemprompt, prompt_template, model_config, overlay_config, preprocessing_config
```

**CHANGE TO:**
```python
preprocessing_config = doc_config.get("preprocessing_config", {}) if preprocessing_enabled else None

use_ocr = doc_config.get("use_ocr", False)

return extraction_fields, systemprompt, prompt_template, model_config, overlay_config, preprocessing_config, use_ocr
```

---

## **BLOCK 10: h2.py - Update process_azure_pdf_files Signature**

**LOCATION:** Around line 150

**FIND:**
```python
def process_azure_pdf_files(config, api_type, azure_folder, doc_type, logger, enable_overlay=None, enable_preprocessing=None):
```

**CHANGE TO:**
```python
def process_azure_pdf_files(config, api_type, azure_folder, doc_type, logger, enable_overlay=None, enable_preprocessing=None, enable_ocr=None):
```

---

## **BLOCK 11: h2.py - Update process_azure_pdf_files Call**

**LOCATION:** Around line 920

**FIND:**
```python
process_azure_pdf_files(config, args.apitype, args.folder, args.doctype, logger, enable_overlay, enable_preprocessing)
```

**CHANGE TO:**
```python
process_azure_pdf_files(config, args.apitype, args.folder, args.doctype, logger, enable_overlay, enable_preprocessing, enable_ocr)
```

---

## **BLOCK 12: h2.py - Get use_ocr from Config**

**LOCATION:** Around line 170

**FIND:**
```python
extraction_fields, systemprompt, prompt_template, model_config, overlay_config, preprocessing_config = get_document_config(config, doc_type)
```

**CHANGE TO:**
```python
extraction_fields, systemprompt, prompt_template, model_config, overlay_config, preprocessing_config, use_ocr = get_document_config(config, doc_type)
```

---

## **BLOCK 13: h2.py - Initialize Document Intelligence**

**LOCATION:** Around line 210, after overlay initialization

**FIND:**
```python
else:
    logger.info(f"Overlay disabled for {doc_type}")

# Initialize storage helper
storage_helper = AzureStorageHelper(...)
```

**ADD BETWEEN overlay AND storage_helper:**
```python
# Handle OCR
if enable_ocr is not None:
    use_ocr = enable_ocr

doc_intelligence = None
if use_ocr:
    di_config = config.get("azure_document_intelligence", {})
    if di_config.get("endpoint") and di_config.get("api_key"):
        from document_intelligence_helper import DocumentIntelligenceHelper
        doc_intelligence = DocumentIntelligenceHelper(
            di_config["endpoint"],
            di_config["api_key"],
            logger
        )
        logger.info(f"OCR initialized for {doc_type}")
    else:
        logger.warning("OCR enabled but not configured")
        use_ocr = False
else:
    logger.info(f"OCR disabled for {doc_type}")
```

---

## **BLOCK 14: h2.py - Extract OCR in Loop**

**LOCATION:** Around line 400, after overlay processing

**FIND:**
```python
# Apply overlay
processed_pdf_content = blob_content
if overlay_processor and filename.lower().endswith('.pdf'):
    processed_pdf_content = overlay_processor.process_pdf_with_overlay(...)

# Extract pages
pages = pdf_processor.extract_pages_from_file(processed_pdf_content, filename)
```

**ADD AFTER OVERLAY, BEFORE EXTRACT PAGES:**
```python
# Extract OCR per page
ocr_pages_dict = {}
if doc_intelligence and use_ocr:
    logger.info(f"Extracting OCR from {filename}")
    ocr_result = doc_intelligence.extract_text_per_page(processed_pdf_content)
    
    if ocr_result:
        for page_num, text in ocr_result:
            ocr_pages_dict[page_num] = text
        logger.info(f"OCR: {len(ocr_pages_dict)} pages")
    else:
        logger.warning(f"OCR failed for {filename}")
```

---

## **BLOCK 15: h2.py - Pass OCR to Batch Processing**

**LOCATION:** Around line 450, in batch loop

**FIND:**
```python
formatted_prompt = create_formatted_prompt(prompt_template, extraction_fields)
prompts = [formatted_prompt for _ in range(len(batch_pages))]

logger.info(f"Processing batch...")

if api_type == "batch":
    raw_results = ai_client.process_batch(base64_strings, prompts, systemprompt)
else:
    raw_results = ai_client.process_general(base64_strings, prompts, systemprompt)
```

**REPLACE WITH:**
```python
formatted_prompt = create_formatted_prompt(prompt_template, extraction_fields)
prompts = [formatted_prompt for _ in range(len(batch_pages))]

# Create OCR list
batch_ocr_texts = []
for page_num, base64_string in batch_pages:
    if page_num in ocr_pages_dict:
        batch_ocr_texts.append(ocr_pages_dict[page_num])
    else:
        batch_ocr_texts.append(None)

logger.info(f"Processing batch of {len(batch_pages)} pages")
if any(batch_ocr_texts):
    logger.debug(f"OCR available for {len([t for t in batch_ocr_texts if t])} pages")

if api_type == "batch":
    raw_results = ai_client.process_batch(base64_strings, prompts, systemprompt, ocr_texts=batch_ocr_texts)
else:
    raw_results = ai_client.process_general(base64_strings, prompts, systemprompt, ocr_texts=batch_ocr_texts)
```

---

## **BLOCK 16: requirements.txt - Add Package**

**ADD:**
```txt
azure-ai-formrecognizer
```

---

## **INSTALLATION:**

```bash
pip install azure-ai-formrecognizer
```

---

## **TESTING:**

```bash
# With OCR (claim)
python h2.py --apitype general --source azure --folder "claims/" --doctype claim

# Without OCR (invoice)
python h2.py --apitype general --source azure --folder "invoices/" --doctype invoice

# Force disable OCR
python h2.py --apitype general --source azure --folder "claims/" --doctype claim --no-ocr
```

---

**That's ALL 16 blocks!** Complete OCR integration ready to use! ðŸŽ¯
