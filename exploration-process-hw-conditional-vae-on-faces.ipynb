{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561},{"sourceId":1461143,"sourceType":"datasetVersion","datasetId":854188}],"dockerImageVersionId":29995,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nimport torch.optim as  optim \nif torch.cuda.is_available():  \n  dev = \"cuda:0\" \n  print(\"gpu up\")\nelse:  \n  dev = \"cpu\"  \ndevice = torch.device(dev)","metadata":{"id":"uvokIGt2cZNx","outputId":"20f88dc7-03a1-4c26-cdcc-cbbf9572e17f","execution":{"iopub.status.busy":"2024-06-11T08:40:58.402567Z","iopub.execute_input":"2024-06-11T08:40:58.402961Z","iopub.status.idle":"2024-06-11T08:41:00.060201Z","shell.execute_reply.started":"2024-06-11T08:40:58.402921Z","shell.execute_reply":"2024-06-11T08:41:00.059175Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/celeba-dataset/list_attr_celeba.csv\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","id":"gFpm3es0cZN6","execution":{"iopub.status.busy":"2024-06-11T08:41:03.092768Z","iopub.execute_input":"2024-06-11T08:41:03.093142Z","iopub.status.idle":"2024-06-11T08:41:03.974216Z","shell.execute_reply.started":"2024-06-11T08:41:03.093110Z","shell.execute_reply":"2024-06-11T08:41:03.973207Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def haircolor(x):\n    if x[\"Blond_Hair\"] == 1:\n        return 0\n    elif x[\"Brown_Hair\"] == 1:\n        return 1\n    elif x[\"Black_Hair\"] == 1:\n        return 2\n    else :\n        return 3\n    \ndf[\"Hair_Color\"] = df.apply(haircolor,axis=1)","metadata":{"id":"PK2VcrE3cZOV","execution":{"iopub.status.busy":"2024-06-11T08:41:07.032000Z","iopub.execute_input":"2024-06-11T08:41:07.032384Z","iopub.status.idle":"2024-06-11T08:41:10.608458Z","shell.execute_reply.started":"2024-06-11T08:41:07.032344Z","shell.execute_reply":"2024-06-11T08:41:10.607559Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfor simplicity I decided to make the VAE capture just for attributes \n\nHair Color (blond,brown,black and neither of these(or unknown))\nPale Skin \nGender \nBeard (in case of male)\n\"\"\"\n\ndf = df[[\"image_id\",\"Hair_Color\",'Pale_Skin',\"Male\",\"No_Beard\"]]","metadata":{"id":"JWNP5HVucZOb","execution":{"iopub.status.busy":"2024-06-11T08:41:14.901925Z","iopub.execute_input":"2024-06-11T08:41:14.902270Z","iopub.status.idle":"2024-06-11T08:41:14.970104Z","shell.execute_reply.started":"2024-06-11T08:41:14.902239Z","shell.execute_reply":"2024-06-11T08:41:14.969098Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nreplacing -1 with 0 without affecting the the 1 values \n\n\"\"\"\ndf.Pale_Skin = df.Pale_Skin.apply(lambda x: max(x,0)) \ndf.Male = df.Male.apply(lambda x: max(x,0))\ndf.No_Beard = df.No_Beard.apply(lambda x: max(x,0))\n","metadata":{"id":"tV-bCS_FcZOx","execution":{"iopub.status.busy":"2024-06-11T08:41:18.797415Z","iopub.execute_input":"2024-06-11T08:41:18.797779Z","iopub.status.idle":"2024-06-11T08:41:19.185941Z","shell.execute_reply.started":"2024-06-11T08:41:18.797747Z","shell.execute_reply":"2024-06-11T08:41:19.185186Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:41:24.388393Z","iopub.execute_input":"2024-06-11T08:41:24.388793Z","iopub.status.idle":"2024-06-11T08:41:24.409426Z","shell.execute_reply.started":"2024-06-11T08:41:24.388756Z","shell.execute_reply":"2024-06-11T08:41:24.408480Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"          image_id  Hair_Color  Pale_Skin  Male  No_Beard\n0       000001.jpg           1          0     0         1\n1       000002.jpg           1          0     0         1\n2       000003.jpg           3          0     1         1\n3       000004.jpg           3          0     0         1\n4       000005.jpg           3          0     0         1\n...            ...         ...        ...   ...       ...\n202594  202595.jpg           0          0     0         1\n202595  202596.jpg           0          1     1         1\n202596  202597.jpg           2          0     1         1\n202597  202598.jpg           2          0     0         1\n202598  202599.jpg           0          1     0         1\n\n[202599 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>Hair_Color</th>\n      <th>Pale_Skin</th>\n      <th>Male</th>\n      <th>No_Beard</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000001.jpg</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000002.jpg</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000003.jpg</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000004.jpg</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000005.jpg</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>202594</th>\n      <td>202595.jpg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>202595</th>\n      <td>202596.jpg</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>202596</th>\n      <td>202597.jpg</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>202597</th>\n      <td>202598.jpg</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>202598</th>\n      <td>202599.jpg</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>202599 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"faces =[]\nfor i in df.iloc[:20000].image_id:\n    pic = Image.open(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/\"+i)\n    pic = pic.resize((64,64))\n    pix = np.array(pic.getdata()).reshape(pic.size[0], pic.size[1], 3)\n    pix = pix/255\n    \"\"\"\n    for the images I had to use np.moveaxis to change the shape from  (64,64,3) to (3,64,64)\n    without messing up the image \n    \n    \"\"\"\n    faces.append(np.moveaxis(pix,-1,0).tolist())\n    \nfaces = np.array(faces)","metadata":{"id":"w31LTAHwcZPY","execution":{"iopub.status.busy":"2024-06-11T08:41:30.437488Z","iopub.execute_input":"2024-06-11T08:41:30.437864Z","iopub.status.idle":"2024-06-11T08:45:48.708756Z","shell.execute_reply.started":"2024-06-11T08:41:30.437831Z","shell.execute_reply":"2024-06-11T08:45:48.707894Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder,self).__init__()\n        # channels_in ,  channels_out, kernel_size, stride , padding,\n        self.conv1 = nn.Conv2d(3,64,3,1,1)\n        self.conv2 = nn.Conv2d(64,64,3,1,1)\n        self.conv3 = nn.Conv2d(64,64,4,2,1)\n        self.conv4 = nn.Conv2d(64,128,4,2,1)\n        self.maxp1 = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.maxp2 = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.maxp3 = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.maxp4 = nn.MaxPool2d(kernel_size=2,stride=2)\n\n\n        \n    def forward(self,x):\n        out = self.conv1(x)\n        out = self.maxp1(out)\n        out = F.relu(out) \n        out = self.conv2(out)\n        out = self.maxp2(out)\n        out = F.relu(out)\n        out = self.conv3(out)\n        out = self.maxp3(out)\n        out = F.relu(out)\n        out= self.conv4(out)\n        out = self.maxp4(out)\n        out = F.relu(out)\n        return out.view(out.shape[0],-1)\n    \nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder,self).__init__()\n        # channels_in ,  channels_out, kernel_size, stride , padding,\n        \"\"\"\n        convtranspose  is different from the regular conv layer (looking at the equations of two of them )\n        this link shows examples about it \n        https://towardsdatascience.com/is-the-transposed-convolution-layer-and-convolution-layer-the-same-thing-8655b751c3a1\n        \n        \"\"\"\n        self.transconv1 = nn.ConvTranspose2d(64+ 40,64,8,4,2)\n        self.transconv2 = nn.ConvTranspose2d(64,64,8,4,2)\n        self.transconv3 = nn.ConvTranspose2d(64,64,4,2,1)\n        self.transconv4 = nn.ConvTranspose2d(64,3,4,2,1)\n        \n        \"\"\"\n        I think Embeddings layers were pretty good  so every state was converted into a vector like \n        Beard or no beard -> vector of 10 weights\n        Male or Female -> vector  of 10 weights \n        \n        \"\"\"\n        self.hairEmbedding = nn.Embedding(4,10)\n        self.beardEmbedding = nn.Embedding(2,10)\n        self.genderEmbedding = nn.Embedding(2,10)\n        self.paleSkinEmbedding = nn.Embedding(2,10)\n\n        \n    def forward(self,x):\n        z = x[:,:64]\n        hair = self.hairEmbedding(x[:,64].long())\n        paleSkin = self.paleSkinEmbedding(x[:,65].long())\n        gender = self.genderEmbedding(x[:,66].long())\n        beard = self.beardEmbedding(x[:,67].long())\n        \"\"\"\n        Concating the embeddings and the encoded image\n        \"\"\"\n        z = torch.cat([z,hair,beard,gender,paleSkin],dim=1)\n        \n        out= self.transconv1(z.view(z.shape[0],z.shape[1],1,1))\n        out = F.relu(out)\n        out= self.transconv2(out)\n        out = F.relu(out)\n\n        out= self.transconv3(out)\n        out = F.relu(out)\n\n        out= self.transconv4(out)\n        out = F.relu(out)\n\n        return out\n        \nclass CVAE(nn.Module):\n    def __init__(self,encoder,decoder):\n        super(CVAE,self).__init__()\n        self.encoder = encoder()\n        self.decoder = decoder()\n    def forward(self,x,attrs):\n        h = self.encoder(x)\n        \n        mu = h[:,:64]\n        logvar = h[:,64:]\n        # this part is for the reparameterization trick\n        s= torch.exp(logvar)\n        eps = torch.randn_like(s)\n        z = s*eps + mu \n        \n        z= torch.cat([z,attrs],dim=1)\n        out = self.decoder(z)\n        return out,mu,logvar\n        \nvae = CVAE(Encoder,Decoder)\nvae.to(device)","metadata":{"id":"n7G5F_qEcZPi","outputId":"6b5f0243-8c40-490b-b17c-faea06bc155e","execution":{"iopub.status.busy":"2024-06-11T08:46:01.460147Z","iopub.execute_input":"2024-06-11T08:46:01.460511Z","iopub.status.idle":"2024-06-11T08:46:01.527652Z","shell.execute_reply.started":"2024-06-11T08:46:01.460475Z","shell.execute_reply":"2024-06-11T08:46:01.526651Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CVAE(\n  (encoder): Encoder(\n    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (conv4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (maxp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (maxp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (maxp3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (maxp4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (decoder): Decoder(\n    (transconv1): ConvTranspose2d(104, 64, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))\n    (transconv2): ConvTranspose2d(64, 64, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))\n    (transconv3): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (transconv4): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (hairEmbedding): Embedding(4, 10)\n    (beardEmbedding): Embedding(2, 10)\n    (genderEmbedding): Embedding(2, 10)\n    (paleSkinEmbedding): Embedding(2, 10)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def ceil(a,b):\n    return -(-a//b)","metadata":{"id":"LJ9v0Kj7cZP_","execution":{"iopub.status.busy":"2024-06-11T08:46:16.177596Z","iopub.execute_input":"2024-06-11T08:46:16.177990Z","iopub.status.idle":"2024-06-11T08:46:16.182530Z","shell.execute_reply.started":"2024-06-11T08:46:16.177930Z","shell.execute_reply":"2024-06-11T08:46:16.181186Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\nloss function contains two parts \nreconstruction  loss and kullback leibler divergence (it basically measures how two distributions are different)\n\n\"\"\"\ndef loss_function(recon_x,x,mu,logvar):\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    KLD /= x.shape[0] * 3 * 64 * 64\n    recon_loss = F.mse_loss(recon_x,x)\n    return recon_loss +  KLD\n\nepochs = 1201\n\nbatch_size= 256\noptimizer = optim.Adagrad(vae.parameters(),lr = 0.001)\n\nnpData = df.iloc[:20000].to_numpy()\n\n\nn_samples = len(npData)\nbetter_batch_size = ceil(n_samples, ceil(n_samples, batch_size))\nlosstrack = []\nfor e in range(epochs):\n    losses = []\n    for i in range(ceil(n_samples, better_batch_size)):\n        batch = npData[i * better_batch_size: (i+1) * better_batch_size]\n        attrs = torch.Tensor(batch[:,1:].astype('float16')).to(device)\n#         break\n        imgs = faces[i * better_batch_size: (i+1) * better_batch_size]\n        imgs = torch.Tensor(imgs.astype('float16')).to(device)\n        vae.zero_grad()\n        recon_imgs,mu,logvar = vae(imgs,attrs)\n        err = loss_function(recon_imgs,imgs,mu,logvar)\n        err.backward()\n        optimizer.step()\n        losses.append(err.item())\n    losstrack.append(np.mean(losses))\n    if e % 100 == 0: \n        torch.save(vae.state_dict(), \"./vae.pt\")\n        print(np.mean(losses), \"mean loss\", e)","metadata":{"id":"LIfB7jPTcZQG","outputId":"fd3eb022-3a81-4814-9815-9fa183328dbe","execution":{"iopub.status.busy":"2024-06-11T08:46:20.559279Z","iopub.execute_input":"2024-06-11T08:46:20.559621Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0.28111351215386693 mean loss 0\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"I just loaded the model I saved while training\"\"\"\n\ncheckpoint = torch.load(\"../input/my-cvae-model/vae.pt\")\nvae.load_state_dict(checkpoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def showImage(x):\n    attrs = torch.Tensor(x)\n    h = torch.cat((torch.randn(1,64),attrs),dim=1).to(device)\n    img = vae.decoder(h)\n    img = img.cpu().detach().numpy().reshape(3,64,64)\n    img = np.moveaxis(img,0,-1)\n    return img","metadata":{"id":"O3eJt5-XcZQ7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axarr = plt.subplots(1,2)\naxarr[0].imshow(showImage([[2,0,0,1]]))\naxarr[0].set_title(\"Woman\")\naxarr[1].imshow(showImage([[2,0,1,0]]))\naxarr[1].set_title(\"Man\")\n\nf.subplots_adjust(hspace=0.3,left=2,right=3)","metadata":{"id":"GvmW09CzVDy0","outputId":"c3ee412e-9611-402b-b957-c423237cf3ca","_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here showing what could the model do after the training \n\nit could kinda generate different genders","metadata":{}},{"cell_type":"code","source":"f, axarr = plt.subplots(1,3)\naxarr[0].imshow(showImage([[2,0,0,1]]))\naxarr[0].set_title(\"Black hair\")\naxarr[1].imshow(showImage([[1,0,0,1]]))\naxarr[1].set_title(\"brown hair\")\naxarr[2].imshow(showImage([[0,0,0,1]]))\naxarr[2].set_title(\"blond hair\")\n\n\nf.subplots_adjust(hspace=0.3,left=2,right=4)","metadata":{"id":"NqIGvz1ccZRE","outputId":"9f190830-cb22-40ae-9ccb-037445ee47dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"it also can make different hair colors but it mostly was trying to just fill the image outside with the color","metadata":{}},{"cell_type":"code","source":"f, axarr = plt.subplots(1,2)\naxarr[0].imshow(showImage([[2,0,0,1]]))\naxarr[0].set_title(\"not Pale skin\")\naxarr[1].imshow(showImage([[2,1,0,1]]))\naxarr[1].set_title(\"Pale Skin\")\n\nf.subplots_adjust(hspace=0.3,left=2,right=3)","metadata":{"id":"_mykTq2PcZRL","outputId":"cb3147a8-f573-4af1-8836-812f754e8ad3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"it could get doing the skin part correctly ","metadata":{}},{"cell_type":"code","source":"f, axarr = plt.subplots(1,2)\naxarr[0].imshow(showImage([[2,0,1,1]]))\naxarr[0].set_title(\"No Beard\")\naxarr[1].imshow(showImage([[2,0,1,0]]))\naxarr[1].set_title(\"Beard\")\n\nf.subplots_adjust(hspace=0.3,left=2,right=3)","metadata":{"id":"v63aPK-rcZRZ","outputId":"17ad6645-68c2-4c91-85c4-8d435ac1b54f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"well this one is not good maybe the model needs to be bigger(and also more data)  because there is barely a difference between the two images ","metadata":{}}]}